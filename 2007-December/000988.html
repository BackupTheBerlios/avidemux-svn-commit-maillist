<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<HTML>
 <HEAD>
   <TITLE> [Avidemux-svn-commit] r3720 - in branches/avidemux_2.4_branch: .	avidemux/ADM_libraries/ADM_lavcodec	avidemux/ADM_libraries/ADM_lavcodec/ppc	avidemux/ADM_libraries/ADM_libmpeg2enc
   </TITLE>
   <LINK REL="Index" HREF="http://lists.berlios.de/pipermail/avidemux-svn-commit/2007-December/index.html" >
   <LINK REL="made" HREF="mailto:avidemux-svn-commit%40lists.berlios.de?Subject=Re%3A%20%5BAvidemux-svn-commit%5D%20r3720%20-%20in%20branches/avidemux_2.4_branch%3A%20.%0A%09avidemux/ADM_libraries/ADM_lavcodec%0A%09avidemux/ADM_libraries/ADM_lavcodec/ppc%0A%09avidemux/ADM_libraries/ADM_libmpeg2enc&In-Reply-To=%3C200712040020.lB40KBuM019102%40sheep.berlios.de%3E">
   <META NAME="robots" CONTENT="index,nofollow">
   <style type="text/css">
       pre {
           white-space: pre-wrap;       /* css-2.1, curent FF, Opera, Safari */
           }
   </style>
   <META http-equiv="Content-Type" content="text/html; charset=us-ascii">
   <LINK REL="Previous"  HREF="000987.html">
   <LINK REL="Next"  HREF="000989.html">
 </HEAD>
 <BODY BGCOLOR="#ffffff">
   <H1>[Avidemux-svn-commit] r3720 - in branches/avidemux_2.4_branch: .	avidemux/ADM_libraries/ADM_lavcodec	avidemux/ADM_libraries/ADM_lavcodec/ppc	avidemux/ADM_libraries/ADM_libmpeg2enc</H1>
    <B>gruntster at mail.berlios.de</B> 
    <A HREF="mailto:avidemux-svn-commit%40lists.berlios.de?Subject=Re%3A%20%5BAvidemux-svn-commit%5D%20r3720%20-%20in%20branches/avidemux_2.4_branch%3A%20.%0A%09avidemux/ADM_libraries/ADM_lavcodec%0A%09avidemux/ADM_libraries/ADM_lavcodec/ppc%0A%09avidemux/ADM_libraries/ADM_libmpeg2enc&In-Reply-To=%3C200712040020.lB40KBuM019102%40sheep.berlios.de%3E"
       TITLE="[Avidemux-svn-commit] r3720 - in branches/avidemux_2.4_branch: .	avidemux/ADM_libraries/ADM_lavcodec	avidemux/ADM_libraries/ADM_lavcodec/ppc	avidemux/ADM_libraries/ADM_libmpeg2enc">gruntster at mail.berlios.de
       </A><BR>
    <I>Tue Dec  4 01:20:11 CET 2007</I>
    <P><UL>
        <LI>Previous message: <A HREF="000987.html">[Avidemux-svn-commit] r3719 -	branches/avidemux_2.4_branch/avidemux/ADM_audiodevice
</A></li>
        <LI>Next message: <A HREF="000989.html">[Avidemux-svn-commit] r3721 - in branches/avidemux_2.4_branch: .	avidemux/ADM_libraries/ADM_libswscale	avidemux/ADM_libraries/ADM_mplex
</A></li>
         <LI> <B>Messages sorted by:</B> 
              <a href="date.html#988">[ date ]</a>
              <a href="thread.html#988">[ thread ]</a>
              <a href="subject.html#988">[ subject ]</a>
              <a href="author.html#988">[ author ]</a>
         </LI>
       </UL>
    <HR>  
<!--beginarticle-->
<PRE>Author: gruntster
Date: 2007-12-04 01:19:48 +0100 (Tue, 04 Dec 2007)
New Revision: 3720

Added:
   branches/avidemux_2.4_branch/avidemux/ADM_libraries/ADM_lavcodec/ppc/float_altivec.c
   branches/avidemux_2.4_branch/avidemux/ADM_libraries/ADM_lavcodec/ppc/h264_altivec.c
   branches/avidemux_2.4_branch/avidemux/ADM_libraries/ADM_lavcodec/ppc/h264_template_altivec.c
   branches/avidemux_2.4_branch/avidemux/ADM_libraries/ADM_lavcodec/ppc/int_altivec.c
   branches/avidemux_2.4_branch/avidemux/ADM_libraries/ADM_lavcodec/ppc/mathops.h
   branches/avidemux_2.4_branch/avidemux/ADM_libraries/ADM_lavcodec/ppc/types_altivec.h
   branches/avidemux_2.4_branch/avidemux/ADM_libraries/ADM_lavcodec/ppc/vc1dsp_altivec.c
Removed:
   branches/avidemux_2.4_branch/avidemux/ADM_libraries/ADM_lavcodec/ppc/dsputil_h264_altivec.c
   branches/avidemux_2.4_branch/avidemux/ADM_libraries/ADM_lavcodec/ppc/dsputil_h264_template_altivec.c
Modified:
   branches/avidemux_2.4_branch/ConfigureChecks.cmake
   branches/avidemux_2.4_branch/avidemux/ADM_libraries/ADM_lavcodec/CMakeLists.txt
   branches/avidemux_2.4_branch/avidemux/ADM_libraries/ADM_lavcodec/ppc/dsputil_altivec.c
   branches/avidemux_2.4_branch/avidemux/ADM_libraries/ADM_lavcodec/ppc/dsputil_altivec.h
   branches/avidemux_2.4_branch/avidemux/ADM_libraries/ADM_lavcodec/ppc/dsputil_ppc.c
   branches/avidemux_2.4_branch/avidemux/ADM_libraries/ADM_lavcodec/ppc/dsputil_ppc.h
   branches/avidemux_2.4_branch/avidemux/ADM_libraries/ADM_lavcodec/ppc/fdct_altivec.c
   branches/avidemux_2.4_branch/avidemux/ADM_libraries/ADM_lavcodec/ppc/fft_altivec.c
   branches/avidemux_2.4_branch/avidemux/ADM_libraries/ADM_lavcodec/ppc/gmc_altivec.c
   branches/avidemux_2.4_branch/avidemux/ADM_libraries/ADM_lavcodec/ppc/idct_altivec.c
   branches/avidemux_2.4_branch/avidemux/ADM_libraries/ADM_lavcodec/ppc/mpegvideo_altivec.c
   branches/avidemux_2.4_branch/avidemux/ADM_libraries/ADM_lavcodec/ppc/mpegvideo_ppc.c
   branches/avidemux_2.4_branch/avidemux/ADM_libraries/ADM_lavcodec/ppc/snow_altivec.c
   branches/avidemux_2.4_branch/avidemux/ADM_libraries/ADM_libmpeg2enc/CMakeLists.txt
   branches/avidemux_2.4_branch/config.h.cmake
Log:
[AltiVec] update libavcodec's AltiVec code + add AltiVec support for CMake build

Modified: branches/avidemux_2.4_branch/ConfigureChecks.cmake
===================================================================
--- branches/avidemux_2.4_branch/ConfigureChecks.cmake	2007-12-03 12:38:06 UTC (rev 3719)
+++ branches/avidemux_2.4_branch/ConfigureChecks.cmake	2007-12-04 00:19:48 UTC (rev 3720)
@@ -1,525 +1,543 @@
-########################################
-# CPU and Host
-########################################
-MESSAGE(STATUS &quot;Checking CPU and OS&quot;)
-INCLUDE(CMakeDetermineSystem)
-
-IF (WIN32)
-	SET(ADM_OS_WINDOWS 1)
-
-	IF (${CMAKE_SYSTEM_PROCESSOR} STREQUAL &quot;x86&quot;)
-		SET(ADM_CPU_X86 1)
-	ENDIF (${CMAKE_SYSTEM_PROCESSOR} STREQUAL &quot;x86&quot;)
-ELSEIF (APPLE)
-	SET(ADM_OS_APPLE 1)
-
-	IF (${CMAKE_SYSTEM_PROCESSOR} STREQUAL &quot;i386&quot;)
-		SET(ADM_CPU_X86 1)
-	ELSEIF (${CMAKE_SYSTEM_PROCESSOR} STREQUAL &quot;powerpc&quot; OR ${CMAKE_SYSTEM_PROCESSOR} STREQUAL &quot;Power Macintosh&quot;)
-		SET(ADM_CPU_PPC 1)
-	ENDIF (${CMAKE_SYSTEM_PROCESSOR} STREQUAL &quot;i386&quot;)
-ELSEIF (UNIX)
-	SET(ADM_OS_UNIX 1)
-
-	IF (${CMAKE_SYSTEM_PROCESSOR} STREQUAL &quot;i586&quot; OR ${CMAKE_SYSTEM_PROCESSOR} STREQUAL &quot;i686&quot;)
-		SET(ADM_CPU_X86 1)
-	ELSEIF (${CMAKE_SYSTEM_PROCESSOR} STREQUAL &quot;x86_64&quot; OR ${CMAKE_SYSTEM_PROCESSOR} STREQUAL &quot;amd64&quot;)
-		SET(ADM_CPU_X86_64 1)
-	ELSEIF (${CMAKE_SYSTEM_PROCESSOR} STREQUAL &quot;ppc&quot;)
-		SET(ADM_CPU_PPC 1)
-	ENDIF (${CMAKE_SYSTEM_PROCESSOR} STREQUAL &quot;i586&quot; OR ${CMAKE_SYSTEM_PROCESSOR} STREQUAL &quot;i686&quot;)
-ENDIF (WIN32)
-
-# Various defines (needs to be removed from Avidemux code one day and be library specific...)
-IF (ADM_CPU_X86)
-	SET(ARCH_X86 1)
-	SET(ARCH_X86_32 1)
-ELSEIF (ADM_CPU_X86_64)
-	SET(ARCH_X86 1)
-	SET(ARCH_X86_64 1)
-	SET(ARCH_64_BITS 1)
-ELSEIF (ADM_CPU_PPC)
-	SET(ADM_BIG_ENDIAN 1)
-	SET(WORDS_BIGENDIAN 1)
-ENDIF (ADM_CPU_X86)
-
-########################################
-# Include CMake scripts
-########################################
-INCLUDE(CheckIncludeFiles)
-INCLUDE(CheckSymbolExists)
-INCLUDE(CheckFunctionExists)
-INCLUDE(CheckLibraryExists)
-INCLUDE(lavcodec)
-INCLUDE(adm_checkHeaderLib)
-INCLUDE(adm_compile)
-
-########################################
-# Mangling
-########################################
-IF (ADM_OS_WINDOWS OR ADM_OS_APPLE)
-	SET(CYG_MANGLING 1)
-ENDIF (ADM_OS_WINDOWS OR ADM_OS_APPLE)
-
-########################################
-# Avidemux OS specific tweaks
-########################################
-IF (ADM_OS_WINDOWS)
-	SET(ADM_WIN32 1)		# needs to be removed one day...
-	ADD_DEFINITIONS(-mms-bitfields -mno-cygwin)
-ELSE (ADM_OS_WINDOWS)
-	IF (ADM_OS_APPLE)
-		SET(CFLAGS_ORIG $ENV{CFLAGS})
-		SET(CXXFLAGS_ORIG $ENV{CXXFLAGS})
-		
-		SET(ENV{CFLAGS} &quot;-I/opt/local/include -L/opt/local/lib $ENV{CFLAGS}&quot;)
-		SET(ENV{CXXFLAGS} &quot;-I/opt/local/include -L/opt/local/lib $ENV{CXXFLAGS}&quot;)
-
-		LINK_DIRECTORIES(/opt/local/lib)
-
-		SET(ADM_BSD_FAMILY 1)
-	ENDIF (ADM_OS_APPLE)
-
-	CHECK_FUNCTION_EXISTS(chmod HAVE_CHMOD)         # __homedir/homedir.cpp, gpg/gpg.cpp
-ENDIF (ADM_OS_WINDOWS)
-
-# Jog shuttle is only available on linux due to its interface
-IF(CMAKE_SYSTEM_NAME STREQUAL &quot;Linux&quot;)
-        SET(USE_JOG 1)
-ENDIF(CMAKE_SYSTEM_NAME STREQUAL &quot;Linux&quot;)
-
-########################################
-# Standard Avidemux defines
-########################################
-SET(VERSION 2.4)
-SET(PACKAGE_VERSION 2.4)
-
-SET(HAVE_BUILTIN_VECTOR 1)
-SET(HAVE_AUDIO 1)
-
-SET(USE_MP3 1)
-SET(USE_AC3 1)
-SET(USE_FFMPEG 1)
-SET(USE_MJPEG 1)
-SET(USE_LIBXML2 1)
-
-########################################
-# Check functions, includes, symbols
-########################################
-CHECK_FUNCTION_EXISTS(gettimeofday HAVE_GETTIMEOFDAY)
-
-CHECK_INCLUDE_FILES(inttypes.h      HAVE_INTTYPES_H)                    # simapi.h
-CHECK_INCLUDE_FILES(stddef.h        HAVE_STDDEF_H)                      # simapi.h
-CHECK_INCLUDE_FILES(stdint.h        HAVE_STDINT_H)                      # simapi.h
-CHECK_INCLUDE_FILES(stdlib.h        HAVE_STDLIB_H)                      # simapi.h
-CHECK_INCLUDE_FILES(string.h        HAVE_STRING_H)                      # _core/libintl.cpp
-CHECK_INCLUDE_FILES(sys/stat.h      HAVE_SYS_STAT_H)                    # gpg/gpg.cpp
-CHECK_INCLUDE_FILES(sys/types.h     HAVE_SYS_TYPES_H)                   # simapi.h
-CHECK_INCLUDE_FILES(unistd.h        HAVE_UNISTD_H)                      # simapi.h
-CHECK_INCLUDE_FILES(malloc.h        HAVE_MALLOC_H)                      # simapi.h
-
-CHECK_SYMBOL_EXISTS(strcasecmp  &quot;strings.h&quot;         HAVE_STRCASECMP)    # simapi.h, various
-
-########################################
-# LibMad
-########################################
-IF (ADM_CPU_X86)
-	SET(FPM_INTEL 1)
-ELSEIF (ADM_CPU_X86_64)
-	SET(FPM_DEFAULT 1)
-ELSEIF (ADM_CPU_PPC)
-	SET(FPM_PPC 1)
-ENDIF (ADM_CPU_X86)
-
-########################################
-# Libavcodec
-########################################
-SET_LAVCODEC_FLAGS()
-
-########################################
-# LibMpeg2Dec
-########################################
-SET(ACCEL_DETECT 1)
-
-########################################
-# Gettext
-########################################
-MESSAGE(STATUS &quot;&lt;Checking gettext&gt;&quot;)
-MESSAGE(STATUS &quot;&lt;****************&gt;&quot;)
-
-IF (NO_NLS)
-	MESSAGE(STATUS &quot;&lt;disabled per request&gt;&quot;)
-ELSE (NO_NLS)
-	FIND_PATH(LIBINTL_H_DIR libintl.h $ENV{CXXFLAGS})
-	MESSAGE(STATUS &quot;libintl Header Path: ${LIBINTL_H_DIR}&quot;)
-
-	IF (NOT LIBINTL_H_DIR STREQUAL &quot;LIBINTL_H-NOTFOUND&quot;)
-		FIND_LIBRARY(LIBINTL_LIB_DIR intl $ENV{CXXFLAGS})
-		MESSAGE(STATUS &quot;libintl Library Path: ${LIBINTL_LIB_DIR}&quot;)
-
-		# Try linking without -lintl
-		ADM_COMPILE(gettext.cpp -I${LIBINTL_H_DIR} &quot;&quot; WITHOUT_LIBINTL outputWithoutLibintl)
-		
-		IF (WITHOUT_LIBINTL)
-			SET(HAVE_GETTEXT 1)
-			MESSAGE(STATUS &quot;Ok, No lib needed (${ADM_GETTEXT_LIB})&quot;)
-		ELSE (WITHOUT_LIBINTL)
-			ADM_COMPILE(gettext.cpp -I${LIBINTL_H_DIR} ${LIBINTL_LIB_DIR} WITH_LIBINTL outputWithLibintl)
-			
-			IF (WITH_LIBINTL)
-				SET(ADM_GETTEXT_LIB ${LIBINTL_LIB_DIR})
-				SET(HAVE_GETTEXT 1)
-				
-				MESSAGE(STATUS &quot;Ok, libintl needed&quot;)
-			ELSE (WITH_LIBINTL)
-				MESSAGE(STATUS &quot;Does not work, without ${outputWithoutLibintl}&quot;)
-				MESSAGE(STATUS &quot;Does not work, with ${outputWithLibintl}&quot;)
-			ENDIF (WITH_LIBINTL)
-		ENDIF (WITHOUT_LIBINTL)
-	ENDIF (NOT LIBINTL_H_DIR STREQUAL &quot;LIBINTL_H-NOTFOUND&quot;)
-
-	IF (HAVE_GETTEXT)
-		SET(ADM_GETTEXT_INCLUDE -I${LIBINTL_H_DIR})
-	ENDIF(HAVE_GETTEXT)
-ENDIF (NO_NLS)
-
-########################################
-# Locale
-########################################
-SET(ADM_LOCALE &quot;${CMAKE_INSTALL_PREFIX}/share/locale&quot;)
-
-########################################
-# ALSA
-########################################
-IF (ADM_OS_UNIX)
-	MESSAGE(STATUS &quot;&lt;Checking for ALSA&gt;&quot;)
-	MESSAGE(STATUS &quot;&lt;*****************&gt;&quot;)
-	IF (NO_ALSA)
-		MESSAGE(STATUS &quot;&lt;disabled per request&gt;&quot;)
-	ELSE (NO_ALSA)
-		INCLUDE(FindAlsa)
-		
-		IF (ALSA_FOUND)
-			ALSA_VERSION_STRING(alsaVersion)
-			
-			MESSAGE(&quot;Found alsa version :${alsaVersion}&quot;) 
-			MESSAGE(&quot;Found alsa lib     :${ASOUND_LIBRARY}&quot;)
-			
-			SET(ALSA_SUPPORT 1)
-			SET(ALSA_1_0_SUPPORT 1)
-		ENDIF (ALSA_FOUND)
-	ENDIF (NO_ALSA)
-ENDIF (ADM_OS_UNIX)
-
-########################################
-# SDL
-########################################
-MESSAGE(STATUS &quot;&lt;Checking for SDL&gt;&quot;)
-MESSAGE(STATUS &quot;&lt;*****************&gt;&quot;)
-
-INCLUDE(admSDL)
-
-IF (NO_SDL)
-	MESSAGE(STATUS &quot;&lt;disabled per request&gt;&quot;)
-ELSE (NO_SDL)
-	include(FindSDL)
-
-	IF (SDL_FOUND)
-		SET(USE_SDL 1)
-		
-		MESSAGE(STATUS &quot;Found&quot;)
-	ELSE (SDL_FOUND)
-		MESSAGE(STATUS &quot;Not Found&quot;)
-	ENDIF (SDL_FOUND)
-	
-	MESSAGE(STATUS &quot;Flags: -I${SDL_INCLUDE_DIR}&quot;)
-	MESSAGE(STATUS &quot;Libraries: ${SDL_LIBRARY}&quot;)
-ENDIF (NO_SDL)
-
-########################################
-# FONTCONFIG
-########################################
-ADM_CHECK_HL(FontConfig fontconfig/fontconfig.h fontconfig FcStrSetCreate USE_FONTCONFIG)
-
-IF (USE_FONTCONFIG)
-	SET(HAVE_FONTCONFIG 1)
-ENDIF (USE_FONTCONFIG)
-
-########################################
-# Xvideo
-########################################
-IF (NOT NO_XV AND NOT ADM_OS_WINDOWS)
-	SET(CMAKE_REQUIRED_FLAGS &quot;-include X11/Xlib.h&quot;)
-	SET(CMAKE_REQUIRED_LIBRARIES &quot;${X11_LIBRARIES}&quot;)
-	SET(CMAKE_REQUIRED_INCLUDE &quot;${X11_INCLUDE_DIR}&quot;)
-
-	ADM_CHECK_HL(Xvideo X11/extensions/Xvlib.h Xv XvShmPutImage USE_XV)
-	
-	MESSAGE(STATUS &quot;Flags: ${CMAKE_REQUIRED_FLAGS} ${X11_INCLUDE_DIR}&quot;)
-	MESSAGE(STATUS &quot;Libraries: ${X11_LIBRARIES}&quot;)
-
-	SET(CMAKE_REQUIRED_FLAGS)
-	SET(CMAKE_REQUIRED_LIBRARIES)
-	SET(CMAKE_REQUIRED_INCLUDE)
-ENDIF (NOT NO_XV AND NOT ADM_OS_WINDOWS)
-
-########################################
-# OSS
-########################################
-IF (NOT ADM_OS_WINDOWS)
-	MESSAGE(STATUS &quot;&lt;Checking for OSS&gt;&quot;)
-	MESSAGE(STATUS &quot;&lt;*****************&gt;&quot;)
-	
-	IF (NO_OSS)
-		MESSAGE(STATUS &quot;&lt;disabled per request&gt;&quot;)
-	ELSE (NO_OSS)
-		CHECK_INCLUDE_FILES(sys/soundcard.h OSS_SUPPORT)
-		
-		IF (OSS_SUPPORT)
-			MESSAGE(STATUS &quot;Found&quot;)
-		ELSE (OSS_SUPPORT)
-			MESSAGE(STATUS &quot;Not found&quot;)
-		ENDIF (OSS_SUPPORT)
-	ENDIF (NO_OSS)
-ENDIF (NOT ADM_OS_WINDOWS)
-
-########################################
-# ARTS
-########################################
-IF (NOT ADM_OS_WINDOWS)
-	INCLUDE(FindArts)
-ENDIF (NOT ADM_OS_WINDOWS)
-
-########################################
-# ESD
-########################################
-IF (NOT ADM_OS_WINDOWS)
-	ADM_CHECK_HL(Esd esd.h  esd esd_close USE_ESD)
-ENDIF (NOT ADM_OS_WINDOWS)
-
-########################################
-# JACK
-########################################
-IF (NOT ADM_OS_WINDOWS)
-	ADM_CHECK_HL(Jack jack/jack.h  jack jack_client_close USE_JACK)
-ENDIF (NOT ADM_OS_WINDOWS)
-
-########################################
-# Aften
-########################################
-IF (ADM_OS_WINDOWS)
-	SET(CMAKE_REQUIRED_FLAGS &quot;-lm -lpthreadGC2&quot;)
-ELSE (ADM_OS_WINDOWS)
-	SET(CMAKE_REQUIRED_FLAGS &quot;-lm -lpthread&quot;)
-ENDIF (ADM_OS_WINDOWS)
-
-ADM_CHECK_HL(Aften aften/aften.h aften aften_encode_init USE_AFTEN)
-
-IF (USE_AFTEN)
-	FIND_LIBRARY(AFTEN_LIB_PATH NAMES aften $ENV{CXXFLAGS})
-
-	TRY_RUN(AFTEN_TEST_RUN_RESULT
-		AFTEN_TEST_COMPILE_RESULT
-		${CMAKE_BINARY_DIR}
-		&quot;${CMAKE_SOURCE_DIR}/cmake_compile_check/aften_check.cpp&quot;
-		CMAKE_FLAGS -DLINK_LIBRARIES=${AFTEN_LIB_PATH})
-
-	IF (AFTEN_TEST_RUN_RESULT EQUAL 8)
-		MESSAGE(STATUS &quot;Aften Version: 0.0.8&quot;)
-		SET(USE_AFTEN_08 1)
-	ELSEIF (AFTEN_TEST_RUN_RESULT EQUAL 7)
-		MESSAGE(STATUS &quot;Aften Version: 0.07&quot;)
-		SET(USE_AFTEN_07 1)
-	ELSE (AFTEN_TEST_RUN_RESULT EQUAL 8)
-		MESSAGE(STATUS &quot;Warning: Unable to determine Aften version - support for Aften will be turned off&quot;)
-		SET(USE_AFTEN 0)
-	ENDIF (AFTEN_TEST_RUN_RESULT EQUAL 8)
-ENDIF (USE_AFTEN)
-
-SET(CMAKE_REQUIRED_FLAGS &quot;&quot;)
-
-########################################
-# Secret Rabbit Code
-########################################
-ADM_CHECK_HL(libsamplerate samplerate.h samplerate src_get_version USE_SRC)
-
-########################################
-# ICONV
-########################################
-MESSAGE(STATUS &quot;&lt;Checking for iconv.h&gt;&quot;)
-MESSAGE(STATUS &quot;&lt;******************************&gt;&quot;)
-
-CHECK_INCLUDE_FILES(iconv.h HAVE_ICONV_H)
-
-IF (NOT HAVE_ICONV_H)
-	MESSAGE(FATAL &quot;iconv.h not found&quot;)
-ENDIF (NOT HAVE_ICONV_H)
-
-# need libiconv ?
-CHECK_LIBRARY_EXISTS(iconv libiconv &quot;&quot; LINK_WITH_ICONV)
-
-IF (LINK_WITH_ICONV)
-	SET(NEED_LIBICONV 1)
-	MESSAGE(STATUS &quot;libiconv found, probably needed&quot;)
-ELSE (LINK_WITH_ICONV)
-	MESSAGE(STATUS &quot;libiconv not found, probably not needed&quot;)
-ENDIF (LINK_WITH_ICONV)
-
-MESSAGE(STATUS &quot;&lt;Checking if iconv needs const&gt;&quot;)
-
-IF (NEED_LIBICONV)
-	ADM_COMPILE_WITH_WITHOUT(iconv_check.cpp &quot;-DICONV_NEED_CONST&quot; &quot;-liconv&quot; ICONV_WITH)
-ELSE (NEED_LIBICONV)
-	ADM_COMPILE_WITH_WITHOUT(iconv_check.cpp &quot;-DICONV_NEED_CONST&quot; &quot;-lm&quot; ICONV_WITH)
-ENDIF (NEED_LIBICONV)
-
-IF (ICONV_WITH)
-	MESSAGE(STATUS &quot;Yes&quot;)
-	SET(ICONV_NEED_CONST 1)
-ELSE (ICONV_WITH)
-    MESSAGE(STATUS &quot;No&quot;)
-ENDIF(ICONV_WITH)
-
-########################################
-# LAME
-########################################
-SET(CMAKE_REQUIRED_LIBRARIES &quot;-lm&quot;)
-ADM_CHECK_HL(Lame lame/lame.h mp3lame lame_init HAVE_LIBMP3LAME)
-SET(CMAKE_REQUIRED_LIBRARIES)
-
-########################################
-# Xvid
-########################################
-ADM_CHECK_HL(Xvid xvid.h xvidcore xvid_plugin_single USE_XVID_4)
-
-########################################
-# AMR_NB
-########################################
-IF (USE_LATE_BINDING)
-	CHECK_INCLUDE_FILES(amrnb/interf_dec.h USE_AMR_NB)
-ELSE (USE_LATE_BINDING)
-	ADM_CHECK_HL(AMRNB amrnb/interf_dec.h amrnb GP3Decoder_Interface_Decode USE_AMR_NB)
-ENDIF (USE_LATE_BINDING)
-
-IF (USE_AMR_NB)
-	SET(CONFIG_AMR_NB 1)
-ENDIF (USE_AMR_NB)
-
-########################################
-# Libdca
-########################################
-SET(CMAKE_REQUIRED_FLAGS &quot;-include stdint.h&quot;)
-SET(CMAKE_REQUIRED_LIBRARIES &quot;-lm&quot;)
-
-IF (USE_LATE_BINDING)
-	CHECK_INCLUDE_FILES(dts.h USE_LIBDCA)
-ELSE (USE_LATE_BINDING)
-	ADM_CHECK_HL(libdca dts.h dts dts_init USE_LIBDCA)
-ENDIF (USE_LATE_BINDING)
-
-SET(CMAKE_REQUIRED_LIBRARIES)
-SET(CMAKE_REQUIRED_FLAGS)
-
-########################################
-# X264
-########################################
-SET(CMAKE_REQUIRED_FLAGS &quot;-include stdint.h&quot;)
-
-IF (ADM_OS_WINDOWS)
-	SET(CMAKE_REQUIRED_LIBRARIES &quot;-lm -lpthreadGC2&quot;)
-ELSE (ADM_OS_WINDOWS)
-	SET(CMAKE_REQUIRED_LIBRARIES &quot;-lm -lpthread&quot;)
-ENDIF (ADM_OS_WINDOWS)
-
-ADM_CHECK_HL(x264 x264.h x264 x264_encoder_open USE_X264)
-SET(CMAKE_REQUIRED_FLAGS)
-SET(CMAKE_REQUIRED_LIBRARIES)
-
-########################################
-# PNG
-########################################
-ADM_CHECK_HL(libPNG png.h png png_malloc USE_PNG)
-
-########################################
-# FAAD
-########################################
-ADM_CHECK_HL(FAAD faad.h faad faacDecInit USE_FAAD_P)
-
-IF(NOT USE_FAAD_P)
-	MESSAGE(STATUS &quot;Trying neaac variant&quot;)
-	ADM_CHECK_HL(NeAAC faad.h faad NeAACDecInit USE_FAAD_A)
-ENDIF(NOT USE_FAAD_P)
-
-IF(USE_FAAD_P OR USE_FAAD_A)
-	SET(USE_FAAD  1)
-ENDIF(USE_FAAD_P OR USE_FAAD_A)
-
-# See if we need old FAAD or new
-IF (USE_FAAD)
-	FIND_PATH(FAAD_H_DIR faad.h $ENV{CXXFLAGS})
-	FIND_LIBRARY(FAAD_LIB_DIR faad $ENV{CXXFLAGS})
-	
-	MESSAGE(STATUS &quot;&lt;Checking if faad needs old proto&gt;&quot;)
-	ADM_COMPILE_WITH_WITHOUT(faad_check.cpp &quot;-DOLD_FAAD_PROTO -I${FAAD_H_DIR}&quot; &quot;${FAAD_LIB_DIR}&quot; FAAD_WITH)
-	
-	IF(FAAD_WITH)
-		MESSAGE(STATUS &quot;Yes&quot;)
-		SET(OLD_FAAD_PROTO 1)
-	ELSE (FAAD_WITH)
-		MESSAGE(STATUS &quot;No&quot;)
-	ENDIF (FAAD_WITH)
-ENDIF(USE_FAAD)
-
-########################################
-# FAAC
-########################################
-SET(CMAKE_REQUIRED_FLAGS &quot;-include stdint.h&quot;)
-ADM_CHECK_HL(FAAC faac.h faac faacEncClose USE_FAAC)
-SET(CMAKE_REQUIRED_FLAGS)
-
-########################################
-# FreeType
-########################################
-IF (FT_FOUND)
-	SET(USE_FREETYPE 1)
-ENDIF (FT_FOUND)
-
-########################################
-# Vorbis
-########################################
-ADM_CHECK_HL(Vorbis vorbis/vorbisenc.h vorbis vorbis_info_init USE_VORBIS1)
-ADM_CHECK_HL(Vorbis vorbis/vorbisenc.h vorbisenc vorbis_encode_init USE_VORBIS2)
-
-IF (USE_VORBIS1 AND USE_VORBIS2)
-	SET(USE_VORBIS 1)
-ENDIF (USE_VORBIS1 AND USE_VORBIS2)
-
-########################################
-# End test
-########################################
-ADM_CHECK_HL(Invalid dummy_header.h dummy_libxyz dummy_func_tyu DUMMY_TEST)
-ADM_CHECK_HL(Invalid stdio.h dummy_libxyz dummy_func_tyu DUMMY_TEST2)
-
-IF (DUMMY_TEST OR DUMMY_TEST2)
-	MESSAGE(FATAL &quot;This test should have failed!!!&quot;)
-	MESSAGE(FATAL &quot;This test should have failed!!!&quot;)
-	MESSAGE(FATAL &quot;This test should have failed!!!&quot;)
-ENDIF (DUMMY_TEST OR DUMMY_TEST2)
-
-INCLUDE(adm_log)
-
-IF (NOT CMAKE_BUILD_TYPE)
-	SET(CMAKE_BUILD_TYPE &quot;Release&quot;)
-ENDIF (NOT CMAKE_BUILD_TYPE)
-
-IF (CMAKE_BUILD_TYPE STREQUAL &quot;Debug&quot;)
-	MESSAGE(STATUS &quot;** DEBUG BUILD (${CMAKE_BUILD_TYPE})**&quot;)
-	
-	SET(ADM_DEBUG 1)
-	ADD_DEFINITIONS(-DADM_DEBUG)
-ELSE (CMAKE_BUILD_TYPE STREQUAL &quot;Debug&quot;)
-	MESSAGE(STATUS &quot;** RELEASE BUILD (${CMAKE_BUILD_TYPE})**&quot;)
-ENDIF(CMAKE_BUILD_TYPE STREQUAL &quot;Debug&quot;)
-
-MESSAGE(&quot;LINK_FLAGS ${CMAKE_LD_FLAGS}&quot;)
-# EOF
+########################################
+# CPU and Host
+########################################
+MESSAGE(STATUS &quot;Checking CPU and OS&quot;)
+INCLUDE(CMakeDetermineSystem)
+
+IF (WIN32)
+	SET(ADM_OS_WINDOWS 1)
+
+	IF (${CMAKE_SYSTEM_PROCESSOR} STREQUAL &quot;x86&quot;)
+		SET(ADM_CPU_X86 1)
+	ENDIF (${CMAKE_SYSTEM_PROCESSOR} STREQUAL &quot;x86&quot;)
+ELSEIF (APPLE)
+	SET(ADM_OS_APPLE 1)
+
+	IF (${CMAKE_SYSTEM_PROCESSOR} STREQUAL &quot;i386&quot;)
+		SET(ADM_CPU_X86 1)
+	ELSEIF (${CMAKE_SYSTEM_PROCESSOR} STREQUAL &quot;powerpc&quot; OR ${CMAKE_SYSTEM_PROCESSOR} STREQUAL &quot;Power Macintosh&quot;)
+		SET(ADM_CPU_PPC 1)
+	ENDIF (${CMAKE_SYSTEM_PROCESSOR} STREQUAL &quot;i386&quot;)
+ELSEIF (UNIX)
+	SET(ADM_OS_UNIX 1)
+
+	IF (${CMAKE_SYSTEM_PROCESSOR} STREQUAL &quot;i586&quot; OR ${CMAKE_SYSTEM_PROCESSOR} STREQUAL &quot;i686&quot;)
+		SET(ADM_CPU_X86 1)
+	ELSEIF (${CMAKE_SYSTEM_PROCESSOR} STREQUAL &quot;x86_64&quot; OR ${CMAKE_SYSTEM_PROCESSOR} STREQUAL &quot;amd64&quot;)
+		SET(ADM_CPU_X86_64 1)
+	ELSEIF (${CMAKE_SYSTEM_PROCESSOR} STREQUAL &quot;ppc&quot;)
+		SET(ADM_CPU_PPC 1)
+	ENDIF (${CMAKE_SYSTEM_PROCESSOR} STREQUAL &quot;i586&quot; OR ${CMAKE_SYSTEM_PROCESSOR} STREQUAL &quot;i686&quot;)
+ENDIF (WIN32)
+
+# Various defines (needs to be removed from Avidemux code one day and be library specific...)
+IF (ADM_CPU_X86)
+	SET(ARCH_X86 1)
+	SET(ARCH_X86_32 1)
+ELSEIF (ADM_CPU_X86_64)
+	SET(ARCH_X86 1)
+	SET(ARCH_X86_64 1)
+	SET(ARCH_64_BITS 1)
+ELSEIF (ADM_CPU_PPC)
+	OPTION(ALTIVEC &quot;&quot; ON)
+
+	SET(ADM_BIG_ENDIAN 1)
+	SET(WORDS_BIGENDIAN 1)
+
+	IF (ALTIVEC)
+		SET(BUILD_ALTIVEC 1)
+		SET(ARCH_PPC 1)
+		SET(ARCH_POWERPC 1)
+		SET(USE_ALTIVEC 1)
+		SET(HAVE_ALTIVEC 1)
+
+		SET(CMAKE_C_FLAGS &quot;${CMAKE_C_FLAGS} -mabi=altivec -maltivec&quot;)
+		SET(CMAKE_CXX_FLAGS &quot;${CMAKE_CXX_FLAGS} -mabi=altivec -maltivec&quot;)
+
+		IF (ADM_OS_APPLE)
+			SET(CMAKE_C_FLAGS &quot;${CMAKE_C_FLAGS} -faltivec -force_cpusubtype_ALL&quot;)
+			SET(CMAKE_CXX_FLAGS &quot;${CMAKE_CXX_FLAGS} -faltivec -force_cpusubtype_ALL&quot;)
+		ENDIF (ADM_OS_APPLE)
+	ENDIF (ALTIVEC)
+ENDIF (ADM_CPU_X86)
+
+########################################
+# Include CMake scripts
+########################################
+INCLUDE(CheckIncludeFiles)
+INCLUDE(CheckSymbolExists)
+INCLUDE(CheckFunctionExists)
+INCLUDE(CheckLibraryExists)
+INCLUDE(lavcodec)
+INCLUDE(adm_checkHeaderLib)
+INCLUDE(adm_compile)
+
+########################################
+# Mangling
+########################################
+IF (ADM_OS_WINDOWS OR ADM_OS_APPLE)
+	SET(CYG_MANGLING 1)
+ENDIF (ADM_OS_WINDOWS OR ADM_OS_APPLE)
+
+########################################
+# Avidemux OS specific tweaks
+########################################
+IF (ADM_OS_WINDOWS)
+	SET(ADM_WIN32 1)		# needs to be removed one day...
+	ADD_DEFINITIONS(-mms-bitfields -mno-cygwin)
+ELSE (ADM_OS_WINDOWS)
+	IF (ADM_OS_APPLE)
+		SET(CFLAGS_ORIG $ENV{CFLAGS})
+		SET(CXXFLAGS_ORIG $ENV{CXXFLAGS})
+		
+		SET(ENV{CFLAGS} &quot;-I/opt/local/include -L/opt/local/lib $ENV{CFLAGS}&quot;)
+		SET(ENV{CXXFLAGS} &quot;-I/opt/local/include -L/opt/local/lib $ENV{CXXFLAGS}&quot;)
+
+		LINK_DIRECTORIES(/opt/local/lib)
+
+		SET(ADM_BSD_FAMILY 1)
+	ENDIF (ADM_OS_APPLE)
+
+	CHECK_FUNCTION_EXISTS(chmod HAVE_CHMOD)         # __homedir/homedir.cpp, gpg/gpg.cpp
+ENDIF (ADM_OS_WINDOWS)
+
+# Jog shuttle is only available on linux due to its interface
+IF(CMAKE_SYSTEM_NAME STREQUAL &quot;Linux&quot;)
+        SET(USE_JOG 1)
+ENDIF(CMAKE_SYSTEM_NAME STREQUAL &quot;Linux&quot;)
+
+########################################
+# Standard Avidemux defines
+########################################
+SET(VERSION 2.4)
+SET(PACKAGE_VERSION 2.4)
+
+SET(HAVE_BUILTIN_VECTOR 1)
+SET(HAVE_AUDIO 1)
+
+SET(USE_MP3 1)
+SET(USE_AC3 1)
+SET(USE_FFMPEG 1)
+SET(USE_MJPEG 1)
+SET(USE_LIBXML2 1)
+
+########################################
+# Check functions, includes, symbols
+########################################
+CHECK_FUNCTION_EXISTS(gettimeofday HAVE_GETTIMEOFDAY)
+
+CHECK_INCLUDE_FILES(inttypes.h      HAVE_INTTYPES_H)                    # simapi.h
+CHECK_INCLUDE_FILES(stddef.h        HAVE_STDDEF_H)                      # simapi.h
+CHECK_INCLUDE_FILES(stdint.h        HAVE_STDINT_H)                      # simapi.h
+CHECK_INCLUDE_FILES(stdlib.h        HAVE_STDLIB_H)                      # simapi.h
+CHECK_INCLUDE_FILES(string.h        HAVE_STRING_H)                      # _core/libintl.cpp
+CHECK_INCLUDE_FILES(sys/stat.h      HAVE_SYS_STAT_H)                    # gpg/gpg.cpp
+CHECK_INCLUDE_FILES(sys/types.h     HAVE_SYS_TYPES_H)                   # simapi.h
+CHECK_INCLUDE_FILES(unistd.h        HAVE_UNISTD_H)                      # simapi.h
+CHECK_INCLUDE_FILES(malloc.h        HAVE_MALLOC_H)                      # simapi.h
+
+CHECK_SYMBOL_EXISTS(strcasecmp  &quot;strings.h&quot;         HAVE_STRCASECMP)    # simapi.h, various
+
+########################################
+# LibMad
+########################################
+IF (ADM_CPU_X86)
+	SET(FPM_INTEL 1)
+ELSEIF (ADM_CPU_X86_64)
+	SET(FPM_DEFAULT 1)
+ELSEIF (ADM_CPU_PPC)
+	SET(FPM_PPC 1)
+ENDIF (ADM_CPU_X86)
+
+########################################
+# Libavcodec
+########################################
+SET_LAVCODEC_FLAGS()
+
+########################################
+# LibMpeg2Dec
+########################################
+SET(ACCEL_DETECT 1)
+
+########################################
+# Gettext
+########################################
+MESSAGE(STATUS &quot;&lt;Checking gettext&gt;&quot;)
+MESSAGE(STATUS &quot;&lt;****************&gt;&quot;)
+
+IF (NO_NLS)
+	MESSAGE(STATUS &quot;&lt;disabled per request&gt;&quot;)
+ELSE (NO_NLS)
+	FIND_PATH(LIBINTL_H_DIR libintl.h $ENV{CXXFLAGS})
+	MESSAGE(STATUS &quot;libintl Header Path: ${LIBINTL_H_DIR}&quot;)
+
+	IF (NOT LIBINTL_H_DIR STREQUAL &quot;LIBINTL_H-NOTFOUND&quot;)
+		FIND_LIBRARY(LIBINTL_LIB_DIR intl $ENV{CXXFLAGS})
+		MESSAGE(STATUS &quot;libintl Library Path: ${LIBINTL_LIB_DIR}&quot;)
+
+		# Try linking without -lintl
+		ADM_COMPILE(gettext.cpp -I${LIBINTL_H_DIR} &quot;&quot; WITHOUT_LIBINTL outputWithoutLibintl)
+		
+		IF (WITHOUT_LIBINTL)
+			SET(HAVE_GETTEXT 1)
+			MESSAGE(STATUS &quot;Ok, No lib needed (${ADM_GETTEXT_LIB})&quot;)
+		ELSE (WITHOUT_LIBINTL)
+			ADM_COMPILE(gettext.cpp -I${LIBINTL_H_DIR} ${LIBINTL_LIB_DIR} WITH_LIBINTL outputWithLibintl)
+			
+			IF (WITH_LIBINTL)
+				SET(ADM_GETTEXT_LIB ${LIBINTL_LIB_DIR})
+				SET(HAVE_GETTEXT 1)
+				
+				MESSAGE(STATUS &quot;Ok, libintl needed&quot;)
+			ELSE (WITH_LIBINTL)
+				MESSAGE(STATUS &quot;Does not work, without ${outputWithoutLibintl}&quot;)
+				MESSAGE(STATUS &quot;Does not work, with ${outputWithLibintl}&quot;)
+			ENDIF (WITH_LIBINTL)
+		ENDIF (WITHOUT_LIBINTL)
+	ENDIF (NOT LIBINTL_H_DIR STREQUAL &quot;LIBINTL_H-NOTFOUND&quot;)
+
+	IF (HAVE_GETTEXT)
+		SET(ADM_GETTEXT_INCLUDE -I${LIBINTL_H_DIR})
+	ENDIF(HAVE_GETTEXT)
+ENDIF (NO_NLS)
+
+########################################
+# Locale
+########################################
+SET(ADM_LOCALE &quot;${CMAKE_INSTALL_PREFIX}/share/locale&quot;)
+
+########################################
+# ALSA
+########################################
+IF (ADM_OS_UNIX)
+	MESSAGE(STATUS &quot;&lt;Checking for ALSA&gt;&quot;)
+	MESSAGE(STATUS &quot;&lt;*****************&gt;&quot;)
+	IF (NO_ALSA)
+		MESSAGE(STATUS &quot;&lt;disabled per request&gt;&quot;)
+	ELSE (NO_ALSA)
+		INCLUDE(FindAlsa)
+		
+		IF (ALSA_FOUND)
+			ALSA_VERSION_STRING(alsaVersion)
+			
+			MESSAGE(&quot;Found alsa version :${alsaVersion}&quot;) 
+			MESSAGE(&quot;Found alsa lib     :${ASOUND_LIBRARY}&quot;)
+			
+			SET(ALSA_SUPPORT 1)
+			SET(ALSA_1_0_SUPPORT 1)
+		ENDIF (ALSA_FOUND)
+	ENDIF (NO_ALSA)
+ENDIF (ADM_OS_UNIX)
+
+########################################
+# SDL
+########################################
+MESSAGE(STATUS &quot;&lt;Checking for SDL&gt;&quot;)
+MESSAGE(STATUS &quot;&lt;*****************&gt;&quot;)
+
+INCLUDE(admSDL)
+
+IF (NO_SDL)
+	MESSAGE(STATUS &quot;&lt;disabled per request&gt;&quot;)
+ELSE (NO_SDL)
+	include(FindSDL)
+
+	IF (SDL_FOUND)
+		SET(USE_SDL 1)
+		
+		MESSAGE(STATUS &quot;Found&quot;)
+	ELSE (SDL_FOUND)
+		MESSAGE(STATUS &quot;Not Found&quot;)
+	ENDIF (SDL_FOUND)
+	
+	MESSAGE(STATUS &quot;Flags: -I${SDL_INCLUDE_DIR}&quot;)
+	MESSAGE(STATUS &quot;Libraries: ${SDL_LIBRARY}&quot;)
+ENDIF (NO_SDL)
+
+########################################
+# FONTCONFIG
+########################################
+ADM_CHECK_HL(FontConfig fontconfig/fontconfig.h fontconfig FcStrSetCreate USE_FONTCONFIG)
+
+IF (USE_FONTCONFIG)
+	SET(HAVE_FONTCONFIG 1)
+ENDIF (USE_FONTCONFIG)
+
+########################################
+# Xvideo
+########################################
+IF (NOT NO_XV AND NOT ADM_OS_WINDOWS)
+	SET(CMAKE_REQUIRED_FLAGS &quot;-include X11/Xlib.h&quot;)
+	SET(CMAKE_REQUIRED_LIBRARIES &quot;${X11_LIBRARIES}&quot;)
+	SET(CMAKE_REQUIRED_INCLUDE &quot;${X11_INCLUDE_DIR}&quot;)
+
+	ADM_CHECK_HL(Xvideo X11/extensions/Xvlib.h Xv XvShmPutImage USE_XV)
+	
+	MESSAGE(STATUS &quot;Flags: ${CMAKE_REQUIRED_FLAGS} ${X11_INCLUDE_DIR}&quot;)
+	MESSAGE(STATUS &quot;Libraries: ${X11_LIBRARIES}&quot;)
+
+	SET(CMAKE_REQUIRED_FLAGS)
+	SET(CMAKE_REQUIRED_LIBRARIES)
+	SET(CMAKE_REQUIRED_INCLUDE)
+ENDIF (NOT NO_XV AND NOT ADM_OS_WINDOWS)
+
+########################################
+# OSS
+########################################
+IF (NOT ADM_OS_WINDOWS)
+	MESSAGE(STATUS &quot;&lt;Checking for OSS&gt;&quot;)
+	MESSAGE(STATUS &quot;&lt;*****************&gt;&quot;)
+	
+	IF (NO_OSS)
+		MESSAGE(STATUS &quot;&lt;disabled per request&gt;&quot;)
+	ELSE (NO_OSS)
+		CHECK_INCLUDE_FILES(sys/soundcard.h OSS_SUPPORT)
+		
+		IF (OSS_SUPPORT)
+			MESSAGE(STATUS &quot;Found&quot;)
+		ELSE (OSS_SUPPORT)
+			MESSAGE(STATUS &quot;Not found&quot;)
+		ENDIF (OSS_SUPPORT)
+	ENDIF (NO_OSS)
+ENDIF (NOT ADM_OS_WINDOWS)
+
+########################################
+# ARTS
+########################################
+IF (NOT ADM_OS_WINDOWS)
+	INCLUDE(FindArts)
+ENDIF (NOT ADM_OS_WINDOWS)
+
+########################################
+# ESD
+########################################
+IF (NOT ADM_OS_WINDOWS)
+	ADM_CHECK_HL(Esd esd.h  esd esd_close USE_ESD)
+ENDIF (NOT ADM_OS_WINDOWS)
+
+########################################
+# JACK
+########################################
+IF (NOT ADM_OS_WINDOWS)
+	ADM_CHECK_HL(Jack jack/jack.h  jack jack_client_close USE_JACK)
+ENDIF (NOT ADM_OS_WINDOWS)
+
+########################################
+# Aften
+########################################
+IF (ADM_OS_WINDOWS)
+	SET(CMAKE_REQUIRED_FLAGS &quot;-lm -lpthreadGC2&quot;)
+ELSE (ADM_OS_WINDOWS)
+	SET(CMAKE_REQUIRED_FLAGS &quot;-lm -lpthread&quot;)
+ENDIF (ADM_OS_WINDOWS)
+
+ADM_CHECK_HL(Aften aften/aften.h aften aften_encode_init USE_AFTEN)
+
+IF (USE_AFTEN)
+	FIND_LIBRARY(AFTEN_LIB_PATH NAMES aften $ENV{CXXFLAGS})
+
+	TRY_RUN(AFTEN_TEST_RUN_RESULT
+		AFTEN_TEST_COMPILE_RESULT
+		${CMAKE_BINARY_DIR}
+		&quot;${CMAKE_SOURCE_DIR}/cmake_compile_check/aften_check.cpp&quot;
+		CMAKE_FLAGS -DLINK_LIBRARIES=${AFTEN_LIB_PATH})
+
+	IF (AFTEN_TEST_RUN_RESULT EQUAL 8)
+		MESSAGE(STATUS &quot;Aften Version: 0.0.8&quot;)
+		SET(USE_AFTEN_08 1)
+	ELSEIF (AFTEN_TEST_RUN_RESULT EQUAL 7)
+		MESSAGE(STATUS &quot;Aften Version: 0.07&quot;)
+		SET(USE_AFTEN_07 1)
+	ELSE (AFTEN_TEST_RUN_RESULT EQUAL 8)
+		MESSAGE(STATUS &quot;Warning: Unable to determine Aften version - support for Aften will be turned off&quot;)
+		SET(USE_AFTEN 0)
+	ENDIF (AFTEN_TEST_RUN_RESULT EQUAL 8)
+ENDIF (USE_AFTEN)
+
+SET(CMAKE_REQUIRED_FLAGS &quot;&quot;)
+
+########################################
+# Secret Rabbit Code
+########################################
+ADM_CHECK_HL(libsamplerate samplerate.h samplerate src_get_version USE_SRC)
+
+########################################
+# ICONV
+########################################
+MESSAGE(STATUS &quot;&lt;Checking for iconv.h&gt;&quot;)
+MESSAGE(STATUS &quot;&lt;******************************&gt;&quot;)
+
+CHECK_INCLUDE_FILES(iconv.h HAVE_ICONV_H)
+
+IF (NOT HAVE_ICONV_H)
+	MESSAGE(FATAL &quot;iconv.h not found&quot;)
+ENDIF (NOT HAVE_ICONV_H)
+
+# need libiconv ?
+CHECK_LIBRARY_EXISTS(iconv libiconv &quot;&quot; LINK_WITH_ICONV)
+
+IF (LINK_WITH_ICONV)
+	SET(NEED_LIBICONV 1)
+	MESSAGE(STATUS &quot;libiconv found, probably needed&quot;)
+ELSE (LINK_WITH_ICONV)
+	MESSAGE(STATUS &quot;libiconv not found, probably not needed&quot;)
+ENDIF (LINK_WITH_ICONV)
+
+MESSAGE(STATUS &quot;&lt;Checking if iconv needs const&gt;&quot;)
+
+IF (NEED_LIBICONV)
+	ADM_COMPILE_WITH_WITHOUT(iconv_check.cpp &quot;-DICONV_NEED_CONST&quot; &quot;-liconv&quot; ICONV_WITH)
+ELSE (NEED_LIBICONV)
+	ADM_COMPILE_WITH_WITHOUT(iconv_check.cpp &quot;-DICONV_NEED_CONST&quot; &quot;-lm&quot; ICONV_WITH)
+ENDIF (NEED_LIBICONV)
+
+IF (ICONV_WITH)
+	MESSAGE(STATUS &quot;Yes&quot;)
+	SET(ICONV_NEED_CONST 1)
+ELSE (ICONV_WITH)
+    MESSAGE(STATUS &quot;No&quot;)
+ENDIF(ICONV_WITH)
+
+########################################
+# LAME
+########################################
+SET(CMAKE_REQUIRED_LIBRARIES &quot;-lm&quot;)
+ADM_CHECK_HL(Lame lame/lame.h mp3lame lame_init HAVE_LIBMP3LAME)
+SET(CMAKE_REQUIRED_LIBRARIES)
+
+########################################
+# Xvid
+########################################
+ADM_CHECK_HL(Xvid xvid.h xvidcore xvid_plugin_single USE_XVID_4)
+
+########################################
+# AMR_NB
+########################################
+IF (USE_LATE_BINDING)
+	CHECK_INCLUDE_FILES(amrnb/interf_dec.h USE_AMR_NB)
+ELSE (USE_LATE_BINDING)
+	ADM_CHECK_HL(AMRNB amrnb/interf_dec.h amrnb GP3Decoder_Interface_Decode USE_AMR_NB)
+ENDIF (USE_LATE_BINDING)
+
+IF (USE_AMR_NB)
+	SET(CONFIG_AMR_NB 1)
+ENDIF (USE_AMR_NB)
+
+########################################
+# Libdca
+########################################
+SET(CMAKE_REQUIRED_FLAGS &quot;-include stdint.h&quot;)
+SET(CMAKE_REQUIRED_LIBRARIES &quot;-lm&quot;)
+
+IF (USE_LATE_BINDING)
+	CHECK_INCLUDE_FILES(dts.h USE_LIBDCA)
+ELSE (USE_LATE_BINDING)
+	ADM_CHECK_HL(libdca dts.h dts dts_init USE_LIBDCA)
+ENDIF (USE_LATE_BINDING)
+
+SET(CMAKE_REQUIRED_LIBRARIES)
+SET(CMAKE_REQUIRED_FLAGS)
+
+########################################
+# X264
+########################################
+SET(CMAKE_REQUIRED_FLAGS &quot;-include stdint.h&quot;)
+
+IF (ADM_OS_WINDOWS)
+	SET(CMAKE_REQUIRED_LIBRARIES &quot;-lm -lpthreadGC2&quot;)
+ELSE (ADM_OS_WINDOWS)
+	SET(CMAKE_REQUIRED_LIBRARIES &quot;-lm -lpthread&quot;)
+ENDIF (ADM_OS_WINDOWS)
+
+ADM_CHECK_HL(x264 x264.h x264 x264_encoder_open USE_X264)
+SET(CMAKE_REQUIRED_FLAGS)
+SET(CMAKE_REQUIRED_LIBRARIES)
+
+########################################
+# PNG
+########################################
+ADM_CHECK_HL(libPNG png.h png png_malloc USE_PNG)
+
+########################################
+# FAAD
+########################################
+ADM_CHECK_HL(FAAD faad.h faad faacDecInit USE_FAAD_P)
+
+IF(NOT USE_FAAD_P)
+	MESSAGE(STATUS &quot;Trying neaac variant&quot;)
+	ADM_CHECK_HL(NeAAC faad.h faad NeAACDecInit USE_FAAD_A)
+ENDIF(NOT USE_FAAD_P)
+
+IF(USE_FAAD_P OR USE_FAAD_A)
+	SET(USE_FAAD  1)
+ENDIF(USE_FAAD_P OR USE_FAAD_A)
+
+# See if we need old FAAD or new
+IF (USE_FAAD)
+	FIND_PATH(FAAD_H_DIR faad.h $ENV{CXXFLAGS})
+	FIND_LIBRARY(FAAD_LIB_DIR faad $ENV{CXXFLAGS})
+	
+	MESSAGE(STATUS &quot;&lt;Checking if faad needs old proto&gt;&quot;)
+	ADM_COMPILE_WITH_WITHOUT(faad_check.cpp &quot;-DOLD_FAAD_PROTO -I${FAAD_H_DIR}&quot; &quot;${FAAD_LIB_DIR}&quot; FAAD_WITH)
+	
+	IF(FAAD_WITH)
+		MESSAGE(STATUS &quot;Yes&quot;)
+		SET(OLD_FAAD_PROTO 1)
+	ELSE (FAAD_WITH)
+		MESSAGE(STATUS &quot;No&quot;)
+	ENDIF (FAAD_WITH)
+ENDIF(USE_FAAD)
+
+########################################
+# FAAC
+########################################
+SET(CMAKE_REQUIRED_FLAGS &quot;-include stdint.h&quot;)
+ADM_CHECK_HL(FAAC faac.h faac faacEncClose USE_FAAC)
+SET(CMAKE_REQUIRED_FLAGS)
+
+########################################
+# FreeType
+########################################
+IF (FT_FOUND)
+	SET(USE_FREETYPE 1)
+ENDIF (FT_FOUND)
+
+########################################
+# Vorbis
+########################################
+ADM_CHECK_HL(Vorbis vorbis/vorbisenc.h vorbis vorbis_info_init USE_VORBIS1)
+ADM_CHECK_HL(Vorbis vorbis/vorbisenc.h vorbisenc vorbis_encode_init USE_VORBIS2)
+
+IF (USE_VORBIS1 AND USE_VORBIS2)
+	SET(USE_VORBIS 1)
+ENDIF (USE_VORBIS1 AND USE_VORBIS2)
+
+########################################
+# End test
+########################################
+ADM_CHECK_HL(Invalid dummy_header.h dummy_libxyz dummy_func_tyu DUMMY_TEST)
+ADM_CHECK_HL(Invalid stdio.h dummy_libxyz dummy_func_tyu DUMMY_TEST2)
+
+IF (DUMMY_TEST OR DUMMY_TEST2)
+	MESSAGE(FATAL &quot;This test should have failed!!!&quot;)
+	MESSAGE(FATAL &quot;This test should have failed!!!&quot;)
+	MESSAGE(FATAL &quot;This test should have failed!!!&quot;)
+ENDIF (DUMMY_TEST OR DUMMY_TEST2)
+
+INCLUDE(adm_log)
+
+IF (NOT CMAKE_BUILD_TYPE)
+	SET(CMAKE_BUILD_TYPE &quot;Release&quot;)
+ENDIF (NOT CMAKE_BUILD_TYPE)
+
+IF (CMAKE_BUILD_TYPE STREQUAL &quot;Debug&quot;)
+	MESSAGE(STATUS &quot;** DEBUG BUILD (${CMAKE_BUILD_TYPE})**&quot;)
+	
+	SET(ADM_DEBUG 1)
+	ADD_DEFINITIONS(-DADM_DEBUG)
+ELSE (CMAKE_BUILD_TYPE STREQUAL &quot;Debug&quot;)
+	MESSAGE(STATUS &quot;** RELEASE BUILD (${CMAKE_BUILD_TYPE})**&quot;)
+ENDIF(CMAKE_BUILD_TYPE STREQUAL &quot;Debug&quot;)
+
+MESSAGE(&quot;LINK_FLAGS ${CMAKE_LD_FLAGS}&quot;)
+# EOF

Modified: branches/avidemux_2.4_branch/avidemux/ADM_libraries/ADM_lavcodec/CMakeLists.txt
===================================================================
--- branches/avidemux_2.4_branch/avidemux/ADM_libraries/ADM_lavcodec/CMakeLists.txt	2007-12-03 12:38:06 UTC (rev 3719)
+++ branches/avidemux_2.4_branch/avidemux/ADM_libraries/ADM_lavcodec/CMakeLists.txt	2007-12-04 00:19:48 UTC (rev 3720)
@@ -46,6 +46,15 @@
 			i386/snowdsp_mmx.c i386/fft_3dn.c 
 )
 ENDIF(ARCH_X86)
+
+IF (BUILD_ALTIVEC)
+	SET(${ADM_LIB}_SRCS  ${${ADM_LIB}_SRCS}
+		ppc/dsputil_altivec.c  ppc/dsputil_ppc.c  ppc/fdct_altivec.c  ppc/fft_altivec.c  ppc/float_altivec.c  ppc/gmc_altivec.c
+		ppc/h264_altivec.c  ppc/idct_altivec.c  ppc/int_altivec.c  ppc/mpegvideo_altivec.c  ppc/mpegvideo_ppc.c
+		ppc/snow_altivec.c  ppc/vc1dsp_altivec.c
+	)
+ENDIF (BUILD_ALTIVEC)
+
 ADD_LIBRARY(${ADM_LIB} STATIC ${${ADM_LIB}_SRCS})
 ADD_ADM_LIB(${ADM_LIB} ADM_libraries)
 

Modified: branches/avidemux_2.4_branch/avidemux/ADM_libraries/ADM_lavcodec/ppc/dsputil_altivec.c
===================================================================
--- branches/avidemux_2.4_branch/avidemux/ADM_libraries/ADM_lavcodec/ppc/dsputil_altivec.c	2007-12-03 12:38:06 UTC (rev 3719)
+++ branches/avidemux_2.4_branch/avidemux/ADM_libraries/ADM_lavcodec/ppc/dsputil_altivec.c	2007-12-04 00:19:48 UTC (rev 3720)
@@ -20,7 +20,7 @@
  * Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA
  */
 
-#include &quot;../dsputil.h&quot;
+#include &quot;dsputil.h&quot;
 
 #include &quot;gcc_fixes.h&quot;
 
@@ -56,7 +56,7 @@
 int sad16_x2_altivec(void *v, uint8_t *pix1, uint8_t *pix2, int line_size, int h)
 {
     int i;
-    int s __attribute__((aligned(16)));
+    DECLARE_ALIGNED_16(int, s);
     const_vector unsigned char zero = (const_vector unsigned char)vec_splat_u8(0);
     vector unsigned char *tv;
     vector unsigned char pix1v, pix2v, pix2iv, avgv, t5;
@@ -103,7 +103,7 @@
 int sad16_y2_altivec(void *v, uint8_t *pix1, uint8_t *pix2, int line_size, int h)
 {
     int i;
-    int s __attribute__((aligned(16)));
+    DECLARE_ALIGNED_16(int, s);
     const_vector unsigned char zero = (const_vector unsigned char)vec_splat_u8(0);
     vector unsigned char *tv;
     vector unsigned char pix1v, pix2v, pix3v, avgv, t5;
@@ -163,7 +163,7 @@
 int sad16_xy2_altivec(void *v, uint8_t *pix1, uint8_t *pix2, int line_size, int h)
 {
     int i;
-    int s __attribute__((aligned(16)));
+    DECLARE_ALIGNED_16(int, s);
     uint8_t *pix3 = pix2 + line_size;
     const_vector unsigned char zero = (const_vector unsigned char)vec_splat_u8(0);
     const_vector unsigned short two = (const_vector unsigned short)vec_splat_u16(2);
@@ -264,7 +264,7 @@
 int sad16_altivec(void *v, uint8_t *pix1, uint8_t *pix2, int line_size, int h)
 {
     int i;
-    int s __attribute__((aligned(16)));
+    DECLARE_ALIGNED_16(int, s);
     const_vector unsigned int zero = (const_vector unsigned int)vec_splat_u32(0);
     vector unsigned char perm1, perm2, *pix1v, *pix2v;
     vector unsigned char t1, t2, t3,t4, t5;
@@ -306,7 +306,7 @@
 int sad8_altivec(void *v, uint8_t *pix1, uint8_t *pix2, int line_size, int h)
 {
     int i;
-    int s __attribute__((aligned(16)));
+    DECLARE_ALIGNED_16(int, s);;
     const_vector unsigned int zero = (const_vector unsigned int)vec_splat_u32(0);
     vector unsigned char perm1, perm2, permclear, *pix1v, *pix2v;
     vector unsigned char t1, t2, t3,t4, t5;
@@ -351,7 +351,7 @@
 int pix_norm1_altivec(uint8_t *pix, int line_size)
 {
     int i;
-    int s __attribute__((aligned(16)));
+    DECLARE_ALIGNED_16(int, s);
     const_vector unsigned int zero = (const_vector unsigned int)vec_splat_u32(0);
     vector unsigned char *tv;
     vector unsigned char pixv;
@@ -387,7 +387,7 @@
 int sse8_altivec(void *v, uint8_t *pix1, uint8_t *pix2, int line_size, int h)
 {
     int i;
-    int s __attribute__((aligned(16)));
+    DECLARE_ALIGNED_16(int, s);
     const_vector unsigned int zero = (const_vector unsigned int)vec_splat_u32(0);
     vector unsigned char perm1, perm2, permclear, *pix1v, *pix2v;
     vector unsigned char t1, t2, t3,t4, t5;
@@ -443,7 +443,7 @@
 int sse16_altivec(void *v, uint8_t *pix1, uint8_t *pix2, int line_size, int h)
 {
     int i;
-    int s __attribute__((aligned(16)));
+    DECLARE_ALIGNED_16(int, s);
     const_vector unsigned int zero = (const_vector unsigned int)vec_splat_u32(0);
     vector unsigned char perm1, perm2, *pix1v, *pix2v;
     vector unsigned char t1, t2, t3,t4, t5;
@@ -495,7 +495,7 @@
     vector signed int sumdiffs;
 
     int i;
-    int s __attribute__((aligned(16)));
+    DECLARE_ALIGNED_16(int, s);
 
     sad = (vector unsigned int)vec_splat_u32(0);
 
@@ -1107,12 +1107,10 @@
       register vector signed short srcV, dstV;                          \
       register vector signed short but0, but1, but2, op1, op2, op3;     \
       src1 = vec_ld(stride * i, src);                                   \
-      if ((((stride * i) + (unsigned long)src) &amp; 0x0000000F) &gt; 8)       \
-        src2 = vec_ld((stride * i) + 16, src);                          \
+      src2 = vec_ld((stride * i) + 15, src);                            \
       srcO = vec_perm(src1, src2, vec_lvsl(stride * i, src));           \
       dst1 = vec_ld(stride * i, dst);                                   \
-      if ((((stride * i) + (unsigned long)dst) &amp; 0x0000000F) &gt; 8)       \
-        dst2 = vec_ld((stride * i) + 16, dst);                          \
+      dst2 = vec_ld((stride * i) + 15, dst);                            \
       dstO = vec_perm(dst1, dst2, vec_lvsl(stride * i, dst));           \
       /* promote the unsigned chars to signed shorts */                 \
       /* we're in the 8x8 function, we only care for the first 8 */     \

Modified: branches/avidemux_2.4_branch/avidemux/ADM_libraries/ADM_lavcodec/ppc/dsputil_altivec.h
===================================================================
--- branches/avidemux_2.4_branch/avidemux/ADM_libraries/ADM_lavcodec/ppc/dsputil_altivec.h	2007-12-03 12:38:06 UTC (rev 3719)
+++ branches/avidemux_2.4_branch/avidemux/ADM_libraries/ADM_lavcodec/ppc/dsputil_altivec.h	2007-12-04 00:19:48 UTC (rev 3720)
@@ -101,6 +101,17 @@
     h = vec_mergel (D2, H2); \
 } while (0)
 
+
+/** \brief loads unaligned vector \a *src with offset \a offset
+    and returns it */
+static inline vector unsigned char unaligned_load(int offset, uint8_t *src)
+{
+    register vector unsigned char first = vec_ld(offset, src);
+    register vector unsigned char second = vec_ld(offset+15, src);
+    register vector unsigned char mask = vec_lvsl(offset, src);
+    return vec_perm(first, second, mask);
+}
+
 #endif /* HAVE_ALTIVEC */
 
 #endif /* _DSPUTIL_ALTIVEC_ */

Deleted: branches/avidemux_2.4_branch/avidemux/ADM_libraries/ADM_lavcodec/ppc/dsputil_h264_altivec.c
===================================================================
--- branches/avidemux_2.4_branch/avidemux/ADM_libraries/ADM_lavcodec/ppc/dsputil_h264_altivec.c	2007-12-03 12:38:06 UTC (rev 3719)
+++ branches/avidemux_2.4_branch/avidemux/ADM_libraries/ADM_lavcodec/ppc/dsputil_h264_altivec.c	2007-12-04 00:19:48 UTC (rev 3720)
@@ -1,319 +0,0 @@
-/*
- * Copyright (c) 2004 Romain Dolbeau &lt;<A HREF="https://lists.berlios.de/mailman/listinfo/avidemux-svn-commit">romain at dolbeau.org</A>&gt;
- *
- * This library is free software; you can redistribute it and/or
- * modify it under the terms of the GNU Lesser General Public
- * License as published by the Free Software Foundation; either
- * version 2 of the License, or (at your option) any later version.
- *
- * This library is distributed in the hope that it will be useful,
- * but WITHOUT ANY WARRANTY; without even the implied warranty of
- * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
- * Lesser General Public License for more details.
- *
- * You should have received a copy of the GNU Lesser General Public
- * License along with this library; if not, write to the Free Software
- * Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA
- */
-
-#include &quot;../dsputil.h&quot;
-
-#include &quot;gcc_fixes.h&quot;
-
-#include &quot;dsputil_altivec.h&quot;
-
-#define PUT_OP_U8_ALTIVEC(d, s, dst) d = s
-#define AVG_OP_U8_ALTIVEC(d, s, dst) d = vec_avg(dst, s)
-
-#define OP_U8_ALTIVEC                          PUT_OP_U8_ALTIVEC
-#define PREFIX_h264_chroma_mc8_altivec         put_h264_chroma_mc8_altivec
-#define PREFIX_h264_chroma_mc8_num             altivec_put_h264_chroma_mc8_num
-#define PREFIX_h264_qpel16_h_lowpass_altivec   put_h264_qpel16_h_lowpass_altivec
-#define PREFIX_h264_qpel16_h_lowpass_num       altivec_put_h264_qpel16_h_lowpass_num
-#define PREFIX_h264_qpel16_v_lowpass_altivec   put_h264_qpel16_v_lowpass_altivec
-#define PREFIX_h264_qpel16_v_lowpass_num       altivec_put_h264_qpel16_v_lowpass_num
-#define PREFIX_h264_qpel16_hv_lowpass_altivec  put_h264_qpel16_hv_lowpass_altivec
-#define PREFIX_h264_qpel16_hv_lowpass_num      altivec_put_h264_qpel16_hv_lowpass_num
-#include &quot;dsputil_h264_template_altivec.c&quot;
-#undef OP_U8_ALTIVEC
-#undef PREFIX_h264_chroma_mc8_altivec
-#undef PREFIX_h264_chroma_mc8_num
-#undef PREFIX_h264_qpel16_h_lowpass_altivec
-#undef PREFIX_h264_qpel16_h_lowpass_num
-#undef PREFIX_h264_qpel16_v_lowpass_altivec
-#undef PREFIX_h264_qpel16_v_lowpass_num
-#undef PREFIX_h264_qpel16_hv_lowpass_altivec
-#undef PREFIX_h264_qpel16_hv_lowpass_num
-
-#define OP_U8_ALTIVEC                          AVG_OP_U8_ALTIVEC
-#define PREFIX_h264_chroma_mc8_altivec         avg_h264_chroma_mc8_altivec
-#define PREFIX_h264_chroma_mc8_num             altivec_avg_h264_chroma_mc8_num
-#define PREFIX_h264_qpel16_h_lowpass_altivec   avg_h264_qpel16_h_lowpass_altivec
-#define PREFIX_h264_qpel16_h_lowpass_num       altivec_avg_h264_qpel16_h_lowpass_num
-#define PREFIX_h264_qpel16_v_lowpass_altivec   avg_h264_qpel16_v_lowpass_altivec
-#define PREFIX_h264_qpel16_v_lowpass_num       altivec_avg_h264_qpel16_v_lowpass_num
-#define PREFIX_h264_qpel16_hv_lowpass_altivec  avg_h264_qpel16_hv_lowpass_altivec
-#define PREFIX_h264_qpel16_hv_lowpass_num      altivec_avg_h264_qpel16_hv_lowpass_num
-#include &quot;dsputil_h264_template_altivec.c&quot;
-#undef OP_U8_ALTIVEC
-#undef PREFIX_h264_chroma_mc8_altivec
-#undef PREFIX_h264_chroma_mc8_num
-#undef PREFIX_h264_qpel16_h_lowpass_altivec
-#undef PREFIX_h264_qpel16_h_lowpass_num
-#undef PREFIX_h264_qpel16_v_lowpass_altivec
-#undef PREFIX_h264_qpel16_v_lowpass_num
-#undef PREFIX_h264_qpel16_hv_lowpass_altivec
-#undef PREFIX_h264_qpel16_hv_lowpass_num
-
-#define H264_MC(OPNAME, SIZE, CODETYPE) \
-static void OPNAME ## h264_qpel ## SIZE ## _mc00_ ## CODETYPE (uint8_t *dst, uint8_t *src, int stride){\
-    OPNAME ## pixels ## SIZE ## _ ## CODETYPE(dst, src, stride, SIZE);\
-}\
-\
-static void OPNAME ## h264_qpel ## SIZE ## _mc10_ ## CODETYPE(uint8_t *dst, uint8_t *src, int stride){ \
-    DECLARE_ALIGNED_16(uint64_t, temp[SIZE*SIZE/8]);\
-    uint8_t * const half= (uint8_t*)temp;\
-    put_h264_qpel ## SIZE ## _h_lowpass_ ## CODETYPE(half, src, SIZE, stride);\
-    OPNAME ## pixels ## SIZE ## _l2_ ## CODETYPE(dst, src, half, stride, stride, SIZE);\
-}\
-\
-static void OPNAME ## h264_qpel ## SIZE ## _mc20_ ## CODETYPE(uint8_t *dst, uint8_t *src, int stride){\
-    OPNAME ## h264_qpel ## SIZE ## _h_lowpass_ ## CODETYPE(dst, src, stride, stride);\
-}\
-\
-static void OPNAME ## h264_qpel ## SIZE ## _mc30_ ## CODETYPE(uint8_t *dst, uint8_t *src, int stride){\
-    DECLARE_ALIGNED_16(uint64_t, temp[SIZE*SIZE/8]);\
-    uint8_t * const half= (uint8_t*)temp;\
-    put_h264_qpel ## SIZE ## _h_lowpass_ ## CODETYPE(half, src, SIZE, stride);\
-    OPNAME ## pixels ## SIZE ## _l2_ ## CODETYPE(dst, src+1, half, stride, stride, SIZE);\
-}\
-\
-static void OPNAME ## h264_qpel ## SIZE ## _mc01_ ## CODETYPE(uint8_t *dst, uint8_t *src, int stride){\
-    DECLARE_ALIGNED_16(uint64_t, temp[SIZE*SIZE/8]);\
-    uint8_t * const half= (uint8_t*)temp;\
-    put_h264_qpel ## SIZE ## _v_lowpass_ ## CODETYPE(half, src, SIZE, stride);\
-    OPNAME ## pixels ## SIZE ## _l2_ ## CODETYPE(dst, src, half, stride, stride, SIZE);\
-}\
-\
-static void OPNAME ## h264_qpel ## SIZE ## _mc02_ ## CODETYPE(uint8_t *dst, uint8_t *src, int stride){\
-    OPNAME ## h264_qpel ## SIZE ## _v_lowpass_ ## CODETYPE(dst, src, stride, stride);\
-}\
-\
-static void OPNAME ## h264_qpel ## SIZE ## _mc03_ ## CODETYPE(uint8_t *dst, uint8_t *src, int stride){\
-    DECLARE_ALIGNED_16(uint64_t, temp[SIZE*SIZE/8]);\
-    uint8_t * const half= (uint8_t*)temp;\
-    put_h264_qpel ## SIZE ## _v_lowpass_ ## CODETYPE(half, src, SIZE, stride);\
-    OPNAME ## pixels ## SIZE ## _l2_ ## CODETYPE(dst, src+stride, half, stride, stride, SIZE);\
-}\
-\
-static void OPNAME ## h264_qpel ## SIZE ## _mc11_ ## CODETYPE(uint8_t *dst, uint8_t *src, int stride){\
-    DECLARE_ALIGNED_16(uint64_t, temp[SIZE*SIZE/4]);\
-    uint8_t * const halfH= (uint8_t*)temp;\
-    uint8_t * const halfV= ((uint8_t*)temp) + SIZE*SIZE;\
-    put_h264_qpel ## SIZE ## _h_lowpass_ ## CODETYPE(halfH, src, SIZE, stride);\
-    put_h264_qpel ## SIZE ## _v_lowpass_ ## CODETYPE(halfV, src, SIZE, stride);\
-    OPNAME ## pixels ## SIZE ## _l2_ ## CODETYPE(dst, halfH, halfV, stride, SIZE, SIZE);\
-}\
-\
-static void OPNAME ## h264_qpel ## SIZE ## _mc31_ ## CODETYPE(uint8_t *dst, uint8_t *src, int stride){\
-    DECLARE_ALIGNED_16(uint64_t, temp[SIZE*SIZE/4]);\
-    uint8_t * const halfH= (uint8_t*)temp;\
-    uint8_t * const halfV= ((uint8_t*)temp) + SIZE*SIZE;\
-    put_h264_qpel ## SIZE ## _h_lowpass_ ## CODETYPE(halfH, src, SIZE, stride);\
-    put_h264_qpel ## SIZE ## _v_lowpass_ ## CODETYPE(halfV, src+1, SIZE, stride);\
-    OPNAME ## pixels ## SIZE ## _l2_ ## CODETYPE(dst, halfH, halfV, stride, SIZE, SIZE);\
-}\
-\
-static void OPNAME ## h264_qpel ## SIZE ## _mc13_ ## CODETYPE(uint8_t *dst, uint8_t *src, int stride){\
-    DECLARE_ALIGNED_16(uint64_t, temp[SIZE*SIZE/4]);\
-    uint8_t * const halfH= (uint8_t*)temp;\
-    uint8_t * const halfV= ((uint8_t*)temp) + SIZE*SIZE;\
-    put_h264_qpel ## SIZE ## _h_lowpass_ ## CODETYPE(halfH, src + stride, SIZE, stride);\
-    put_h264_qpel ## SIZE ## _v_lowpass_ ## CODETYPE(halfV, src, SIZE, stride);\
-    OPNAME ## pixels ## SIZE ## _l2_ ## CODETYPE(dst, halfH, halfV, stride, SIZE, SIZE);\
-}\
-\
-static void OPNAME ## h264_qpel ## SIZE ## _mc33_ ## CODETYPE(uint8_t *dst, uint8_t *src, int stride){\
-    DECLARE_ALIGNED_16(uint64_t, temp[SIZE*SIZE/4]);\
-    uint8_t * const halfH= (uint8_t*)temp;\
-    uint8_t * const halfV= ((uint8_t*)temp) + SIZE*SIZE;\
-    put_h264_qpel ## SIZE ## _h_lowpass_ ## CODETYPE(halfH, src + stride, SIZE, stride);\
-    put_h264_qpel ## SIZE ## _v_lowpass_ ## CODETYPE(halfV, src+1, SIZE, stride);\
-    OPNAME ## pixels ## SIZE ## _l2_ ## CODETYPE(dst, halfH, halfV, stride, SIZE, SIZE);\
-}\
-\
-static void OPNAME ## h264_qpel ## SIZE ## _mc22_ ## CODETYPE(uint8_t *dst, uint8_t *src, int stride){\
-    DECLARE_ALIGNED_16(uint64_t, temp[SIZE*(SIZE+8)/4]);\
-    int16_t * const tmp= (int16_t*)temp;\
-    OPNAME ## h264_qpel ## SIZE ## _hv_lowpass_ ## CODETYPE(dst, tmp, src, stride, SIZE, stride);\
-}\
-\
-static void OPNAME ## h264_qpel ## SIZE ## _mc21_ ## CODETYPE(uint8_t *dst, uint8_t *src, int stride){\
-    DECLARE_ALIGNED_16(uint64_t, temp[SIZE*(SIZE+8)/4 + SIZE*SIZE/4]);\
-    uint8_t * const halfH= (uint8_t*)temp;\
-    uint8_t * const halfHV= ((uint8_t*)temp) + SIZE*SIZE;\
-    int16_t * const tmp= ((int16_t*)temp) + SIZE*SIZE;\
-    put_h264_qpel ## SIZE ## _h_lowpass_ ## CODETYPE(halfH, src, SIZE, stride);\
-    put_h264_qpel ## SIZE ## _hv_lowpass_ ## CODETYPE(halfHV, tmp, src, SIZE, SIZE, stride);\
-    OPNAME ## pixels ## SIZE ## _l2_ ## CODETYPE(dst, halfH, halfHV, stride, SIZE, SIZE);\
-}\
-\
-static void OPNAME ## h264_qpel ## SIZE ## _mc23_ ## CODETYPE(uint8_t *dst, uint8_t *src, int stride){\
-    DECLARE_ALIGNED_16(uint64_t, temp[SIZE*(SIZE+8)/4 + SIZE*SIZE/4]);\
-    uint8_t * const halfH= (uint8_t*)temp;\
-    uint8_t * const halfHV= ((uint8_t*)temp) + SIZE*SIZE;\
-    int16_t * const tmp= ((int16_t*)temp) + SIZE*SIZE;\
-    put_h264_qpel ## SIZE ## _h_lowpass_ ## CODETYPE(halfH, src + stride, SIZE, stride);\
-    put_h264_qpel ## SIZE ## _hv_lowpass_ ## CODETYPE(halfHV, tmp, src, SIZE, SIZE, stride);\
-    OPNAME ## pixels ## SIZE ## _l2_ ## CODETYPE(dst, halfH, halfHV, stride, SIZE, SIZE);\
-}\
-\
-static void OPNAME ## h264_qpel ## SIZE ## _mc12_ ## CODETYPE(uint8_t *dst, uint8_t *src, int stride){\
-    DECLARE_ALIGNED_16(uint64_t, temp[SIZE*(SIZE+8)/4 + SIZE*SIZE/4]);\
-    uint8_t * const halfV= (uint8_t*)temp;\
-    uint8_t * const halfHV= ((uint8_t*)temp) + SIZE*SIZE;\
-    int16_t * const tmp= ((int16_t*)temp) + SIZE*SIZE;\
-    put_h264_qpel ## SIZE ## _v_lowpass_ ## CODETYPE(halfV, src, SIZE, stride);\
-    put_h264_qpel ## SIZE ## _hv_lowpass_ ## CODETYPE(halfHV, tmp, src, SIZE, SIZE, stride);\
-    OPNAME ## pixels ## SIZE ## _l2_ ## CODETYPE(dst, halfV, halfHV, stride, SIZE, SIZE);\
-}\
-\
-static void OPNAME ## h264_qpel ## SIZE ## _mc32_ ## CODETYPE(uint8_t *dst, uint8_t *src, int stride){\
-    DECLARE_ALIGNED_16(uint64_t, temp[SIZE*(SIZE+8)/4 + SIZE*SIZE/4]);\
-    uint8_t * const halfV= (uint8_t*)temp;\
-    uint8_t * const halfHV= ((uint8_t*)temp) + SIZE*SIZE;\
-    int16_t * const tmp= ((int16_t*)temp) + SIZE*SIZE;\
-    put_h264_qpel ## SIZE ## _v_lowpass_ ## CODETYPE(halfV, src+1, SIZE, stride);\
-    put_h264_qpel ## SIZE ## _hv_lowpass_ ## CODETYPE(halfHV, tmp, src, SIZE, SIZE, stride);\
-    OPNAME ## pixels ## SIZE ## _l2_ ## CODETYPE(dst, halfV, halfHV, stride, SIZE, SIZE);\
-}\
-
-static inline void put_pixels16_l2_altivec( uint8_t * dst, const uint8_t * src1,
-                                    const uint8_t * src2, int dst_stride,
-                                    int src_stride1, int h)
-{
-    int i;
-    vector unsigned char a, b, d, tmp1, tmp2, mask, mask_, edges, align;
-
-    mask_ = vec_lvsl(0, src2);
-
-    for (i = 0; i &lt; h; i++) {
-
-        tmp1 = vec_ld(i * src_stride1, src1);
-        mask = vec_lvsl(i * src_stride1, src1);
-        tmp2 = vec_ld(i * src_stride1 + 15, src1);
-
-        a = vec_perm(tmp1, tmp2, mask);
-
-        tmp1 = vec_ld(i * 16, src2);
-        tmp2 = vec_ld(i * 16 + 15, src2);
-
-        b = vec_perm(tmp1, tmp2, mask_);
-
-        tmp1 = vec_ld(0, dst);
-        mask = vec_lvsl(0, dst);
-        tmp2 = vec_ld(15, dst);
-
-        d = vec_avg(a, b);
-
-        edges = vec_perm(tmp2, tmp1, mask);
-
-        align = vec_lvsr(0, dst);
-
-        tmp1 = vec_perm(edges, d, align);
-        tmp2 = vec_perm(d, edges, align);
-
-        vec_st(tmp2, 15, dst);
-        vec_st(tmp1, 0 , dst);
-
-        dst += dst_stride;
-    }
-}
-
-static inline void avg_pixels16_l2_altivec( uint8_t * dst, const uint8_t * src1,
-                                    const uint8_t * src2, int dst_stride,
-                                    int src_stride1, int h)
-{
-    int i;
-    vector unsigned char a, b, d, tmp1, tmp2, mask, mask_, edges, align;
-
-    mask_ = vec_lvsl(0, src2);
-
-    for (i = 0; i &lt; h; i++) {
-
-        tmp1 = vec_ld(i * src_stride1, src1);
-        mask = vec_lvsl(i * src_stride1, src1);
-        tmp2 = vec_ld(i * src_stride1 + 15, src1);
-
-        a = vec_perm(tmp1, tmp2, mask);
-
-        tmp1 = vec_ld(i * 16, src2);
-        tmp2 = vec_ld(i * 16 + 15, src2);
-
-        b = vec_perm(tmp1, tmp2, mask_);
-
-        tmp1 = vec_ld(0, dst);
-        mask = vec_lvsl(0, dst);
-        tmp2 = vec_ld(15, dst);
-
-        d = vec_avg(vec_perm(tmp1, tmp2, mask), vec_avg(a, b));
-
-        edges = vec_perm(tmp2, tmp1, mask);
-
-        align = vec_lvsr(0, dst);
-
-        tmp1 = vec_perm(edges, d, align);
-        tmp2 = vec_perm(d, edges, align);
-
-        vec_st(tmp2, 15, dst);
-        vec_st(tmp1, 0 , dst);
-
-        dst += dst_stride;
-    }
-}
-
-/* Implemented but could be faster
-#define put_pixels16_l2_altivec(d,s1,s2,ds,s1s,h) put_pixels16_l2(d,s1,s2,ds,s1s,16,h)
-#define avg_pixels16_l2_altivec(d,s1,s2,ds,s1s,h) avg_pixels16_l2(d,s1,s2,ds,s1s,16,h)
- */
-
-  H264_MC(put_, 16, altivec)
-  H264_MC(avg_, 16, altivec)
-
-void dsputil_h264_init_ppc(DSPContext* c, AVCodecContext *avctx) {
-
-#ifdef HAVE_ALTIVEC
-  if (has_altivec()) {
-    c-&gt;put_h264_chroma_pixels_tab[0] = put_h264_chroma_mc8_altivec;
-    c-&gt;avg_h264_chroma_pixels_tab[0] = avg_h264_chroma_mc8_altivec;
-
-#define dspfunc(PFX, IDX, NUM) \
-    c-&gt;PFX ## _pixels_tab[IDX][ 0] = PFX ## NUM ## _mc00_altivec; \
-    c-&gt;PFX ## _pixels_tab[IDX][ 1] = PFX ## NUM ## _mc10_altivec; \
-    c-&gt;PFX ## _pixels_tab[IDX][ 2] = PFX ## NUM ## _mc20_altivec; \
-    c-&gt;PFX ## _pixels_tab[IDX][ 3] = PFX ## NUM ## _mc30_altivec; \
-    c-&gt;PFX ## _pixels_tab[IDX][ 4] = PFX ## NUM ## _mc01_altivec; \
-    c-&gt;PFX ## _pixels_tab[IDX][ 5] = PFX ## NUM ## _mc11_altivec; \
-    c-&gt;PFX ## _pixels_tab[IDX][ 6] = PFX ## NUM ## _mc21_altivec; \
-    c-&gt;PFX ## _pixels_tab[IDX][ 7] = PFX ## NUM ## _mc31_altivec; \
-    c-&gt;PFX ## _pixels_tab[IDX][ 8] = PFX ## NUM ## _mc02_altivec; \
-    c-&gt;PFX ## _pixels_tab[IDX][ 9] = PFX ## NUM ## _mc12_altivec; \
-    c-&gt;PFX ## _pixels_tab[IDX][10] = PFX ## NUM ## _mc22_altivec; \
-    c-&gt;PFX ## _pixels_tab[IDX][11] = PFX ## NUM ## _mc32_altivec; \
-    c-&gt;PFX ## _pixels_tab[IDX][12] = PFX ## NUM ## _mc03_altivec; \
-    c-&gt;PFX ## _pixels_tab[IDX][13] = PFX ## NUM ## _mc13_altivec; \
-    c-&gt;PFX ## _pixels_tab[IDX][14] = PFX ## NUM ## _mc23_altivec; \
-    c-&gt;PFX ## _pixels_tab[IDX][15] = PFX ## NUM ## _mc33_altivec
-
-    dspfunc(put_h264_qpel, 0, 16);
-    dspfunc(avg_h264_qpel, 0, 16);
-#undef dspfunc
-
-  } else
-#endif /* HAVE_ALTIVEC */
-  {
-    // Non-AltiVec PPC optimisations
-
-    // ... pending ...
-  }
-}

Deleted: branches/avidemux_2.4_branch/avidemux/ADM_libraries/ADM_lavcodec/ppc/dsputil_h264_template_altivec.c
===================================================================
--- branches/avidemux_2.4_branch/avidemux/ADM_libraries/ADM_lavcodec/ppc/dsputil_h264_template_altivec.c	2007-12-03 12:38:06 UTC (rev 3719)
+++ branches/avidemux_2.4_branch/avidemux/ADM_libraries/ADM_lavcodec/ppc/dsputil_h264_template_altivec.c	2007-12-04 00:19:48 UTC (rev 3720)
@@ -1,717 +0,0 @@
-/*
- * Copyright (c) 2004 Romain Dolbeau &lt;<A HREF="https://lists.berlios.de/mailman/listinfo/avidemux-svn-commit">romain at dolbeau.org</A>&gt;
- *
- * This library is free software; you can redistribute it and/or
- * modify it under the terms of the GNU Lesser General Public
- * License as published by the Free Software Foundation; either
- * version 2 of the License, or (at your option) any later version.
- *
- * This library is distributed in the hope that it will be useful,
- * but WITHOUT ANY WARRANTY; without even the implied warranty of
- * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
- * Lesser General Public License for more details.
- *
- * You should have received a copy of the GNU Lesser General Public
- * License along with this library; if not, write to the Free Software
- * Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA
- */
-
-/* this code assume that stride % 16 == 0 */
-void PREFIX_h264_chroma_mc8_altivec(uint8_t * dst, uint8_t * src, int stride, int h, int x, int y) {
-  POWERPC_PERF_DECLARE(PREFIX_h264_chroma_mc8_num, 1);
-    signed int ABCD[4] __attribute__((aligned(16))) =
-                        {((8 - x) * (8 - y)),
-                          ((x) * (8 - y)),
-                          ((8 - x) * (y)),
-                          ((x) * (y))};
-    register int i;
-    vector unsigned char fperm;
-    const vector signed int vABCD = vec_ld(0, ABCD);
-    const vector signed short vA = vec_splat((vector signed short)vABCD, 1);
-    const vector signed short vB = vec_splat((vector signed short)vABCD, 3);
-    const vector signed short vC = vec_splat((vector signed short)vABCD, 5);
-    const vector signed short vD = vec_splat((vector signed short)vABCD, 7);
-    const vector signed int vzero = vec_splat_s32(0);
-    const vector signed short v32ss = vec_sl(vec_splat_s16(1),vec_splat_u16(5));
-    const vector unsigned short v6us = vec_splat_u16(6);
-    register int loadSecond = (((unsigned long)src) % 16) &lt;= 7 ? 0 : 1;
-    register int reallyBadAlign = (((unsigned long)src) % 16) == 15 ? 1 : 0;
-
-    vector unsigned char vsrcAuc, vsrcBuc, vsrcperm0, vsrcperm1;
-    vector unsigned char vsrc0uc, vsrc1uc;
-    vector signed short vsrc0ssH, vsrc1ssH;
-    vector unsigned char vsrcCuc, vsrc2uc, vsrc3uc;
-    vector signed short vsrc2ssH, vsrc3ssH, psum;
-    vector unsigned char vdst, ppsum, vfdst, fsum;
-
-  POWERPC_PERF_START_COUNT(PREFIX_h264_chroma_mc8_num, 1);
-
-    if (((unsigned long)dst) % 16 == 0) {
-      fperm = (vector unsigned char)AVV(0x10, 0x11, 0x12, 0x13,
-                                        0x14, 0x15, 0x16, 0x17,
-                                        0x08, 0x09, 0x0A, 0x0B,
-                                        0x0C, 0x0D, 0x0E, 0x0F);
-    } else {
-      fperm = (vector unsigned char)AVV(0x00, 0x01, 0x02, 0x03,
-                                        0x04, 0x05, 0x06, 0x07,
-                                        0x18, 0x19, 0x1A, 0x1B,
-                                        0x1C, 0x1D, 0x1E, 0x1F);
-    }
-
-    vsrcAuc = vec_ld(0, src);
-
-    if (loadSecond)
-      vsrcBuc = vec_ld(16, src);
-    vsrcperm0 = vec_lvsl(0, src);
-    vsrcperm1 = vec_lvsl(1, src);
-
-    vsrc0uc = vec_perm(vsrcAuc, vsrcBuc, vsrcperm0);
-    if (reallyBadAlign)
-      vsrc1uc = vsrcBuc;
-    else
-      vsrc1uc = vec_perm(vsrcAuc, vsrcBuc, vsrcperm1);
-
-    vsrc0ssH = (vector signed short)vec_mergeh((vector unsigned char)vzero,
-                                               (vector unsigned char)vsrc0uc);
-    vsrc1ssH = (vector signed short)vec_mergeh((vector unsigned char)vzero,
-                                               (vector unsigned char)vsrc1uc);
-
-    if (!loadSecond) {// -&gt; !reallyBadAlign
-      for (i = 0 ; i &lt; h ; i++) {
-
-
-        vsrcCuc = vec_ld(stride + 0, src);
-
-        vsrc2uc = vec_perm(vsrcCuc, vsrcCuc, vsrcperm0);
-        vsrc3uc = vec_perm(vsrcCuc, vsrcCuc, vsrcperm1);
-
-        vsrc2ssH = (vector signed short)vec_mergeh((vector unsigned char)vzero,
-                                                (vector unsigned char)vsrc2uc);
-        vsrc3ssH = (vector signed short)vec_mergeh((vector unsigned char)vzero,
-                                                (vector unsigned char)vsrc3uc);
-
-        psum = vec_mladd(vA, vsrc0ssH, vec_splat_s16(0));
-        psum = vec_mladd(vB, vsrc1ssH, psum);
-        psum = vec_mladd(vC, vsrc2ssH, psum);
-        psum = vec_mladd(vD, vsrc3ssH, psum);
-        psum = vec_add(v32ss, psum);
-        psum = vec_sra(psum, v6us);
-
-        vdst = vec_ld(0, dst);
-        ppsum = (vector unsigned char)vec_packsu(psum, psum);
-        vfdst = vec_perm(vdst, ppsum, fperm);
-
-        OP_U8_ALTIVEC(fsum, vfdst, vdst);
-
-        vec_st(fsum, 0, dst);
-
-        vsrc0ssH = vsrc2ssH;
-        vsrc1ssH = vsrc3ssH;
-
-        dst += stride;
-        src += stride;
-      }
-    } else {
-        vector unsigned char vsrcDuc;
-      for (i = 0 ; i &lt; h ; i++) {
-        vsrcCuc = vec_ld(stride + 0, src);
-        vsrcDuc = vec_ld(stride + 16, src);
-
-        vsrc2uc = vec_perm(vsrcCuc, vsrcDuc, vsrcperm0);
-        if (reallyBadAlign)
-          vsrc3uc = vsrcDuc;
-        else
-          vsrc3uc = vec_perm(vsrcCuc, vsrcDuc, vsrcperm1);
-
-        vsrc2ssH = (vector signed short)vec_mergeh((vector unsigned char)vzero,
-                                                (vector unsigned char)vsrc2uc);
-        vsrc3ssH = (vector signed short)vec_mergeh((vector unsigned char)vzero,
-                                                (vector unsigned char)vsrc3uc);
-
-        psum = vec_mladd(vA, vsrc0ssH, vec_splat_s16(0));
-        psum = vec_mladd(vB, vsrc1ssH, psum);
-        psum = vec_mladd(vC, vsrc2ssH, psum);
-        psum = vec_mladd(vD, vsrc3ssH, psum);
-        psum = vec_add(v32ss, psum);
-        psum = vec_sr(psum, v6us);
-
-        vdst = vec_ld(0, dst);
-        ppsum = (vector unsigned char)vec_pack(psum, psum);
-        vfdst = vec_perm(vdst, ppsum, fperm);
-
-        OP_U8_ALTIVEC(fsum, vfdst, vdst);
-
-        vec_st(fsum, 0, dst);
-
-        vsrc0ssH = vsrc2ssH;
-        vsrc1ssH = vsrc3ssH;
-
-        dst += stride;
-        src += stride;
-      }
-    }
-    POWERPC_PERF_STOP_COUNT(PREFIX_h264_chroma_mc8_num, 1);
-}
-
-/* this code assume stride % 16 == 0 */
-static void PREFIX_h264_qpel16_h_lowpass_altivec(uint8_t * dst, uint8_t * src, int dstStride, int srcStride) {
-  POWERPC_PERF_DECLARE(PREFIX_h264_qpel16_h_lowpass_num, 1);
-  register int i;
-
-  const vector signed int vzero = vec_splat_s32(0);
-  const vector unsigned char permM2 = vec_lvsl(-2, src);
-  const vector unsigned char permM1 = vec_lvsl(-1, src);
-  const vector unsigned char permP0 = vec_lvsl(+0, src);
-  const vector unsigned char permP1 = vec_lvsl(+1, src);
-  const vector unsigned char permP2 = vec_lvsl(+2, src);
-  const vector unsigned char permP3 = vec_lvsl(+3, src);
-  const vector signed short v5ss = vec_splat_s16(5);
-  const vector unsigned short v5us = vec_splat_u16(5);
-  const vector signed short v20ss = vec_sl(vec_splat_s16(5),vec_splat_u16(2));
-  const vector signed short v16ss = vec_sl(vec_splat_s16(1),vec_splat_u16(4));
-  const vector unsigned char dstperm = vec_lvsr(0, dst);
-  const vector unsigned char neg1 =
-                                (const vector unsigned char) vec_splat_s8(-1);
-
-  const vector unsigned char dstmask =
-                                vec_perm((const vector unsigned char)vzero,
-                                                               neg1, dstperm);
-
-  vector unsigned char srcM2, srcM1, srcP0, srcP1, srcP2, srcP3;
-
-  register int align = ((((unsigned long)src) - 2) % 16);
-
-  vector signed short srcP0A, srcP0B, srcP1A, srcP1B,
-                      srcP2A, srcP2B, srcP3A, srcP3B,
-                      srcM1A, srcM1B, srcM2A, srcM2B,
-                      sum1A, sum1B, sum2A, sum2B, sum3A, sum3B,
-                      pp1A, pp1B, pp2A, pp2B, pp3A, pp3B,
-                      psumA, psumB, sumA, sumB;
-
-  vector unsigned char sum, dst1, dst2, vdst, fsum,
-                       rsum, fdst1, fdst2;
-
-  POWERPC_PERF_START_COUNT(PREFIX_h264_qpel16_h_lowpass_num, 1);
-
-  for (i = 0 ; i &lt; 16 ; i ++) {
-    vector unsigned char srcR1 = vec_ld(-2, src);
-    vector unsigned char srcR2 = vec_ld(14, src);
-
-    switch (align) {
-    default: {
-      srcM2 = vec_perm(srcR1, srcR2, permM2);
-      srcM1 = vec_perm(srcR1, srcR2, permM1);
-      srcP0 = vec_perm(srcR1, srcR2, permP0);
-      srcP1 = vec_perm(srcR1, srcR2, permP1);
-      srcP2 = vec_perm(srcR1, srcR2, permP2);
-      srcP3 = vec_perm(srcR1, srcR2, permP3);
-    } break;
-    case 11: {
-      srcM2 = vec_perm(srcR1, srcR2, permM2);
-      srcM1 = vec_perm(srcR1, srcR2, permM1);
-      srcP0 = vec_perm(srcR1, srcR2, permP0);
-      srcP1 = vec_perm(srcR1, srcR2, permP1);
-      srcP2 = vec_perm(srcR1, srcR2, permP2);
-      srcP3 = srcR2;
-    } break;
-    case 12: {
-      vector unsigned char srcR3 = vec_ld(30, src);
-      srcM2 = vec_perm(srcR1, srcR2, permM2);
-      srcM1 = vec_perm(srcR1, srcR2, permM1);
-      srcP0 = vec_perm(srcR1, srcR2, permP0);
-      srcP1 = vec_perm(srcR1, srcR2, permP1);
-      srcP2 = srcR2;
-      srcP3 = vec_perm(srcR2, srcR3, permP3);
-    } break;
-    case 13: {
-      vector unsigned char srcR3 = vec_ld(30, src);
-      srcM2 = vec_perm(srcR1, srcR2, permM2);
-      srcM1 = vec_perm(srcR1, srcR2, permM1);
-      srcP0 = vec_perm(srcR1, srcR2, permP0);
-      srcP1 = srcR2;
-      srcP2 = vec_perm(srcR2, srcR3, permP2);
-      srcP3 = vec_perm(srcR2, srcR3, permP3);
-    } break;
-    case 14: {
-      vector unsigned char srcR3 = vec_ld(30, src);
-      srcM2 = vec_perm(srcR1, srcR2, permM2);
-      srcM1 = vec_perm(srcR1, srcR2, permM1);
-      srcP0 = srcR2;
-      srcP1 = vec_perm(srcR2, srcR3, permP1);
-      srcP2 = vec_perm(srcR2, srcR3, permP2);
-      srcP3 = vec_perm(srcR2, srcR3, permP3);
-    } break;
-    case 15: {
-      vector unsigned char srcR3 = vec_ld(30, src);
-      srcM2 = vec_perm(srcR1, srcR2, permM2);
-      srcM1 = srcR2;
-      srcP0 = vec_perm(srcR2, srcR3, permP0);
-      srcP1 = vec_perm(srcR2, srcR3, permP1);
-      srcP2 = vec_perm(srcR2, srcR3, permP2);
-      srcP3 = vec_perm(srcR2, srcR3, permP3);
-    } break;
-    }
-
-    srcP0A = (vector signed short)
-                vec_mergeh((vector unsigned char)vzero, srcP0);
-    srcP0B = (vector signed short)
-                vec_mergel((vector unsigned char)vzero, srcP0);
-    srcP1A = (vector signed short)
-                vec_mergeh((vector unsigned char)vzero, srcP1);
-    srcP1B = (vector signed short)
-                vec_mergel((vector unsigned char)vzero, srcP1);
-
-    srcP2A = (vector signed short)
-                vec_mergeh((vector unsigned char)vzero, srcP2);
-    srcP2B = (vector signed short)
-                vec_mergel((vector unsigned char)vzero, srcP2);
-    srcP3A = (vector signed short)
-                vec_mergeh((vector unsigned char)vzero, srcP3);
-    srcP3B = (vector signed short)
-                vec_mergel((vector unsigned char)vzero, srcP3);
-
-    srcM1A = (vector signed short)
-                vec_mergeh((vector unsigned char)vzero, srcM1);
-    srcM1B = (vector signed short)
-                vec_mergel((vector unsigned char)vzero, srcM1);
-    srcM2A = (vector signed short)
-                vec_mergeh((vector unsigned char)vzero, srcM2);
-    srcM2B = (vector signed short)
-                vec_mergel((vector unsigned char)vzero, srcM2);
-
-    sum1A = vec_adds(srcP0A, srcP1A);
-    sum1B = vec_adds(srcP0B, srcP1B);
-    sum2A = vec_adds(srcM1A, srcP2A);
-    sum2B = vec_adds(srcM1B, srcP2B);
-    sum3A = vec_adds(srcM2A, srcP3A);
-    sum3B = vec_adds(srcM2B, srcP3B);
-
-    pp1A = vec_mladd(sum1A, v20ss, v16ss);
-    pp1B = vec_mladd(sum1B, v20ss, v16ss);
-
-    pp2A = vec_mladd(sum2A, v5ss, (vector signed short)vzero);
-    pp2B = vec_mladd(sum2B, v5ss, (vector signed short)vzero);
-
-    pp3A = vec_add(sum3A, pp1A);
-    pp3B = vec_add(sum3B, pp1B);
-
-    psumA = vec_sub(pp3A, pp2A);
-    psumB = vec_sub(pp3B, pp2B);
-
-    sumA = vec_sra(psumA, v5us);
-    sumB = vec_sra(psumB, v5us);
-
-    sum = vec_packsu(sumA, sumB);
-
-    dst1 = vec_ld(0, dst);
-    dst2 = vec_ld(16, dst);
-    vdst = vec_perm(dst1, dst2, vec_lvsl(0, dst));
-
-    OP_U8_ALTIVEC(fsum, sum, vdst);
-
-    rsum = vec_perm(fsum, fsum, dstperm);
-    fdst1 = vec_sel(dst1, rsum, dstmask);
-    fdst2 = vec_sel(rsum, dst2, dstmask);
-
-    vec_st(fdst1, 0, dst);
-    vec_st(fdst2, 16, dst);
-
-    src += srcStride;
-    dst += dstStride;
-  }
-POWERPC_PERF_STOP_COUNT(PREFIX_h264_qpel16_h_lowpass_num, 1);
-}
-
-/* this code assume stride % 16 == 0 */
-static void PREFIX_h264_qpel16_v_lowpass_altivec(uint8_t * dst, uint8_t * src, int dstStride, int srcStride) {
-  POWERPC_PERF_DECLARE(PREFIX_h264_qpel16_v_lowpass_num, 1);
-
-  register int i;
-
-  const vector signed int vzero = vec_splat_s32(0);
-  const vector unsigned char perm = vec_lvsl(0, src);
-  const vector signed short v20ss = vec_sl(vec_splat_s16(5),vec_splat_u16(2));
-  const vector unsigned short v5us = vec_splat_u16(5);
-  const vector signed short v5ss = vec_splat_s16(5);
-  const vector signed short v16ss = vec_sl(vec_splat_s16(1),vec_splat_u16(4));
-  const vector unsigned char dstperm = vec_lvsr(0, dst);
-  const vector unsigned char neg1 = (const vector unsigned char)vec_splat_s8(-1);
-  const vector unsigned char dstmask = vec_perm((const vector unsigned char)vzero, neg1, dstperm);
-
-  uint8_t *srcbis = src - (srcStride * 2);
-
-  const vector unsigned char srcM2a = vec_ld(0, srcbis);
-  const vector unsigned char srcM2b = vec_ld(16, srcbis);
-  const vector unsigned char srcM2 = vec_perm(srcM2a, srcM2b, perm);
-//  srcbis += srcStride;
-  const vector unsigned char srcM1a = vec_ld(0, srcbis += srcStride);
-  const vector unsigned char srcM1b = vec_ld(16, srcbis);
-  const vector unsigned char srcM1 = vec_perm(srcM1a, srcM1b, perm);
-//  srcbis += srcStride;
-  const vector unsigned char srcP0a = vec_ld(0, srcbis += srcStride);
-  const vector unsigned char srcP0b = vec_ld(16, srcbis);
-  const vector unsigned char srcP0 = vec_perm(srcP0a, srcP0b, perm);
-//  srcbis += srcStride;
-  const vector unsigned char srcP1a = vec_ld(0, srcbis += srcStride);
-  const vector unsigned char srcP1b = vec_ld(16, srcbis);
-  const vector unsigned char srcP1 = vec_perm(srcP1a, srcP1b, perm);
-//  srcbis += srcStride;
-  const vector unsigned char srcP2a = vec_ld(0, srcbis += srcStride);
-  const vector unsigned char srcP2b = vec_ld(16, srcbis);
-  const vector unsigned char srcP2 = vec_perm(srcP2a, srcP2b, perm);
-//  srcbis += srcStride;
-
-  vector signed short srcM2ssA = (vector signed short)
-                                vec_mergeh((vector unsigned char)vzero, srcM2);
-  vector signed short srcM2ssB = (vector signed short)
-                                vec_mergel((vector unsigned char)vzero, srcM2);
-  vector signed short srcM1ssA = (vector signed short)
-                                vec_mergeh((vector unsigned char)vzero, srcM1);
-  vector signed short srcM1ssB = (vector signed short)
-                                vec_mergel((vector unsigned char)vzero, srcM1);
-  vector signed short srcP0ssA = (vector signed short)
-                                vec_mergeh((vector unsigned char)vzero, srcP0);
-  vector signed short srcP0ssB = (vector signed short)
-                                vec_mergel((vector unsigned char)vzero, srcP0);
-  vector signed short srcP1ssA = (vector signed short)
-                                vec_mergeh((vector unsigned char)vzero, srcP1);
-  vector signed short srcP1ssB = (vector signed short)
-                                vec_mergel((vector unsigned char)vzero, srcP1);
-  vector signed short srcP2ssA = (vector signed short)
-                                vec_mergeh((vector unsigned char)vzero, srcP2);
-  vector signed short srcP2ssB = (vector signed short)
-                                vec_mergel((vector unsigned char)vzero, srcP2);
-
-  vector signed short pp1A, pp1B, pp2A, pp2B, pp3A, pp3B,
-                      psumA, psumB, sumA, sumB,
-                      srcP3ssA, srcP3ssB,
-                      sum1A, sum1B, sum2A, sum2B, sum3A, sum3B;
-
-  vector unsigned char sum, dst1, dst2, vdst, fsum, rsum, fdst1, fdst2,
-                       srcP3a, srcP3b, srcP3;
-
-  POWERPC_PERF_START_COUNT(PREFIX_h264_qpel16_v_lowpass_num, 1);
-
-  for (i = 0 ; i &lt; 16 ; i++) {
-    srcP3a = vec_ld(0, srcbis += srcStride);
-    srcP3b = vec_ld(16, srcbis);
-    srcP3 = vec_perm(srcP3a, srcP3b, perm);
-    srcP3ssA = (vector signed short)
-                                vec_mergeh((vector unsigned char)vzero, srcP3);
-    srcP3ssB = (vector signed short)
-                                vec_mergel((vector unsigned char)vzero, srcP3);
-//    srcbis += srcStride;
-
-    sum1A = vec_adds(srcP0ssA, srcP1ssA);
-    sum1B = vec_adds(srcP0ssB, srcP1ssB);
-    sum2A = vec_adds(srcM1ssA, srcP2ssA);
-    sum2B = vec_adds(srcM1ssB, srcP2ssB);
-    sum3A = vec_adds(srcM2ssA, srcP3ssA);
-    sum3B = vec_adds(srcM2ssB, srcP3ssB);
-
-    srcM2ssA = srcM1ssA;
-    srcM2ssB = srcM1ssB;
-    srcM1ssA = srcP0ssA;
-    srcM1ssB = srcP0ssB;
-    srcP0ssA = srcP1ssA;
-    srcP0ssB = srcP1ssB;
-    srcP1ssA = srcP2ssA;
-    srcP1ssB = srcP2ssB;
-    srcP2ssA = srcP3ssA;
-    srcP2ssB = srcP3ssB;
-
-    pp1A = vec_mladd(sum1A, v20ss, v16ss);
-    pp1B = vec_mladd(sum1B, v20ss, v16ss);
-
-    pp2A = vec_mladd(sum2A, v5ss, (vector signed short)vzero);
-    pp2B = vec_mladd(sum2B, v5ss, (vector signed short)vzero);
-
-    pp3A = vec_add(sum3A, pp1A);
-    pp3B = vec_add(sum3B, pp1B);
-
-    psumA = vec_sub(pp3A, pp2A);
-    psumB = vec_sub(pp3B, pp2B);
-
-    sumA = vec_sra(psumA, v5us);
-    sumB = vec_sra(psumB, v5us);
-
-    sum = vec_packsu(sumA, sumB);
-
-    dst1 = vec_ld(0, dst);
-    dst2 = vec_ld(16, dst);
-    vdst = vec_perm(dst1, dst2, vec_lvsl(0, dst));
-
-    OP_U8_ALTIVEC(fsum, sum, vdst);
-
-    rsum = vec_perm(fsum, fsum, dstperm);
-    fdst1 = vec_sel(dst1, rsum, dstmask);
-    fdst2 = vec_sel(rsum, dst2, dstmask);
-
-    vec_st(fdst1, 0, dst);
-    vec_st(fdst2, 16, dst);
-
-    dst += dstStride;
-  }
-  POWERPC_PERF_STOP_COUNT(PREFIX_h264_qpel16_v_lowpass_num, 1);
-}
-
-/* this code assume stride % 16 == 0 *and* tmp is properly aligned */
-static void PREFIX_h264_qpel16_hv_lowpass_altivec(uint8_t * dst, int16_t * tmp, uint8_t * src, int dstStride, int tmpStride, int srcStride) {
-  POWERPC_PERF_DECLARE(PREFIX_h264_qpel16_hv_lowpass_num, 1);
-  register int i;
-  const vector signed int vzero = vec_splat_s32(0);
-  const vector unsigned char permM2 = vec_lvsl(-2, src);
-  const vector unsigned char permM1 = vec_lvsl(-1, src);
-  const vector unsigned char permP0 = vec_lvsl(+0, src);
-  const vector unsigned char permP1 = vec_lvsl(+1, src);
-  const vector unsigned char permP2 = vec_lvsl(+2, src);
-  const vector unsigned char permP3 = vec_lvsl(+3, src);
-  const vector signed short v20ss = vec_sl(vec_splat_s16(5),vec_splat_u16(2));
-  const vector unsigned int v10ui = vec_splat_u32(10);
-  const vector signed short v5ss = vec_splat_s16(5);
-  const vector signed short v1ss = vec_splat_s16(1);
-  const vector signed int v512si = vec_sl(vec_splat_s32(1),vec_splat_u32(9));
-  const vector unsigned int v16ui = vec_sl(vec_splat_u32(1),vec_splat_u32(4));
-
-  register int align = ((((unsigned long)src) - 2) % 16);
-
-  const vector unsigned char neg1 = (const vector unsigned char)
-                                                        vec_splat_s8(-1);
-
-  vector signed short srcP0A, srcP0B, srcP1A, srcP1B,
-                      srcP2A, srcP2B, srcP3A, srcP3B,
-                      srcM1A, srcM1B, srcM2A, srcM2B,
-                      sum1A, sum1B, sum2A, sum2B, sum3A, sum3B,
-                      pp1A, pp1B, pp2A, pp2B, psumA, psumB;
-
-  const vector unsigned char dstperm = vec_lvsr(0, dst);
-
-  const vector unsigned char dstmask = vec_perm((const vector unsigned char)vzero, neg1, dstperm);
-
-  const vector unsigned char mperm = (const vector unsigned char)
-    AVV(0x00, 0x08, 0x01, 0x09, 0x02, 0x0A, 0x03, 0x0B,
-        0x04, 0x0C, 0x05, 0x0D, 0x06, 0x0E, 0x07, 0x0F);
-  int16_t *tmpbis = tmp;
-
-  vector signed short tmpM1ssA, tmpM1ssB, tmpM2ssA, tmpM2ssB,
-                      tmpP0ssA, tmpP0ssB, tmpP1ssA, tmpP1ssB,
-                      tmpP2ssA, tmpP2ssB;
-
-  vector signed int pp1Ae, pp1Ao, pp1Be, pp1Bo, pp2Ae, pp2Ao, pp2Be, pp2Bo,
-                    pp3Ae, pp3Ao, pp3Be, pp3Bo, pp1cAe, pp1cAo, pp1cBe, pp1cBo,
-                    pp32Ae, pp32Ao, pp32Be, pp32Bo, sumAe, sumAo, sumBe, sumBo,
-                    ssumAe, ssumAo, ssumBe, ssumBo;
-  vector unsigned char fsum, sumv, sum, dst1, dst2, vdst,
-                       rsum, fdst1, fdst2;
-  vector signed short ssume, ssumo;
-
-  POWERPC_PERF_START_COUNT(PREFIX_h264_qpel16_hv_lowpass_num, 1);
-  src -= (2 * srcStride);
-  for (i = 0 ; i &lt; 21 ; i ++) {
-    vector unsigned char srcM2, srcM1, srcP0, srcP1, srcP2, srcP3;
-    vector unsigned char srcR1 = vec_ld(-2, src);
-    vector unsigned char srcR2 = vec_ld(14, src);
-
-    switch (align) {
-    default: {
-      srcM2 = vec_perm(srcR1, srcR2, permM2);
-      srcM1 = vec_perm(srcR1, srcR2, permM1);
-      srcP0 = vec_perm(srcR1, srcR2, permP0);
-      srcP1 = vec_perm(srcR1, srcR2, permP1);
-      srcP2 = vec_perm(srcR1, srcR2, permP2);
-      srcP3 = vec_perm(srcR1, srcR2, permP3);
-    } break;
-    case 11: {
-      srcM2 = vec_perm(srcR1, srcR2, permM2);
-      srcM1 = vec_perm(srcR1, srcR2, permM1);
-      srcP0 = vec_perm(srcR1, srcR2, permP0);
-      srcP1 = vec_perm(srcR1, srcR2, permP1);
-      srcP2 = vec_perm(srcR1, srcR2, permP2);
-      srcP3 = srcR2;
-    } break;
-    case 12: {
-      vector unsigned char srcR3 = vec_ld(30, src);
-      srcM2 = vec_perm(srcR1, srcR2, permM2);
-      srcM1 = vec_perm(srcR1, srcR2, permM1);
-      srcP0 = vec_perm(srcR1, srcR2, permP0);
-      srcP1 = vec_perm(srcR1, srcR2, permP1);
-      srcP2 = srcR2;
-      srcP3 = vec_perm(srcR2, srcR3, permP3);
-    } break;
-    case 13: {
-      vector unsigned char srcR3 = vec_ld(30, src);
-      srcM2 = vec_perm(srcR1, srcR2, permM2);
-      srcM1 = vec_perm(srcR1, srcR2, permM1);
-      srcP0 = vec_perm(srcR1, srcR2, permP0);
-      srcP1 = srcR2;
-      srcP2 = vec_perm(srcR2, srcR3, permP2);
-      srcP3 = vec_perm(srcR2, srcR3, permP3);
-    } break;
-    case 14: {
-      vector unsigned char srcR3 = vec_ld(30, src);
-      srcM2 = vec_perm(srcR1, srcR2, permM2);
-      srcM1 = vec_perm(srcR1, srcR2, permM1);
-      srcP0 = srcR2;
-      srcP1 = vec_perm(srcR2, srcR3, permP1);
-      srcP2 = vec_perm(srcR2, srcR3, permP2);
-      srcP3 = vec_perm(srcR2, srcR3, permP3);
-    } break;
-    case 15: {
-      vector unsigned char srcR3 = vec_ld(30, src);
-      srcM2 = vec_perm(srcR1, srcR2, permM2);
-      srcM1 = srcR2;
-      srcP0 = vec_perm(srcR2, srcR3, permP0);
-      srcP1 = vec_perm(srcR2, srcR3, permP1);
-      srcP2 = vec_perm(srcR2, srcR3, permP2);
-      srcP3 = vec_perm(srcR2, srcR3, permP3);
-    } break;
-    }
-
-    srcP0A = (vector signed short)
-                            vec_mergeh((vector unsigned char)vzero, srcP0);
-    srcP0B = (vector signed short)
-                            vec_mergel((vector unsigned char)vzero, srcP0);
-    srcP1A = (vector signed short)
-                            vec_mergeh((vector unsigned char)vzero, srcP1);
-    srcP1B = (vector signed short)
-                            vec_mergel((vector unsigned char)vzero, srcP1);
-
-    srcP2A = (vector signed short)
-                            vec_mergeh((vector unsigned char)vzero, srcP2);
-    srcP2B = (vector signed short)
-                            vec_mergel((vector unsigned char)vzero, srcP2);
-    srcP3A = (vector signed short)
-                            vec_mergeh((vector unsigned char)vzero, srcP3);
-    srcP3B = (vector signed short)
-                            vec_mergel((vector unsigned char)vzero, srcP3);
-
-    srcM1A = (vector signed short)
-                            vec_mergeh((vector unsigned char)vzero, srcM1);
-    srcM1B = (vector signed short)
-                            vec_mergel((vector unsigned char)vzero, srcM1);
-    srcM2A = (vector signed short)
-                            vec_mergeh((vector unsigned char)vzero, srcM2);
-    srcM2B = (vector signed short)
-                            vec_mergel((vector unsigned char)vzero, srcM2);
-
-    sum1A = vec_adds(srcP0A, srcP1A);
-    sum1B = vec_adds(srcP0B, srcP1B);
-    sum2A = vec_adds(srcM1A, srcP2A);
-    sum2B = vec_adds(srcM1B, srcP2B);
-    sum3A = vec_adds(srcM2A, srcP3A);
-    sum3B = vec_adds(srcM2B, srcP3B);
-
-    pp1A = vec_mladd(sum1A, v20ss, sum3A);
-    pp1B = vec_mladd(sum1B, v20ss, sum3B);
-
-    pp2A = vec_mladd(sum2A, v5ss, (vector signed short)vzero);
-    pp2B = vec_mladd(sum2B, v5ss, (vector signed short)vzero);
-
-    psumA = vec_sub(pp1A, pp2A);
-    psumB = vec_sub(pp1B, pp2B);
-
-    vec_st(psumA, 0, tmp);
-    vec_st(psumB, 16, tmp);
-
-    src += srcStride;
-    tmp += tmpStride; /* int16_t*, and stride is 16, so it's OK here */
-  }
-
-  tmpM2ssA = vec_ld(0, tmpbis);
-  tmpM2ssB = vec_ld(16, tmpbis);
-  tmpbis += tmpStride;
-  tmpM1ssA = vec_ld(0, tmpbis);
-  tmpM1ssB = vec_ld(16, tmpbis);
-  tmpbis += tmpStride;
-  tmpP0ssA = vec_ld(0, tmpbis);
-  tmpP0ssB = vec_ld(16, tmpbis);
-  tmpbis += tmpStride;
-  tmpP1ssA = vec_ld(0, tmpbis);
-  tmpP1ssB = vec_ld(16, tmpbis);
-  tmpbis += tmpStride;
-  tmpP2ssA = vec_ld(0, tmpbis);
-  tmpP2ssB = vec_ld(16, tmpbis);
-  tmpbis += tmpStride;
-
-  for (i = 0 ; i &lt; 16 ; i++) {
-    const vector signed short tmpP3ssA = vec_ld(0, tmpbis);
-    const vector signed short tmpP3ssB = vec_ld(16, tmpbis);
-
-    const vector signed short sum1A = vec_adds(tmpP0ssA, tmpP1ssA);
-    const vector signed short sum1B = vec_adds(tmpP0ssB, tmpP1ssB);
-    const vector signed short sum2A = vec_adds(tmpM1ssA, tmpP2ssA);
-    const vector signed short sum2B = vec_adds(tmpM1ssB, tmpP2ssB);
-    const vector signed short sum3A = vec_adds(tmpM2ssA, tmpP3ssA);
-    const vector signed short sum3B = vec_adds(tmpM2ssB, tmpP3ssB);
-
-    tmpbis += tmpStride;
-
-    tmpM2ssA = tmpM1ssA;
-    tmpM2ssB = tmpM1ssB;
-    tmpM1ssA = tmpP0ssA;
-    tmpM1ssB = tmpP0ssB;
-    tmpP0ssA = tmpP1ssA;
-    tmpP0ssB = tmpP1ssB;
-    tmpP1ssA = tmpP2ssA;
-    tmpP1ssB = tmpP2ssB;
-    tmpP2ssA = tmpP3ssA;
-    tmpP2ssB = tmpP3ssB;
-
-    pp1Ae = vec_mule(sum1A, v20ss);
-    pp1Ao = vec_mulo(sum1A, v20ss);
-    pp1Be = vec_mule(sum1B, v20ss);
-    pp1Bo = vec_mulo(sum1B, v20ss);
-
-    pp2Ae = vec_mule(sum2A, v5ss);
-    pp2Ao = vec_mulo(sum2A, v5ss);
-    pp2Be = vec_mule(sum2B, v5ss);
-    pp2Bo = vec_mulo(sum2B, v5ss);
-
-    pp3Ae = vec_sra((vector signed int)sum3A, v16ui);
-    pp3Ao = vec_mulo(sum3A, v1ss);
-    pp3Be = vec_sra((vector signed int)sum3B, v16ui);
-    pp3Bo = vec_mulo(sum3B, v1ss);
-
-    pp1cAe = vec_add(pp1Ae, v512si);
-    pp1cAo = vec_add(pp1Ao, v512si);
-    pp1cBe = vec_add(pp1Be, v512si);
-    pp1cBo = vec_add(pp1Bo, v512si);
-
-    pp32Ae = vec_sub(pp3Ae, pp2Ae);
-    pp32Ao = vec_sub(pp3Ao, pp2Ao);
-    pp32Be = vec_sub(pp3Be, pp2Be);
-    pp32Bo = vec_sub(pp3Bo, pp2Bo);
-
-    sumAe = vec_add(pp1cAe, pp32Ae);
-    sumAo = vec_add(pp1cAo, pp32Ao);
-    sumBe = vec_add(pp1cBe, pp32Be);
-    sumBo = vec_add(pp1cBo, pp32Bo);
-
-    ssumAe = vec_sra(sumAe, v10ui);
-    ssumAo = vec_sra(sumAo, v10ui);
-    ssumBe = vec_sra(sumBe, v10ui);
-    ssumBo = vec_sra(sumBo, v10ui);
-
-    ssume = vec_packs(ssumAe, ssumBe);
-    ssumo = vec_packs(ssumAo, ssumBo);
-
-    sumv = vec_packsu(ssume, ssumo);
-    sum = vec_perm(sumv, sumv, mperm);
-
-    dst1 = vec_ld(0, dst);
-    dst2 = vec_ld(16, dst);
-    vdst = vec_perm(dst1, dst2, vec_lvsl(0, dst));
-
-    OP_U8_ALTIVEC(fsum, sum, vdst);
-
-    rsum = vec_perm(fsum, fsum, dstperm);
-    fdst1 = vec_sel(dst1, rsum, dstmask);
-    fdst2 = vec_sel(rsum, dst2, dstmask);
-
-    vec_st(fdst1, 0, dst);
-    vec_st(fdst2, 16, dst);
-
-    dst += dstStride;
-  }
-  POWERPC_PERF_STOP_COUNT(PREFIX_h264_qpel16_hv_lowpass_num, 1);
-}

Modified: branches/avidemux_2.4_branch/avidemux/ADM_libraries/ADM_lavcodec/ppc/dsputil_ppc.c
===================================================================
--- branches/avidemux_2.4_branch/avidemux/ADM_libraries/ADM_lavcodec/ppc/dsputil_ppc.c	2007-12-03 12:38:06 UTC (rev 3719)
+++ branches/avidemux_2.4_branch/avidemux/ADM_libraries/ADM_lavcodec/ppc/dsputil_ppc.c	2007-12-04 00:19:48 UTC (rev 3720)
@@ -20,7 +20,7 @@
  * Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA
  */
 
-#include &quot;../dsputil.h&quot;
+#include &quot;dsputil.h&quot;
 
 #include &quot;dsputil_ppc.h&quot;
 
@@ -39,6 +39,7 @@
 void vc1dsp_init_altivec(DSPContext* c, AVCodecContext *avctx);
 void snow_init_altivec(DSPContext* c, AVCodecContext *avctx);
 void float_init_altivec(DSPContext* c, AVCodecContext *avctx);
+void int_init_altivec(DSPContext* c, AVCodecContext *avctx);
 
 #endif
 
@@ -55,7 +56,7 @@
     return result;
 }
 
-#ifdef POWERPC_PERFORMANCE_REPORT
+#ifdef CONFIG_POWERPC_PERF
 unsigned long long perfdata[POWERPC_NUM_PMC_ENABLED][powerpc_perf_total][powerpc_data_total];
 /* list below must match enum in dsputil_ppc.h */
 static unsigned char* perfname[] = {
@@ -90,7 +91,7 @@
 #include &lt;stdio.h&gt;
 #endif
 
-#ifdef POWERPC_PERFORMANCE_REPORT
+#ifdef CONFIG_POWERPC_PERF
 void powerpc_display_perf_report(void)
 {
   int i, j;
@@ -112,7 +113,7 @@
       }
   }
 }
-#endif /* POWERPC_PERFORMANCE_REPORT */
+#endif /* CONFIG_POWERPC_PERF */
 
 /* ***** WARNING ***** WARNING ***** WARNING ***** */
 /*
@@ -284,6 +285,7 @@
         if(ENABLE_VC1_DECODER || ENABLE_WMV3_DECODER)
             vc1dsp_init_altivec(c, avctx);
         float_init_altivec(c, avctx);
+        int_init_altivec(c, avctx);
         c-&gt;gmc1 = gmc1_altivec;
 
 #ifdef CONFIG_ENCODERS
@@ -305,7 +307,7 @@
         }
         }
 
-#ifdef POWERPC_PERFORMANCE_REPORT
+#ifdef CONFIG_POWERPC_PERF
         {
           int i, j;
           for (i = 0 ; i &lt; powerpc_perf_total ; i++)
@@ -319,7 +321,7 @@
               }
           }
         }
-#endif /* POWERPC_PERFORMANCE_REPORT */
+#endif /* CONFIG_POWERPC_PERF */
     }
 #endif /* HAVE_ALTIVEC */
 }

Modified: branches/avidemux_2.4_branch/avidemux/ADM_libraries/ADM_lavcodec/ppc/dsputil_ppc.h
===================================================================
--- branches/avidemux_2.4_branch/avidemux/ADM_libraries/ADM_lavcodec/ppc/dsputil_ppc.h	2007-12-03 12:38:06 UTC (rev 3719)
+++ branches/avidemux_2.4_branch/avidemux/ADM_libraries/ADM_lavcodec/ppc/dsputil_ppc.h	2007-12-04 00:19:48 UTC (rev 3720)
@@ -21,11 +21,11 @@
 #ifndef _DSPUTIL_PPC_
 #define _DSPUTIL_PPC_
 
-#ifdef POWERPC_PERFORMANCE_REPORT
+#ifdef CONFIG_POWERPC_PERF
 void powerpc_display_perf_report(void);
 /* the 604* have 2, the G3* have 4, the G4s have 6,
    and the G5 are completely different (they MUST use
-   POWERPC_MODE_64BITS, and let's hope all future 64 bis PPC
+   HAVE_PPC64, and let's hope all future 64 bis PPC
    will use the same PMCs... */
 #define POWERPC_NUM_PMC_ENABLED 6
 /* if you add to the enum below, also add to the perfname array
@@ -68,7 +68,7 @@
 };
 extern unsigned long long perfdata[POWERPC_NUM_PMC_ENABLED][powerpc_perf_total][powerpc_data_total];
 
-#ifndef POWERPC_MODE_64BITS
+#ifndef HAVE_PPC64
 #define POWERP_PMC_DATATYPE unsigned long
 #define POWERPC_GET_PMC1(a) asm volatile(&quot;mfspr %0, 937&quot; : &quot;=r&quot; (a))
 #define POWERPC_GET_PMC2(a) asm volatile(&quot;mfspr %0, 938&quot; : &quot;=r&quot; (a))
@@ -86,7 +86,7 @@
 #define POWERPC_GET_PMC5(a) do {} while (0)
 #define POWERPC_GET_PMC6(a) do {} while (0)
 #endif
-#else /* POWERPC_MODE_64BITS */
+#else /* HAVE_PPC64 */
 #define POWERP_PMC_DATATYPE unsigned long long
 #define POWERPC_GET_PMC1(a) asm volatile(&quot;mfspr %0, 771&quot; : &quot;=r&quot; (a))
 #define POWERPC_GET_PMC2(a) asm volatile(&quot;mfspr %0, 772&quot; : &quot;=r&quot; (a))
@@ -104,7 +104,7 @@
 #define POWERPC_GET_PMC5(a) do {} while (0)
 #define POWERPC_GET_PMC6(a) do {} while (0)
 #endif
-#endif /* POWERPC_MODE_64BITS */
+#endif /* HAVE_PPC64 */
 #define POWERPC_PERF_DECLARE(a, cond)   \
   POWERP_PMC_DATATYPE                   \
     pmc_start[POWERPC_NUM_PMC_ENABLED], \
@@ -145,11 +145,11 @@
     }                             \
   }                               \
 } while (0)
-#else /* POWERPC_PERFORMANCE_REPORT */
+#else /* CONFIG_POWERPC_PERF */
 // those are needed to avoid empty statements.
 #define POWERPC_PERF_DECLARE(a, cond)        int altivec_placeholder __attribute__ ((unused))
 #define POWERPC_PERF_START_COUNT(a, cond)    do {} while (0)
 #define POWERPC_PERF_STOP_COUNT(a, cond)     do {} while (0)
-#endif /* POWERPC_PERFORMANCE_REPORT */
+#endif /* CONFIG_POWERPC_PERF */
 
 #endif /*  _DSPUTIL_PPC_ */

Modified: branches/avidemux_2.4_branch/avidemux/ADM_libraries/ADM_lavcodec/ppc/fdct_altivec.c
===================================================================
--- branches/avidemux_2.4_branch/avidemux/ADM_libraries/ADM_lavcodec/ppc/fdct_altivec.c	2007-12-03 12:38:06 UTC (rev 3719)
+++ branches/avidemux_2.4_branch/avidemux/ADM_libraries/ADM_lavcodec/ppc/fdct_altivec.c	2007-12-04 00:19:48 UTC (rev 3720)
@@ -21,7 +21,7 @@
 
 
 #include &quot;common.h&quot;
-#include &quot;../dsputil.h&quot;
+#include &quot;dsputil.h&quot;
 #include &quot;dsputil_altivec.h&quot;
 #include &quot;gcc_fixes.h&quot;
 

Modified: branches/avidemux_2.4_branch/avidemux/ADM_libraries/ADM_lavcodec/ppc/fft_altivec.c
===================================================================
--- branches/avidemux_2.4_branch/avidemux/ADM_libraries/ADM_lavcodec/ppc/fft_altivec.c	2007-12-03 12:38:06 UTC (rev 3719)
+++ branches/avidemux_2.4_branch/avidemux/ADM_libraries/ADM_lavcodec/ppc/fft_altivec.c	2007-12-04 00:19:48 UTC (rev 3720)
@@ -20,7 +20,7 @@
  * License along with FFmpeg; if not, write to the Free Software
  * Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA
  */
-#include &quot;../dsputil.h&quot;
+#include &quot;dsputil.h&quot;
 
 #include &quot;gcc_fixes.h&quot;
 

Added: branches/avidemux_2.4_branch/avidemux/ADM_libraries/ADM_lavcodec/ppc/float_altivec.c
===================================================================
--- branches/avidemux_2.4_branch/avidemux/ADM_libraries/ADM_lavcodec/ppc/float_altivec.c	2007-12-03 12:38:06 UTC (rev 3719)
+++ branches/avidemux_2.4_branch/avidemux/ADM_libraries/ADM_lavcodec/ppc/float_altivec.c	2007-12-04 00:19:48 UTC (rev 3720)
@@ -0,0 +1,193 @@
+/*
+ * Copyright (c) 2006 Luca Barbato &lt;<A HREF="https://lists.berlios.de/mailman/listinfo/avidemux-svn-commit">lu_zero at gentoo.org</A>&gt;
+ *
+ * This file is part of FFmpeg.
+ *
+ * FFmpeg is free software; you can redistribute it and/or
+ * modify it under the terms of the GNU Lesser General Public
+ * License as published by the Free Software Foundation; either
+ * version 2.1 of the License, or (at your option) any later version.
+ *
+ * FFmpeg is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+ * Lesser General Public License for more details.
+ *
+ * You should have received a copy of the GNU Lesser General Public
+ * License along with FFmpeg; if not, write to the Free Software
+ * Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA
+ */
+
+#include &quot;dsputil.h&quot;
+
+#include &quot;gcc_fixes.h&quot;
+
+#include &quot;dsputil_altivec.h&quot;
+
+static void vector_fmul_altivec(float *dst, const float *src, int len)
+{
+    int i;
+    vector float d0, d1, s, zero = (vector float)vec_splat_u32(0);
+    for(i=0; i&lt;len-7; i+=8) {
+        d0 = vec_ld(0, dst+i);
+        s = vec_ld(0, src+i);
+        d1 = vec_ld(16, dst+i);
+        d0 = vec_madd(d0, s, zero);
+        d1 = vec_madd(d1, vec_ld(16,src+i), zero);
+        vec_st(d0, 0, dst+i);
+        vec_st(d1, 16, dst+i);
+    }
+}
+
+static void vector_fmul_reverse_altivec(float *dst, const float *src0,
+                                        const float *src1, int len)
+{
+    int i;
+    vector float d, s0, s1, h0, l0,
+                 s2, s3, zero = (vector float)vec_splat_u32(0);
+    src1 += len-4;
+    for(i=0; i&lt;len-7; i+=8) {
+        s1 = vec_ld(0, src1-i);              // [a,b,c,d]
+        s0 = vec_ld(0, src0+i);
+        l0 = vec_mergel(s1, s1);             // [c,c,d,d]
+        s3 = vec_ld(-16, src1-i);
+        h0 = vec_mergeh(s1, s1);             // [a,a,b,b]
+        s2 = vec_ld(16, src0+i);
+        s1 = vec_mergeh(vec_mergel(l0,h0),   // [d,b,d,b]
+                        vec_mergeh(l0,h0));  // [c,a,c,a]
+                                             // [d,c,b,a]
+        l0 = vec_mergel(s3, s3);
+        d = vec_madd(s0, s1, zero);
+        h0 = vec_mergeh(s3, s3);
+        vec_st(d, 0, dst+i);
+        s3 = vec_mergeh(vec_mergel(l0,h0),
+                        vec_mergeh(l0,h0));
+        d = vec_madd(s2, s3, zero);
+        vec_st(d, 16, dst+i);
+    }
+}
+
+static void vector_fmul_add_add_altivec(float *dst, const float *src0,
+                                        const float *src1, const float *src2,
+                                        int src3, int len, int step)
+{
+    int i;
+    vector float d, s0, s1, s2, t0, t1, edges;
+    vector unsigned char align = vec_lvsr(0,dst),
+                         mask = vec_lvsl(0, dst);
+
+#if 0 //FIXME: there is still something wrong
+    if (step == 2) {
+        int y;
+        vector float d0, d1, s3, t2;
+        vector unsigned int sel =
+                vec_mergeh(vec_splat_u32(-1), vec_splat_u32(0));
+        t1 = vec_ld(16, dst);
+        for (i=0,y=0; i&lt;len-3; i+=4,y+=8) {
+
+            s0 = vec_ld(0,src0+i);
+            s1 = vec_ld(0,src1+i);
+            s2 = vec_ld(0,src2+i);
+
+//          t0 = vec_ld(0, dst+y);  //[x x x|a]
+//          t1 = vec_ld(16, dst+y); //[b c d|e]
+            t2 = vec_ld(31, dst+y); //[f g h|x]
+
+            d = vec_madd(s0,s1,s2); // [A B C D]
+
+                                                 // [A A B B]
+
+                                                 // [C C D D]
+
+            d0 = vec_perm(t0, t1, mask); // [a b c d]
+
+            d0 = vec_sel(vec_mergeh(d, d), d0, sel);   // [A b B d]
+
+            edges = vec_perm(t1, t0, mask);
+
+            t0 = vec_perm(edges, d0, align); // [x x x|A]
+
+            t1 = vec_perm(d0, edges, align); // [b B d|e]
+
+            vec_stl(t0, 0, dst+y);
+
+            d1 = vec_perm(t1, t2, mask); // [e f g h]
+
+            d1 = vec_sel(vec_mergel(d, d), d1, sel); // [C f D h]
+
+            edges = vec_perm(t2, t1, mask);
+
+            t1 = vec_perm(edges, d1, align); // [b B d|C]
+
+            t2 = vec_perm(d1, edges, align); // [f D h|x]
+
+            vec_stl(t1, 16, dst+y);
+
+            t0 = t1;
+
+            vec_stl(t2, 31, dst+y);
+
+            t1 = t2;
+        }
+    } else
+    #endif
+    if (step == 1 &amp;&amp; src3 == 0)
+        for (i=0; i&lt;len-3; i+=4) {
+            t0 = vec_ld(0, dst+i);
+            t1 = vec_ld(15, dst+i);
+            s0 = vec_ld(0, src0+i);
+            s1 = vec_ld(0, src1+i);
+            s2 = vec_ld(0, src2+i);
+            edges = vec_perm(t1 ,t0, mask);
+            d = vec_madd(s0,s1,s2);
+            t1 = vec_perm(d, edges, align);
+            t0 = vec_perm(edges, d, align);
+            vec_st(t1, 15, dst+i);
+            vec_st(t0, 0, dst+i);
+        }
+    else
+        ff_vector_fmul_add_add_c(dst, src0, src1, src2, src3, len, step);
+}
+
+void float_to_int16_altivec(int16_t *dst, const float *src, int len)
+{
+    int i;
+    vector float s0, s1;
+    vector signed int t0, t1;
+    vector signed short d0, d1, d;
+    vector unsigned char align;
+    if(((long)dst)&amp;15) //FIXME
+    for(i=0; i&lt;len-7; i+=8) {
+        s0 = vec_ld(0, src+i);
+        s1 = vec_ld(16, src+i);
+        t0 = vec_cts(s0, 0);
+        d0 = vec_ld(0, dst+i);
+        t1 = vec_cts(s1, 0);
+        d1 = vec_ld(15, dst+i);
+        d = vec_packs(t0,t1);
+        d1 = vec_perm(d1, d0, vec_lvsl(0,dst+i));
+        align = vec_lvsr(0, dst+i);
+        d0 = vec_perm(d1, d, align);
+        d1 = vec_perm(d, d1, align);
+        vec_st(d0, 0, dst+i);
+        vec_st(d1,15, dst+i);
+    }
+    else
+    for(i=0; i&lt;len-7; i+=8) {
+        s0 = vec_ld(0, src+i);
+        s1 = vec_ld(16, src+i);
+        t0 = vec_cts(s0, 0);
+        t1 = vec_cts(s1, 0);
+        d = vec_packs(t0,t1);
+        vec_st(d, 0, dst+i);
+    }
+}
+
+void float_init_altivec(DSPContext* c, AVCodecContext *avctx)
+{
+    c-&gt;vector_fmul = vector_fmul_altivec;
+    c-&gt;vector_fmul_reverse = vector_fmul_reverse_altivec;
+    c-&gt;vector_fmul_add_add = vector_fmul_add_add_altivec;
+    if(!(avctx-&gt;flags &amp; CODEC_FLAG_BITEXACT))
+        c-&gt;float_to_int16 = float_to_int16_altivec;
+}

Modified: branches/avidemux_2.4_branch/avidemux/ADM_libraries/ADM_lavcodec/ppc/gmc_altivec.c
===================================================================
--- branches/avidemux_2.4_branch/avidemux/ADM_libraries/ADM_lavcodec/ppc/gmc_altivec.c	2007-12-03 12:38:06 UTC (rev 3719)
+++ branches/avidemux_2.4_branch/avidemux/ADM_libraries/ADM_lavcodec/ppc/gmc_altivec.c	2007-12-04 00:19:48 UTC (rev 3720)
@@ -20,7 +20,7 @@
  * Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA
  */
 
-#include &quot;../dsputil.h&quot;
+#include &quot;dsputil.h&quot;
 
 #include &quot;gcc_fixes.h&quot;
 
@@ -34,10 +34,10 @@
 void gmc1_altivec(uint8_t *dst /* align 8 */, uint8_t *src /* align1 */, int stride, int h, int x16, int y16, int rounder)
 {
 POWERPC_PERF_DECLARE(altivec_gmc1_num, GMC1_PERF_COND);
-    const unsigned short __attribute__ ((aligned(16))) rounder_a[8] =
+    const DECLARE_ALIGNED_16(unsigned short, rounder_a[8]) =
       {rounder, rounder, rounder, rounder,
        rounder, rounder, rounder, rounder};
-    const unsigned short __attribute__ ((aligned(16))) ABCD[8] =
+    const DECLARE_ALIGNED_16(unsigned short, ABCD[8]) =
       {
         (16-x16)*(16-y16), /* A */
         (   x16)*(16-y16), /* B */

Copied: branches/avidemux_2.4_branch/avidemux/ADM_libraries/ADM_lavcodec/ppc/h264_altivec.c (from rev 3719, branches/avidemux_2.4_branch/avidemux/ADM_libraries/ADM_lavcodec/ppc/dsputil_h264_altivec.c)
===================================================================
--- branches/avidemux_2.4_branch/avidemux/ADM_libraries/ADM_lavcodec/ppc/dsputil_h264_altivec.c	2007-12-03 12:38:06 UTC (rev 3719)
+++ branches/avidemux_2.4_branch/avidemux/ADM_libraries/ADM_lavcodec/ppc/h264_altivec.c	2007-12-04 00:19:48 UTC (rev 3720)
@@ -0,0 +1,917 @@
+/*
+ * Copyright (c) 2004 Romain Dolbeau &lt;<A HREF="https://lists.berlios.de/mailman/listinfo/avidemux-svn-commit">romain at dolbeau.org</A>&gt;
+ *
+ * This file is part of FFmpeg.
+ *
+ * FFmpeg is free software; you can redistribute it and/or
+ * modify it under the terms of the GNU Lesser General Public
+ * License as published by the Free Software Foundation; either
+ * version 2.1 of the License, or (at your option) any later version.
+ *
+ * FFmpeg is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+ * Lesser General Public License for more details.
+ *
+ * You should have received a copy of the GNU Lesser General Public
+ * License along with FFmpeg; if not, write to the Free Software
+ * Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA
+ */
+
+#include &quot;dsputil.h&quot;
+
+#include &quot;gcc_fixes.h&quot;
+
+#include &quot;dsputil_altivec.h&quot;
+#include &quot;types_altivec.h&quot;
+
+#define PUT_OP_U8_ALTIVEC(d, s, dst) d = s
+#define AVG_OP_U8_ALTIVEC(d, s, dst) d = vec_avg(dst, s)
+
+#define OP_U8_ALTIVEC                          PUT_OP_U8_ALTIVEC
+#define PREFIX_h264_chroma_mc8_altivec         put_h264_chroma_mc8_altivec
+#define PREFIX_h264_chroma_mc8_num             altivec_put_h264_chroma_mc8_num
+#define PREFIX_h264_qpel16_h_lowpass_altivec   put_h264_qpel16_h_lowpass_altivec
+#define PREFIX_h264_qpel16_h_lowpass_num       altivec_put_h264_qpel16_h_lowpass_num
+#define PREFIX_h264_qpel16_v_lowpass_altivec   put_h264_qpel16_v_lowpass_altivec
+#define PREFIX_h264_qpel16_v_lowpass_num       altivec_put_h264_qpel16_v_lowpass_num
+#define PREFIX_h264_qpel16_hv_lowpass_altivec  put_h264_qpel16_hv_lowpass_altivec
+#define PREFIX_h264_qpel16_hv_lowpass_num      altivec_put_h264_qpel16_hv_lowpass_num
+#include &quot;h264_template_altivec.c&quot;
+#undef OP_U8_ALTIVEC
+#undef PREFIX_h264_chroma_mc8_altivec
+#undef PREFIX_h264_chroma_mc8_num
+#undef PREFIX_h264_qpel16_h_lowpass_altivec
+#undef PREFIX_h264_qpel16_h_lowpass_num
+#undef PREFIX_h264_qpel16_v_lowpass_altivec
+#undef PREFIX_h264_qpel16_v_lowpass_num
+#undef PREFIX_h264_qpel16_hv_lowpass_altivec
+#undef PREFIX_h264_qpel16_hv_lowpass_num
+
+#define OP_U8_ALTIVEC                          AVG_OP_U8_ALTIVEC
+#define PREFIX_h264_chroma_mc8_altivec         avg_h264_chroma_mc8_altivec
+#define PREFIX_h264_chroma_mc8_num             altivec_avg_h264_chroma_mc8_num
+#define PREFIX_h264_qpel16_h_lowpass_altivec   avg_h264_qpel16_h_lowpass_altivec
+#define PREFIX_h264_qpel16_h_lowpass_num       altivec_avg_h264_qpel16_h_lowpass_num
+#define PREFIX_h264_qpel16_v_lowpass_altivec   avg_h264_qpel16_v_lowpass_altivec
+#define PREFIX_h264_qpel16_v_lowpass_num       altivec_avg_h264_qpel16_v_lowpass_num
+#define PREFIX_h264_qpel16_hv_lowpass_altivec  avg_h264_qpel16_hv_lowpass_altivec
+#define PREFIX_h264_qpel16_hv_lowpass_num      altivec_avg_h264_qpel16_hv_lowpass_num
+#include &quot;h264_template_altivec.c&quot;
+#undef OP_U8_ALTIVEC
+#undef PREFIX_h264_chroma_mc8_altivec
+#undef PREFIX_h264_chroma_mc8_num
+#undef PREFIX_h264_qpel16_h_lowpass_altivec
+#undef PREFIX_h264_qpel16_h_lowpass_num
+#undef PREFIX_h264_qpel16_v_lowpass_altivec
+#undef PREFIX_h264_qpel16_v_lowpass_num
+#undef PREFIX_h264_qpel16_hv_lowpass_altivec
+#undef PREFIX_h264_qpel16_hv_lowpass_num
+
+#define H264_MC(OPNAME, SIZE, CODETYPE) \
+static void OPNAME ## h264_qpel ## SIZE ## _mc00_ ## CODETYPE (uint8_t *dst, uint8_t *src, int stride){\
+    OPNAME ## pixels ## SIZE ## _ ## CODETYPE(dst, src, stride, SIZE);\
+}\
+\
+static void OPNAME ## h264_qpel ## SIZE ## _mc10_ ## CODETYPE(uint8_t *dst, uint8_t *src, int stride){ \
+    DECLARE_ALIGNED_16(uint8_t, half[SIZE*SIZE]);\
+    put_h264_qpel ## SIZE ## _h_lowpass_ ## CODETYPE(half, src, SIZE, stride);\
+    OPNAME ## pixels ## SIZE ## _l2_ ## CODETYPE(dst, src, half, stride, stride, SIZE);\
+}\
+\
+static void OPNAME ## h264_qpel ## SIZE ## _mc20_ ## CODETYPE(uint8_t *dst, uint8_t *src, int stride){\
+    OPNAME ## h264_qpel ## SIZE ## _h_lowpass_ ## CODETYPE(dst, src, stride, stride);\
+}\
+\
+static void OPNAME ## h264_qpel ## SIZE ## _mc30_ ## CODETYPE(uint8_t *dst, uint8_t *src, int stride){\
+    DECLARE_ALIGNED_16(uint8_t, half[SIZE*SIZE]);\
+    put_h264_qpel ## SIZE ## _h_lowpass_ ## CODETYPE(half, src, SIZE, stride);\
+    OPNAME ## pixels ## SIZE ## _l2_ ## CODETYPE(dst, src+1, half, stride, stride, SIZE);\
+}\
+\
+static void OPNAME ## h264_qpel ## SIZE ## _mc01_ ## CODETYPE(uint8_t *dst, uint8_t *src, int stride){\
+    DECLARE_ALIGNED_16(uint8_t, half[SIZE*SIZE]);\
+    put_h264_qpel ## SIZE ## _v_lowpass_ ## CODETYPE(half, src, SIZE, stride);\
+    OPNAME ## pixels ## SIZE ## _l2_ ## CODETYPE(dst, src, half, stride, stride, SIZE);\
+}\
+\
+static void OPNAME ## h264_qpel ## SIZE ## _mc02_ ## CODETYPE(uint8_t *dst, uint8_t *src, int stride){\
+    OPNAME ## h264_qpel ## SIZE ## _v_lowpass_ ## CODETYPE(dst, src, stride, stride);\
+}\
+\
+static void OPNAME ## h264_qpel ## SIZE ## _mc03_ ## CODETYPE(uint8_t *dst, uint8_t *src, int stride){\
+    DECLARE_ALIGNED_16(uint8_t, half[SIZE*SIZE]);\
+    put_h264_qpel ## SIZE ## _v_lowpass_ ## CODETYPE(half, src, SIZE, stride);\
+    OPNAME ## pixels ## SIZE ## _l2_ ## CODETYPE(dst, src+stride, half, stride, stride, SIZE);\
+}\
+\
+static void OPNAME ## h264_qpel ## SIZE ## _mc11_ ## CODETYPE(uint8_t *dst, uint8_t *src, int stride){\
+    DECLARE_ALIGNED_16(uint8_t, halfH[SIZE*SIZE]);\
+    DECLARE_ALIGNED_16(uint8_t, halfV[SIZE*SIZE]);\
+    put_h264_qpel ## SIZE ## _h_lowpass_ ## CODETYPE(halfH, src, SIZE, stride);\
+    put_h264_qpel ## SIZE ## _v_lowpass_ ## CODETYPE(halfV, src, SIZE, stride);\
+    OPNAME ## pixels ## SIZE ## _l2_ ## CODETYPE(dst, halfH, halfV, stride, SIZE, SIZE);\
+}\
+\
+static void OPNAME ## h264_qpel ## SIZE ## _mc31_ ## CODETYPE(uint8_t *dst, uint8_t *src, int stride){\
+    DECLARE_ALIGNED_16(uint8_t, halfH[SIZE*SIZE]);\
+    DECLARE_ALIGNED_16(uint8_t, halfV[SIZE*SIZE]);\
+    put_h264_qpel ## SIZE ## _h_lowpass_ ## CODETYPE(halfH, src, SIZE, stride);\
+    put_h264_qpel ## SIZE ## _v_lowpass_ ## CODETYPE(halfV, src+1, SIZE, stride);\
+    OPNAME ## pixels ## SIZE ## _l2_ ## CODETYPE(dst, halfH, halfV, stride, SIZE, SIZE);\
+}\
+\
+static void OPNAME ## h264_qpel ## SIZE ## _mc13_ ## CODETYPE(uint8_t *dst, uint8_t *src, int stride){\
+    DECLARE_ALIGNED_16(uint8_t, halfH[SIZE*SIZE]);\
+    DECLARE_ALIGNED_16(uint8_t, halfV[SIZE*SIZE]);\
+    put_h264_qpel ## SIZE ## _h_lowpass_ ## CODETYPE(halfH, src + stride, SIZE, stride);\
+    put_h264_qpel ## SIZE ## _v_lowpass_ ## CODETYPE(halfV, src, SIZE, stride);\
+    OPNAME ## pixels ## SIZE ## _l2_ ## CODETYPE(dst, halfH, halfV, stride, SIZE, SIZE);\
+}\
+\
+static void OPNAME ## h264_qpel ## SIZE ## _mc33_ ## CODETYPE(uint8_t *dst, uint8_t *src, int stride){\
+    DECLARE_ALIGNED_16(uint8_t, halfH[SIZE*SIZE]);\
+    DECLARE_ALIGNED_16(uint8_t, halfV[SIZE*SIZE]);\
+    put_h264_qpel ## SIZE ## _h_lowpass_ ## CODETYPE(halfH, src + stride, SIZE, stride);\
+    put_h264_qpel ## SIZE ## _v_lowpass_ ## CODETYPE(halfV, src+1, SIZE, stride);\
+    OPNAME ## pixels ## SIZE ## _l2_ ## CODETYPE(dst, halfH, halfV, stride, SIZE, SIZE);\
+}\
+\
+static void OPNAME ## h264_qpel ## SIZE ## _mc22_ ## CODETYPE(uint8_t *dst, uint8_t *src, int stride){\
+    DECLARE_ALIGNED_16(int16_t, tmp[SIZE*(SIZE+8)]);\
+    OPNAME ## h264_qpel ## SIZE ## _hv_lowpass_ ## CODETYPE(dst, tmp, src, stride, SIZE, stride);\
+}\
+\
+static void OPNAME ## h264_qpel ## SIZE ## _mc21_ ## CODETYPE(uint8_t *dst, uint8_t *src, int stride){\
+    DECLARE_ALIGNED_16(uint8_t, halfH[SIZE*SIZE]);\
+    DECLARE_ALIGNED_16(uint8_t, halfHV[SIZE*SIZE]);\
+    DECLARE_ALIGNED_16(int16_t, tmp[SIZE*(SIZE+8)]);\
+    put_h264_qpel ## SIZE ## _h_lowpass_ ## CODETYPE(halfH, src, SIZE, stride);\
+    put_h264_qpel ## SIZE ## _hv_lowpass_ ## CODETYPE(halfHV, tmp, src, SIZE, SIZE, stride);\
+    OPNAME ## pixels ## SIZE ## _l2_ ## CODETYPE(dst, halfH, halfHV, stride, SIZE, SIZE);\
+}\
+\
+static void OPNAME ## h264_qpel ## SIZE ## _mc23_ ## CODETYPE(uint8_t *dst, uint8_t *src, int stride){\
+    DECLARE_ALIGNED_16(uint8_t, halfH[SIZE*SIZE]);\
+    DECLARE_ALIGNED_16(uint8_t, halfHV[SIZE*SIZE]);\
+    DECLARE_ALIGNED_16(int16_t, tmp[SIZE*(SIZE+8)]);\
+    put_h264_qpel ## SIZE ## _h_lowpass_ ## CODETYPE(halfH, src + stride, SIZE, stride);\
+    put_h264_qpel ## SIZE ## _hv_lowpass_ ## CODETYPE(halfHV, tmp, src, SIZE, SIZE, stride);\
+    OPNAME ## pixels ## SIZE ## _l2_ ## CODETYPE(dst, halfH, halfHV, stride, SIZE, SIZE);\
+}\
+\
+static void OPNAME ## h264_qpel ## SIZE ## _mc12_ ## CODETYPE(uint8_t *dst, uint8_t *src, int stride){\
+    DECLARE_ALIGNED_16(uint8_t, halfV[SIZE*SIZE]);\
+    DECLARE_ALIGNED_16(uint8_t, halfHV[SIZE*SIZE]);\
+    DECLARE_ALIGNED_16(int16_t, tmp[SIZE*(SIZE+8)]);\
+    put_h264_qpel ## SIZE ## _v_lowpass_ ## CODETYPE(halfV, src, SIZE, stride);\
+    put_h264_qpel ## SIZE ## _hv_lowpass_ ## CODETYPE(halfHV, tmp, src, SIZE, SIZE, stride);\
+    OPNAME ## pixels ## SIZE ## _l2_ ## CODETYPE(dst, halfV, halfHV, stride, SIZE, SIZE);\
+}\
+\
+static void OPNAME ## h264_qpel ## SIZE ## _mc32_ ## CODETYPE(uint8_t *dst, uint8_t *src, int stride){\
+    DECLARE_ALIGNED_16(uint8_t, halfV[SIZE*SIZE]);\
+    DECLARE_ALIGNED_16(uint8_t, halfHV[SIZE*SIZE]);\
+    DECLARE_ALIGNED_16(int16_t, tmp[SIZE*(SIZE+8)]);\
+    put_h264_qpel ## SIZE ## _v_lowpass_ ## CODETYPE(halfV, src+1, SIZE, stride);\
+    put_h264_qpel ## SIZE ## _hv_lowpass_ ## CODETYPE(halfHV, tmp, src, SIZE, SIZE, stride);\
+    OPNAME ## pixels ## SIZE ## _l2_ ## CODETYPE(dst, halfV, halfHV, stride, SIZE, SIZE);\
+}\
+
+/* this code assume that stride % 16 == 0 */
+void put_no_rnd_h264_chroma_mc8_altivec(uint8_t * dst, uint8_t * src, int stride, int h, int x, int y) {
+   DECLARE_ALIGNED_16(signed int, ABCD[4]) =
+                        {((8 - x) * (8 - y)),
+                          ((x) * (8 - y)),
+                          ((8 - x) * (y)),
+                          ((x) * (y))};
+    register int i;
+    vector unsigned char fperm;
+    const vector signed int vABCD = vec_ld(0, ABCD);
+    const vector signed short vA = vec_splat((vector signed short)vABCD, 1);
+    const vector signed short vB = vec_splat((vector signed short)vABCD, 3);
+    const vector signed short vC = vec_splat((vector signed short)vABCD, 5);
+    const vector signed short vD = vec_splat((vector signed short)vABCD, 7);
+    const vector signed int vzero = vec_splat_s32(0);
+    const vector signed short v28ss = vec_sub(vec_sl(vec_splat_s16(1),vec_splat_u16(5)),vec_splat_s16(4));
+    const vector unsigned short v6us = vec_splat_u16(6);
+    register int loadSecond = (((unsigned long)src) % 16) &lt;= 7 ? 0 : 1;
+    register int reallyBadAlign = (((unsigned long)src) % 16) == 15 ? 1 : 0;
+
+    vector unsigned char vsrcAuc, vsrcBuc, vsrcperm0, vsrcperm1;
+    vector unsigned char vsrc0uc, vsrc1uc;
+    vector signed short vsrc0ssH, vsrc1ssH;
+    vector unsigned char vsrcCuc, vsrc2uc, vsrc3uc;
+    vector signed short vsrc2ssH, vsrc3ssH, psum;
+    vector unsigned char vdst, ppsum, fsum;
+
+    if (((unsigned long)dst) % 16 == 0) {
+      fperm = (vector unsigned char)AVV(0x10, 0x11, 0x12, 0x13,
+                                        0x14, 0x15, 0x16, 0x17,
+                                        0x08, 0x09, 0x0A, 0x0B,
+                                        0x0C, 0x0D, 0x0E, 0x0F);
+    } else {
+      fperm = (vector unsigned char)AVV(0x00, 0x01, 0x02, 0x03,
+                                        0x04, 0x05, 0x06, 0x07,
+                                        0x18, 0x19, 0x1A, 0x1B,
+                                        0x1C, 0x1D, 0x1E, 0x1F);
+    }
+
+    vsrcAuc = vec_ld(0, src);
+
+    if (loadSecond)
+      vsrcBuc = vec_ld(16, src);
+    vsrcperm0 = vec_lvsl(0, src);
+    vsrcperm1 = vec_lvsl(1, src);
+
+    vsrc0uc = vec_perm(vsrcAuc, vsrcBuc, vsrcperm0);
+    if (reallyBadAlign)
+      vsrc1uc = vsrcBuc;
+    else
+      vsrc1uc = vec_perm(vsrcAuc, vsrcBuc, vsrcperm1);
+
+    vsrc0ssH = (vector signed short)vec_mergeh((vector unsigned char)vzero,
+                                               (vector unsigned char)vsrc0uc);
+    vsrc1ssH = (vector signed short)vec_mergeh((vector unsigned char)vzero,
+                                               (vector unsigned char)vsrc1uc);
+
+    if (!loadSecond) {// -&gt; !reallyBadAlign
+      for (i = 0 ; i &lt; h ; i++) {
+
+
+        vsrcCuc = vec_ld(stride + 0, src);
+
+        vsrc2uc = vec_perm(vsrcCuc, vsrcCuc, vsrcperm0);
+        vsrc3uc = vec_perm(vsrcCuc, vsrcCuc, vsrcperm1);
+
+        vsrc2ssH = (vector signed short)vec_mergeh((vector unsigned char)vzero,
+                                                (vector unsigned char)vsrc2uc);
+        vsrc3ssH = (vector signed short)vec_mergeh((vector unsigned char)vzero,
+                                                (vector unsigned char)vsrc3uc);
+
+        psum = vec_mladd(vA, vsrc0ssH, vec_splat_s16(0));
+        psum = vec_mladd(vB, vsrc1ssH, psum);
+        psum = vec_mladd(vC, vsrc2ssH, psum);
+        psum = vec_mladd(vD, vsrc3ssH, psum);
+        psum = vec_add(v28ss, psum);
+        psum = vec_sra(psum, v6us);
+
+        vdst = vec_ld(0, dst);
+        ppsum = (vector unsigned char)vec_packsu(psum, psum);
+        fsum = vec_perm(vdst, ppsum, fperm);
+
+        vec_st(fsum, 0, dst);
+
+        vsrc0ssH = vsrc2ssH;
+        vsrc1ssH = vsrc3ssH;
+
+        dst += stride;
+        src += stride;
+      }
+    } else {
+        vector unsigned char vsrcDuc;
+      for (i = 0 ; i &lt; h ; i++) {
+        vsrcCuc = vec_ld(stride + 0, src);
+        vsrcDuc = vec_ld(stride + 16, src);
+
+        vsrc2uc = vec_perm(vsrcCuc, vsrcDuc, vsrcperm0);
+        if (reallyBadAlign)
+          vsrc3uc = vsrcDuc;
+        else
+          vsrc3uc = vec_perm(vsrcCuc, vsrcDuc, vsrcperm1);
+
+        vsrc2ssH = (vector signed short)vec_mergeh((vector unsigned char)vzero,
+                                                (vector unsigned char)vsrc2uc);
+        vsrc3ssH = (vector signed short)vec_mergeh((vector unsigned char)vzero,
+                                                (vector unsigned char)vsrc3uc);
+
+        psum = vec_mladd(vA, vsrc0ssH, vec_splat_s16(0));
+        psum = vec_mladd(vB, vsrc1ssH, psum);
+        psum = vec_mladd(vC, vsrc2ssH, psum);
+        psum = vec_mladd(vD, vsrc3ssH, psum);
+        psum = vec_add(v28ss, psum);
+        psum = vec_sr(psum, v6us);
+
+        vdst = vec_ld(0, dst);
+        ppsum = (vector unsigned char)vec_pack(psum, psum);
+        fsum = vec_perm(vdst, ppsum, fperm);
+
+        vec_st(fsum, 0, dst);
+
+        vsrc0ssH = vsrc2ssH;
+        vsrc1ssH = vsrc3ssH;
+
+        dst += stride;
+        src += stride;
+      }
+    }
+}
+
+static inline void put_pixels16_l2_altivec( uint8_t * dst, const uint8_t * src1,
+                                    const uint8_t * src2, int dst_stride,
+                                    int src_stride1, int h)
+{
+    int i;
+    vector unsigned char a, b, d, tmp1, tmp2, mask, mask_, edges, align;
+
+    mask_ = vec_lvsl(0, src2);
+
+    for (i = 0; i &lt; h; i++) {
+
+        tmp1 = vec_ld(i * src_stride1, src1);
+        mask = vec_lvsl(i * src_stride1, src1);
+        tmp2 = vec_ld(i * src_stride1 + 15, src1);
+
+        a = vec_perm(tmp1, tmp2, mask);
+
+        tmp1 = vec_ld(i * 16, src2);
+        tmp2 = vec_ld(i * 16 + 15, src2);
+
+        b = vec_perm(tmp1, tmp2, mask_);
+
+        tmp1 = vec_ld(0, dst);
+        mask = vec_lvsl(0, dst);
+        tmp2 = vec_ld(15, dst);
+
+        d = vec_avg(a, b);
+
+        edges = vec_perm(tmp2, tmp1, mask);
+
+        align = vec_lvsr(0, dst);
+
+        tmp2 = vec_perm(d, edges, align);
+        tmp1 = vec_perm(edges, d, align);
+
+        vec_st(tmp2, 15, dst);
+        vec_st(tmp1, 0 , dst);
+
+        dst += dst_stride;
+    }
+}
+
+static inline void avg_pixels16_l2_altivec( uint8_t * dst, const uint8_t * src1,
+                                    const uint8_t * src2, int dst_stride,
+                                    int src_stride1, int h)
+{
+    int i;
+    vector unsigned char a, b, d, tmp1, tmp2, mask, mask_, edges, align;
+
+    mask_ = vec_lvsl(0, src2);
+
+    for (i = 0; i &lt; h; i++) {
+
+        tmp1 = vec_ld(i * src_stride1, src1);
+        mask = vec_lvsl(i * src_stride1, src1);
+        tmp2 = vec_ld(i * src_stride1 + 15, src1);
+
+        a = vec_perm(tmp1, tmp2, mask);
+
+        tmp1 = vec_ld(i * 16, src2);
+        tmp2 = vec_ld(i * 16 + 15, src2);
+
+        b = vec_perm(tmp1, tmp2, mask_);
+
+        tmp1 = vec_ld(0, dst);
+        mask = vec_lvsl(0, dst);
+        tmp2 = vec_ld(15, dst);
+
+        d = vec_avg(vec_perm(tmp1, tmp2, mask), vec_avg(a, b));
+
+        edges = vec_perm(tmp2, tmp1, mask);
+
+        align = vec_lvsr(0, dst);
+
+        tmp2 = vec_perm(d, edges, align);
+        tmp1 = vec_perm(edges, d, align);
+
+        vec_st(tmp2, 15, dst);
+        vec_st(tmp1, 0 , dst);
+
+        dst += dst_stride;
+    }
+}
+
+/* Implemented but could be faster
+#define put_pixels16_l2_altivec(d,s1,s2,ds,s1s,h) put_pixels16_l2(d,s1,s2,ds,s1s,16,h)
+#define avg_pixels16_l2_altivec(d,s1,s2,ds,s1s,h) avg_pixels16_l2(d,s1,s2,ds,s1s,16,h)
+ */
+
+  H264_MC(put_, 16, altivec)
+  H264_MC(avg_, 16, altivec)
+
+
+/****************************************************************************
+ * IDCT transform:
+ ****************************************************************************/
+
+#define VEC_1D_DCT(vb0,vb1,vb2,vb3,va0,va1,va2,va3)              \
+   /* 1st stage */                                               \
+   vz0 = vec_add(vb0,vb2);       /* temp[0] = Y[0] + Y[2] */     \
+   vz1 = vec_sub(vb0,vb2);       /* temp[1] = Y[0] - Y[2] */     \
+   vz2 = vec_sra(vb1,vec_splat_u16(1));                          \
+   vz2 = vec_sub(vz2,vb3);       /* temp[2] = Y[1].1/2 - Y[3] */ \
+   vz3 = vec_sra(vb3,vec_splat_u16(1));                          \
+   vz3 = vec_add(vb1,vz3);       /* temp[3] = Y[1] + Y[3].1/2 */ \
+   /* 2nd stage: output */                                       \
+   va0 = vec_add(vz0,vz3);       /* x[0] = temp[0] + temp[3] */  \
+   va1 = vec_add(vz1,vz2);       /* x[1] = temp[1] + temp[2] */  \
+   va2 = vec_sub(vz1,vz2);       /* x[2] = temp[1] - temp[2] */  \
+   va3 = vec_sub(vz0,vz3)        /* x[3] = temp[0] - temp[3] */
+
+#define VEC_TRANSPOSE_4(a0,a1,a2,a3,b0,b1,b2,b3) \
+    b0 = vec_mergeh( a0, a0 ); \
+    b1 = vec_mergeh( a1, a0 ); \
+    b2 = vec_mergeh( a2, a0 ); \
+    b3 = vec_mergeh( a3, a0 ); \
+    a0 = vec_mergeh( b0, b2 ); \
+    a1 = vec_mergel( b0, b2 ); \
+    a2 = vec_mergeh( b1, b3 ); \
+    a3 = vec_mergel( b1, b3 ); \
+    b0 = vec_mergeh( a0, a2 ); \
+    b1 = vec_mergel( a0, a2 ); \
+    b2 = vec_mergeh( a1, a3 ); \
+    b3 = vec_mergel( a1, a3 )
+
+#define VEC_LOAD_U8_ADD_S16_STORE_U8(va)                      \
+    vdst_orig = vec_ld(0, dst);                               \
+    vdst = vec_perm(vdst_orig, zero_u8v, vdst_mask);          \
+    vdst_ss = (vec_s16_t) vec_mergeh(zero_u8v, vdst);         \
+    va = vec_add(va, vdst_ss);                                \
+    va_u8 = vec_packsu(va, zero_s16v);                        \
+    va_u32 = vec_splat((vec_u32_t)va_u8, 0);                  \
+    vec_ste(va_u32, element, (uint32_t*)dst);
+
+static void ff_h264_idct_add_altivec(uint8_t *dst, DCTELEM *block, int stride)
+{
+    vec_s16_t va0, va1, va2, va3;
+    vec_s16_t vz0, vz1, vz2, vz3;
+    vec_s16_t vtmp0, vtmp1, vtmp2, vtmp3;
+    vec_u8_t va_u8;
+    vec_u32_t va_u32;
+    vec_s16_t vdst_ss;
+    const vec_u16_t v6us = vec_splat_u16(6);
+    vec_u8_t vdst, vdst_orig;
+    vec_u8_t vdst_mask = vec_lvsl(0, dst);
+    int element = ((unsigned long)dst &amp; 0xf) &gt;&gt; 2;
+    LOAD_ZERO;
+
+    block[0] += 32;  /* add 32 as a DC-level for rounding */
+
+    vtmp0 = vec_ld(0,block);
+    vtmp1 = vec_sld(vtmp0, vtmp0, 8);
+    vtmp2 = vec_ld(16,block);
+    vtmp3 = vec_sld(vtmp2, vtmp2, 8);
+
+    VEC_1D_DCT(vtmp0,vtmp1,vtmp2,vtmp3,va0,va1,va2,va3);
+    VEC_TRANSPOSE_4(va0,va1,va2,va3,vtmp0,vtmp1,vtmp2,vtmp3);
+    VEC_1D_DCT(vtmp0,vtmp1,vtmp2,vtmp3,va0,va1,va2,va3);
+
+    va0 = vec_sra(va0,v6us);
+    va1 = vec_sra(va1,v6us);
+    va2 = vec_sra(va2,v6us);
+    va3 = vec_sra(va3,v6us);
+
+    VEC_LOAD_U8_ADD_S16_STORE_U8(va0);
+    dst += stride;
+    VEC_LOAD_U8_ADD_S16_STORE_U8(va1);
+    dst += stride;
+    VEC_LOAD_U8_ADD_S16_STORE_U8(va2);
+    dst += stride;
+    VEC_LOAD_U8_ADD_S16_STORE_U8(va3);
+}
+
+#define IDCT8_1D_ALTIVEC(s0, s1, s2, s3, s4, s5, s6, s7,  d0, d1, d2, d3, d4, d5, d6, d7) {\
+    /*        a0  = SRC(0) + SRC(4); */ \
+    vec_s16_t a0v = vec_add(s0, s4);    \
+    /*        a2  = SRC(0) - SRC(4); */ \
+    vec_s16_t a2v = vec_sub(s0, s4);    \
+    /*        a4  =           (SRC(2)&gt;&gt;1) - SRC(6); */ \
+    vec_s16_t a4v = vec_sub(vec_sra(s2, onev), s6);    \
+    /*        a6  =           (SRC(6)&gt;&gt;1) + SRC(2); */ \
+    vec_s16_t a6v = vec_add(vec_sra(s6, onev), s2);    \
+    /*        b0  =         a0 + a6; */ \
+    vec_s16_t b0v = vec_add(a0v, a6v);  \
+    /*        b2  =         a2 + a4; */ \
+    vec_s16_t b2v = vec_add(a2v, a4v);  \
+    /*        b4  =         a2 - a4; */ \
+    vec_s16_t b4v = vec_sub(a2v, a4v);  \
+    /*        b6  =         a0 - a6; */ \
+    vec_s16_t b6v = vec_sub(a0v, a6v);  \
+    /* a1 =  SRC(5) - SRC(3) - SRC(7) - (SRC(7)&gt;&gt;1); */ \
+    /*        a1 =             (SRC(5)-SRC(3)) -  (SRC(7)  +  (SRC(7)&gt;&gt;1)); */ \
+    vec_s16_t a1v = vec_sub( vec_sub(s5, s3), vec_add(s7, vec_sra(s7, onev)) ); \
+    /* a3 =  SRC(7) + SRC(1) - SRC(3) - (SRC(3)&gt;&gt;1); */ \
+    /*        a3 =             (SRC(7)+SRC(1)) -  (SRC(3)  +  (SRC(3)&gt;&gt;1)); */ \
+    vec_s16_t a3v = vec_sub( vec_add(s7, s1), vec_add(s3, vec_sra(s3, onev)) );\
+    /* a5 =  SRC(7) - SRC(1) + SRC(5) + (SRC(5)&gt;&gt;1); */ \
+    /*        a5 =             (SRC(7)-SRC(1)) +   SRC(5) +   (SRC(5)&gt;&gt;1); */ \
+    vec_s16_t a5v = vec_add( vec_sub(s7, s1), vec_add(s5, vec_sra(s5, onev)) );\
+    /*        a7 =                SRC(5)+SRC(3) +  SRC(1) +   (SRC(1)&gt;&gt;1); */ \
+    vec_s16_t a7v = vec_add( vec_add(s5, s3), vec_add(s1, vec_sra(s1, onev)) );\
+    /*        b1 =                  (a7&gt;&gt;2)  +  a1; */ \
+    vec_s16_t b1v = vec_add( vec_sra(a7v, twov), a1v); \
+    /*        b3 =          a3 +        (a5&gt;&gt;2); */ \
+    vec_s16_t b3v = vec_add(a3v, vec_sra(a5v, twov)); \
+    /*        b5 =                  (a3&gt;&gt;2)  -   a5; */ \
+    vec_s16_t b5v = vec_sub( vec_sra(a3v, twov), a5v); \
+    /*        b7 =           a7 -        (a1&gt;&gt;2); */ \
+    vec_s16_t b7v = vec_sub( a7v, vec_sra(a1v, twov)); \
+    /* DST(0,    b0 + b7); */ \
+    d0 = vec_add(b0v, b7v); \
+    /* DST(1,    b2 + b5); */ \
+    d1 = vec_add(b2v, b5v); \
+    /* DST(2,    b4 + b3); */ \
+    d2 = vec_add(b4v, b3v); \
+    /* DST(3,    b6 + b1); */ \
+    d3 = vec_add(b6v, b1v); \
+    /* DST(4,    b6 - b1); */ \
+    d4 = vec_sub(b6v, b1v); \
+    /* DST(5,    b4 - b3); */ \
+    d5 = vec_sub(b4v, b3v); \
+    /* DST(6,    b2 - b5); */ \
+    d6 = vec_sub(b2v, b5v); \
+    /* DST(7,    b0 - b7); */ \
+    d7 = vec_sub(b0v, b7v); \
+}
+
+#define ALTIVEC_STORE_SUM_CLIP(dest, idctv, perm_ldv, perm_stv, sel) { \
+    /* unaligned load */                                       \
+    vec_u8_t hv = vec_ld( 0, dest );                           \
+    vec_u8_t lv = vec_ld( 7, dest );                           \
+    vec_u8_t dstv   = vec_perm( hv, lv, (vec_u8_t)perm_ldv );  \
+    vec_s16_t idct_sh6 = vec_sra(idctv, sixv);                 \
+    vec_u16_t dst16 = (vec_u16_t)vec_mergeh(zero_u8v, dstv);   \
+    vec_s16_t idstsum = vec_adds(idct_sh6, (vec_s16_t)dst16);  \
+    vec_u8_t idstsum8 = vec_packsu(zero_s16v, idstsum);        \
+    vec_u8_t edgehv;                                           \
+    /* unaligned store */                                      \
+    vec_u8_t bodyv  = vec_perm( idstsum8, idstsum8, perm_stv );\
+    vec_u8_t edgelv = vec_perm( sel, zero_u8v, perm_stv );     \
+    lv    = vec_sel( lv, bodyv, edgelv );                      \
+    vec_st( lv, 7, dest );                                     \
+    hv    = vec_ld( 0, dest );                                 \
+    edgehv = vec_perm( zero_u8v, sel, perm_stv );              \
+    hv    = vec_sel( hv, bodyv, edgehv );                      \
+    vec_st( hv, 0, dest );                                     \
+ }
+
+void ff_h264_idct8_add_altivec( uint8_t *dst, DCTELEM *dct, int stride ) {
+    vec_s16_t s0, s1, s2, s3, s4, s5, s6, s7;
+    vec_s16_t d0, d1, d2, d3, d4, d5, d6, d7;
+    vec_s16_t idct0, idct1, idct2, idct3, idct4, idct5, idct6, idct7;
+
+    vec_u8_t perm_ldv = vec_lvsl(0, dst);
+    vec_u8_t perm_stv = vec_lvsr(8, dst);
+
+    const vec_u16_t onev = vec_splat_u16(1);
+    const vec_u16_t twov = vec_splat_u16(2);
+    const vec_u16_t sixv = vec_splat_u16(6);
+
+    const vec_u8_t sel = (vec_u8_t) AVV(0,0,0,0,0,0,0,0,
+                                        -1,-1,-1,-1,-1,-1,-1,-1);
+    LOAD_ZERO;
+
+    dct[0] += 32; // rounding for the &gt;&gt;6 at the end
+
+    s0 = vec_ld(0x00, (int16_t*)dct);
+    s1 = vec_ld(0x10, (int16_t*)dct);
+    s2 = vec_ld(0x20, (int16_t*)dct);
+    s3 = vec_ld(0x30, (int16_t*)dct);
+    s4 = vec_ld(0x40, (int16_t*)dct);
+    s5 = vec_ld(0x50, (int16_t*)dct);
+    s6 = vec_ld(0x60, (int16_t*)dct);
+    s7 = vec_ld(0x70, (int16_t*)dct);
+
+    IDCT8_1D_ALTIVEC(s0, s1, s2, s3, s4, s5, s6, s7,
+                     d0, d1, d2, d3, d4, d5, d6, d7);
+
+    TRANSPOSE8( d0,  d1,  d2,  d3,  d4,  d5,  d6, d7 );
+
+    IDCT8_1D_ALTIVEC(d0,  d1,  d2,  d3,  d4,  d5,  d6, d7,
+                     idct0, idct1, idct2, idct3, idct4, idct5, idct6, idct7);
+
+    ALTIVEC_STORE_SUM_CLIP(&amp;dst[0*stride], idct0, perm_ldv, perm_stv, sel);
+    ALTIVEC_STORE_SUM_CLIP(&amp;dst[1*stride], idct1, perm_ldv, perm_stv, sel);
+    ALTIVEC_STORE_SUM_CLIP(&amp;dst[2*stride], idct2, perm_ldv, perm_stv, sel);
+    ALTIVEC_STORE_SUM_CLIP(&amp;dst[3*stride], idct3, perm_ldv, perm_stv, sel);
+    ALTIVEC_STORE_SUM_CLIP(&amp;dst[4*stride], idct4, perm_ldv, perm_stv, sel);
+    ALTIVEC_STORE_SUM_CLIP(&amp;dst[5*stride], idct5, perm_ldv, perm_stv, sel);
+    ALTIVEC_STORE_SUM_CLIP(&amp;dst[6*stride], idct6, perm_ldv, perm_stv, sel);
+    ALTIVEC_STORE_SUM_CLIP(&amp;dst[7*stride], idct7, perm_ldv, perm_stv, sel);
+}
+
+#define transpose4x16(r0, r1, r2, r3) {      \
+    register vector unsigned char r4;        \
+    register vector unsigned char r5;        \
+    register vector unsigned char r6;        \
+    register vector unsigned char r7;        \
+                                             \
+    r4 = vec_mergeh(r0, r2);  /*0, 2 set 0*/ \
+    r5 = vec_mergel(r0, r2);  /*0, 2 set 1*/ \
+    r6 = vec_mergeh(r1, r3);  /*1, 3 set 0*/ \
+    r7 = vec_mergel(r1, r3);  /*1, 3 set 1*/ \
+                                             \
+    r0 = vec_mergeh(r4, r6);  /*all set 0*/  \
+    r1 = vec_mergel(r4, r6);  /*all set 1*/  \
+    r2 = vec_mergeh(r5, r7);  /*all set 2*/  \
+    r3 = vec_mergel(r5, r7);  /*all set 3*/  \
+}
+
+static inline void write16x4(uint8_t *dst, int dst_stride,
+                             register vector unsigned char r0, register vector unsigned char r1,
+                             register vector unsigned char r2, register vector unsigned char r3) {
+    DECLARE_ALIGNED_16(unsigned char, result[64]);
+    uint32_t *src_int = (uint32_t *)result, *dst_int = (uint32_t *)dst;
+    int int_dst_stride = dst_stride/4;
+
+    vec_st(r0, 0, result);
+    vec_st(r1, 16, result);
+    vec_st(r2, 32, result);
+    vec_st(r3, 48, result);
+    /* FIXME: there has to be a better way!!!! */
+    *dst_int = *src_int;
+    *(dst_int+   int_dst_stride) = *(src_int + 1);
+    *(dst_int+ 2*int_dst_stride) = *(src_int + 2);
+    *(dst_int+ 3*int_dst_stride) = *(src_int + 3);
+    *(dst_int+ 4*int_dst_stride) = *(src_int + 4);
+    *(dst_int+ 5*int_dst_stride) = *(src_int + 5);
+    *(dst_int+ 6*int_dst_stride) = *(src_int + 6);
+    *(dst_int+ 7*int_dst_stride) = *(src_int + 7);
+    *(dst_int+ 8*int_dst_stride) = *(src_int + 8);
+    *(dst_int+ 9*int_dst_stride) = *(src_int + 9);
+    *(dst_int+10*int_dst_stride) = *(src_int + 10);
+    *(dst_int+11*int_dst_stride) = *(src_int + 11);
+    *(dst_int+12*int_dst_stride) = *(src_int + 12);
+    *(dst_int+13*int_dst_stride) = *(src_int + 13);
+    *(dst_int+14*int_dst_stride) = *(src_int + 14);
+    *(dst_int+15*int_dst_stride) = *(src_int + 15);
+}
+
+/** \brief performs a 6x16 transpose of data in src, and stores it to dst
+    \todo FIXME: see if we can't spare some vec_lvsl() by them factorizing
+    out of unaligned_load() */
+#define readAndTranspose16x6(src, src_stride, r8, r9, r10, r11, r12, r13) {\
+    register vector unsigned char r0  = unaligned_load(0,             src);\
+    register vector unsigned char r1  = unaligned_load(   src_stride, src);\
+    register vector unsigned char r2  = unaligned_load(2* src_stride, src);\
+    register vector unsigned char r3  = unaligned_load(3* src_stride, src);\
+    register vector unsigned char r4  = unaligned_load(4* src_stride, src);\
+    register vector unsigned char r5  = unaligned_load(5* src_stride, src);\
+    register vector unsigned char r6  = unaligned_load(6* src_stride, src);\
+    register vector unsigned char r7  = unaligned_load(7* src_stride, src);\
+    register vector unsigned char r14 = unaligned_load(14*src_stride, src);\
+    register vector unsigned char r15 = unaligned_load(15*src_stride, src);\
+                                                                           \
+    r8  = unaligned_load( 8*src_stride, src);                              \
+    r9  = unaligned_load( 9*src_stride, src);                              \
+    r10 = unaligned_load(10*src_stride, src);                              \
+    r11 = unaligned_load(11*src_stride, src);                              \
+    r12 = unaligned_load(12*src_stride, src);                              \
+    r13 = unaligned_load(13*src_stride, src);                              \
+                                                                           \
+    /*Merge first pairs*/                                                  \
+    r0 = vec_mergeh(r0, r8);    /*0, 8*/                                   \
+    r1 = vec_mergeh(r1, r9);    /*1, 9*/                                   \
+    r2 = vec_mergeh(r2, r10);   /*2,10*/                                   \
+    r3 = vec_mergeh(r3, r11);   /*3,11*/                                   \
+    r4 = vec_mergeh(r4, r12);   /*4,12*/                                   \
+    r5 = vec_mergeh(r5, r13);   /*5,13*/                                   \
+    r6 = vec_mergeh(r6, r14);   /*6,14*/                                   \
+    r7 = vec_mergeh(r7, r15);   /*7,15*/                                   \
+                                                                           \
+    /*Merge second pairs*/                                                 \
+    r8  = vec_mergeh(r0, r4);   /*0,4, 8,12 set 0*/                        \
+    r9  = vec_mergel(r0, r4);   /*0,4, 8,12 set 1*/                        \
+    r10 = vec_mergeh(r1, r5);   /*1,5, 9,13 set 0*/                        \
+    r11 = vec_mergel(r1, r5);   /*1,5, 9,13 set 1*/                        \
+    r12 = vec_mergeh(r2, r6);   /*2,6,10,14 set 0*/                        \
+    r13 = vec_mergel(r2, r6);   /*2,6,10,14 set 1*/                        \
+    r14 = vec_mergeh(r3, r7);   /*3,7,11,15 set 0*/                        \
+    r15 = vec_mergel(r3, r7);   /*3,7,11,15 set 1*/                        \
+                                                                           \
+    /*Third merge*/                                                        \
+    r0 = vec_mergeh(r8, r12);   /*0,2,4,6,8,10,12,14 set 0*/               \
+    r1 = vec_mergel(r8, r12);   /*0,2,4,6,8,10,12,14 set 1*/               \
+    r2 = vec_mergeh(r9, r13);   /*0,2,4,6,8,10,12,14 set 2*/               \
+    r4 = vec_mergeh(r10, r14);  /*1,3,5,7,9,11,13,15 set 0*/               \
+    r5 = vec_mergel(r10, r14);  /*1,3,5,7,9,11,13,15 set 1*/               \
+    r6 = vec_mergeh(r11, r15);  /*1,3,5,7,9,11,13,15 set 2*/               \
+    /* Don't need to compute 3 and 7*/                                     \
+                                                                           \
+    /*Final merge*/                                                        \
+    r8  = vec_mergeh(r0, r4);   /*all set 0*/                              \
+    r9  = vec_mergel(r0, r4);   /*all set 1*/                              \
+    r10 = vec_mergeh(r1, r5);   /*all set 2*/                              \
+    r11 = vec_mergel(r1, r5);   /*all set 3*/                              \
+    r12 = vec_mergeh(r2, r6);   /*all set 4*/                              \
+    r13 = vec_mergel(r2, r6);   /*all set 5*/                              \
+    /* Don't need to compute 14 and 15*/                                   \
+                                                                           \
+}
+
+// out: o = |x-y| &lt; a
+static inline vector unsigned char diff_lt_altivec ( register vector unsigned char x,
+                                                     register vector unsigned char y,
+                                                     register vector unsigned char a) {
+
+    register vector unsigned char diff = vec_subs(x, y);
+    register vector unsigned char diffneg = vec_subs(y, x);
+    register vector unsigned char o = vec_or(diff, diffneg); /* |x-y| */
+    o = (vector unsigned char)vec_cmplt(o, a);
+    return o;
+}
+
+static inline vector unsigned char h264_deblock_mask ( register vector unsigned char p0,
+                                                       register vector unsigned char p1,
+                                                       register vector unsigned char q0,
+                                                       register vector unsigned char q1,
+                                                       register vector unsigned char alpha,
+                                                       register vector unsigned char beta) {
+
+    register vector unsigned char mask;
+    register vector unsigned char tempmask;
+
+    mask = diff_lt_altivec(p0, q0, alpha);
+    tempmask = diff_lt_altivec(p1, p0, beta);
+    mask = vec_and(mask, tempmask);
+    tempmask = diff_lt_altivec(q1, q0, beta);
+    mask = vec_and(mask, tempmask);
+
+    return mask;
+}
+
+// out: newp1 = clip((p2 + ((p0 + q0 + 1) &gt;&gt; 1)) &gt;&gt; 1, p1-tc0, p1+tc0)
+static inline vector unsigned char h264_deblock_q1(register vector unsigned char p0,
+                                                   register vector unsigned char p1,
+                                                   register vector unsigned char p2,
+                                                   register vector unsigned char q0,
+                                                   register vector unsigned char tc0) {
+
+    register vector unsigned char average = vec_avg(p0, q0);
+    register vector unsigned char temp;
+    register vector unsigned char uncliped;
+    register vector unsigned char ones;
+    register vector unsigned char max;
+    register vector unsigned char min;
+    register vector unsigned char newp1;
+
+    temp = vec_xor(average, p2);
+    average = vec_avg(average, p2);     /*avg(p2, avg(p0, q0)) */
+    ones = vec_splat_u8(1);
+    temp = vec_and(temp, ones);         /*(p2^avg(p0, q0)) &amp; 1 */
+    uncliped = vec_subs(average, temp); /*(p2+((p0+q0+1)&gt;&gt;1))&gt;&gt;1 */
+    max = vec_adds(p1, tc0);
+    min = vec_subs(p1, tc0);
+    newp1 = vec_max(min, uncliped);
+    newp1 = vec_min(max, newp1);
+    return newp1;
+}
+
+#define h264_deblock_p0_q0(p0, p1, q0, q1, tc0masked) {                                           \
+                                                                                                  \
+    const vector unsigned char A0v = vec_sl(vec_splat_u8(10), vec_splat_u8(4));                   \
+                                                                                                  \
+    register vector unsigned char pq0bit = vec_xor(p0,q0);                                        \
+    register vector unsigned char q1minus;                                                        \
+    register vector unsigned char p0minus;                                                        \
+    register vector unsigned char stage1;                                                         \
+    register vector unsigned char stage2;                                                         \
+    register vector unsigned char vec160;                                                         \
+    register vector unsigned char delta;                                                          \
+    register vector unsigned char deltaneg;                                                       \
+                                                                                                  \
+    q1minus = vec_nor(q1, q1);                 /* 255 - q1 */                                     \
+    stage1 = vec_avg(p1, q1minus);             /* (p1 - q1 + 256)&gt;&gt;1 */                           \
+    stage2 = vec_sr(stage1, vec_splat_u8(1));  /* (p1 - q1 + 256)&gt;&gt;2 = 64 + (p1 - q1) &gt;&gt; 2 */     \
+    p0minus = vec_nor(p0, p0);                 /* 255 - p0 */                                     \
+    stage1 = vec_avg(q0, p0minus);             /* (q0 - p0 + 256)&gt;&gt;1 */                           \
+    pq0bit = vec_and(pq0bit, vec_splat_u8(1));                                                    \
+    stage2 = vec_avg(stage2, pq0bit);          /* 32 + ((q0 - p0)&amp;1 + (p1 - q1) &gt;&gt; 2 + 1) &gt;&gt; 1 */ \
+    stage2 = vec_adds(stage2, stage1);         /* 160 + ((p0 - q0) + (p1 - q1) &gt;&gt; 2 + 1) &gt;&gt; 1 */  \
+    vec160 = vec_ld(0, &amp;A0v);                                                                     \
+    deltaneg = vec_subs(vec160, stage2);       /* -d */                                           \
+    delta = vec_subs(stage2, vec160);          /* d */                                            \
+    deltaneg = vec_min(tc0masked, deltaneg);                                                      \
+    delta = vec_min(tc0masked, delta);                                                            \
+    p0 = vec_subs(p0, deltaneg);                                                                  \
+    q0 = vec_subs(q0, delta);                                                                     \
+    p0 = vec_adds(p0, delta);                                                                     \
+    q0 = vec_adds(q0, deltaneg);                                                                  \
+}
+
+#define h264_loop_filter_luma_altivec(p2, p1, p0, q0, q1, q2, alpha, beta, tc0) {            \
+    DECLARE_ALIGNED_16(unsigned char, temp[16]);                                             \
+    register vector unsigned char alphavec;                                                  \
+    register vector unsigned char betavec;                                                   \
+    register vector unsigned char mask;                                                      \
+    register vector unsigned char p1mask;                                                    \
+    register vector unsigned char q1mask;                                                    \
+    register vector signed   char tc0vec;                                                    \
+    register vector unsigned char finaltc0;                                                  \
+    register vector unsigned char tc0masked;                                                 \
+    register vector unsigned char newp1;                                                     \
+    register vector unsigned char newq1;                                                     \
+                                                                                             \
+    temp[0] = alpha;                                                                         \
+    temp[1] = beta;                                                                          \
+    alphavec = vec_ld(0, temp);                                                              \
+    betavec = vec_splat(alphavec, 0x1);                                                      \
+    alphavec = vec_splat(alphavec, 0x0);                                                     \
+    mask = h264_deblock_mask(p0, p1, q0, q1, alphavec, betavec); /*if in block */            \
+                                                                                             \
+    *((int *)temp) = *((int *)tc0);                                                          \
+    tc0vec = vec_ld(0, (signed char*)temp);                                                  \
+    tc0vec = vec_mergeh(tc0vec, tc0vec);                                                     \
+    tc0vec = vec_mergeh(tc0vec, tc0vec);                                                     \
+    mask = vec_and(mask, vec_cmpgt(tc0vec, vec_splat_s8(-1)));  /* if tc0[i] &gt;= 0 */         \
+    finaltc0 = vec_and((vector unsigned char)tc0vec, mask);     /* tc = tc0 */               \
+                                                                                             \
+    p1mask = diff_lt_altivec(p2, p0, betavec);                                               \
+    p1mask = vec_and(p1mask, mask);                             /* if( |p2 - p0| &lt; beta) */  \
+    tc0masked = vec_and(p1mask, (vector unsigned char)tc0vec);                               \
+    finaltc0 = vec_sub(finaltc0, p1mask);                       /* tc++ */                   \
+    newp1 = h264_deblock_q1(p0, p1, p2, q0, tc0masked);                                      \
+    /*end if*/                                                                               \
+                                                                                             \
+    q1mask = diff_lt_altivec(q2, q0, betavec);                                               \
+    q1mask = vec_and(q1mask, mask);                             /* if ( |q2 - q0| &lt; beta ) */\
+    tc0masked = vec_and(q1mask, (vector unsigned char)tc0vec);                               \
+    finaltc0 = vec_sub(finaltc0, q1mask);                       /* tc++ */                   \
+    newq1 = h264_deblock_q1(p0, q1, q2, q0, tc0masked);                                      \
+    /*end if*/                                                                               \
+                                                                                             \
+    h264_deblock_p0_q0(p0, p1, q0, q1, finaltc0);                                            \
+    p1 = newp1;                                                                              \
+    q1 = newq1;                                                                              \
+}
+
+static void h264_v_loop_filter_luma_altivec(uint8_t *pix, int stride, int alpha, int beta, int8_t *tc0) {
+
+    if((tc0[0] &amp; tc0[1] &amp; tc0[2] &amp; tc0[3]) &gt;= 0) {
+        register vector unsigned char p2 = vec_ld(-3*stride, pix);
+        register vector unsigned char p1 = vec_ld(-2*stride, pix);
+        register vector unsigned char p0 = vec_ld(-1*stride, pix);
+        register vector unsigned char q0 = vec_ld(0, pix);
+        register vector unsigned char q1 = vec_ld(stride, pix);
+        register vector unsigned char q2 = vec_ld(2*stride, pix);
+        h264_loop_filter_luma_altivec(p2, p1, p0, q0, q1, q2, alpha, beta, tc0);
+        vec_st(p1, -2*stride, pix);
+        vec_st(p0, -1*stride, pix);
+        vec_st(q0, 0, pix);
+        vec_st(q1, stride, pix);
+    }
+}
+
+static void h264_h_loop_filter_luma_altivec(uint8_t *pix, int stride, int alpha, int beta, int8_t *tc0) {
+
+    register vector unsigned char line0, line1, line2, line3, line4, line5;
+    if((tc0[0] &amp; tc0[1] &amp; tc0[2] &amp; tc0[3]) &lt; 0)
+        return;
+    readAndTranspose16x6(pix-3, stride, line0, line1, line2, line3, line4, line5);
+    h264_loop_filter_luma_altivec(line0, line1, line2, line3, line4, line5, alpha, beta, tc0);
+    transpose4x16(line1, line2, line3, line4);
+    write16x4(pix-2, stride, line1, line2, line3, line4);
+}
+
+void dsputil_h264_init_ppc(DSPContext* c, AVCodecContext *avctx) {
+
+#ifdef HAVE_ALTIVEC
+  if (has_altivec()) {
+    c-&gt;put_h264_chroma_pixels_tab[0] = put_h264_chroma_mc8_altivec;
+    c-&gt;put_no_rnd_h264_chroma_pixels_tab[0] = put_no_rnd_h264_chroma_mc8_altivec;
+    c-&gt;avg_h264_chroma_pixels_tab[0] = avg_h264_chroma_mc8_altivec;
+    c-&gt;h264_idct_add = ff_h264_idct_add_altivec;
+    c-&gt;h264_idct8_add = ff_h264_idct8_add_altivec;
+    c-&gt;h264_v_loop_filter_luma= h264_v_loop_filter_luma_altivec;
+    c-&gt;h264_h_loop_filter_luma= h264_h_loop_filter_luma_altivec;
+
+#define dspfunc(PFX, IDX, NUM) \
+    c-&gt;PFX ## _pixels_tab[IDX][ 0] = PFX ## NUM ## _mc00_altivec; \
+    c-&gt;PFX ## _pixels_tab[IDX][ 1] = PFX ## NUM ## _mc10_altivec; \
+    c-&gt;PFX ## _pixels_tab[IDX][ 2] = PFX ## NUM ## _mc20_altivec; \
+    c-&gt;PFX ## _pixels_tab[IDX][ 3] = PFX ## NUM ## _mc30_altivec; \
+    c-&gt;PFX ## _pixels_tab[IDX][ 4] = PFX ## NUM ## _mc01_altivec; \
+    c-&gt;PFX ## _pixels_tab[IDX][ 5] = PFX ## NUM ## _mc11_altivec; \
+    c-&gt;PFX ## _pixels_tab[IDX][ 6] = PFX ## NUM ## _mc21_altivec; \
+    c-&gt;PFX ## _pixels_tab[IDX][ 7] = PFX ## NUM ## _mc31_altivec; \
+    c-&gt;PFX ## _pixels_tab[IDX][ 8] = PFX ## NUM ## _mc02_altivec; \
+    c-&gt;PFX ## _pixels_tab[IDX][ 9] = PFX ## NUM ## _mc12_altivec; \
+    c-&gt;PFX ## _pixels_tab[IDX][10] = PFX ## NUM ## _mc22_altivec; \
+    c-&gt;PFX ## _pixels_tab[IDX][11] = PFX ## NUM ## _mc32_altivec; \
+    c-&gt;PFX ## _pixels_tab[IDX][12] = PFX ## NUM ## _mc03_altivec; \
+    c-&gt;PFX ## _pixels_tab[IDX][13] = PFX ## NUM ## _mc13_altivec; \
+    c-&gt;PFX ## _pixels_tab[IDX][14] = PFX ## NUM ## _mc23_altivec; \
+    c-&gt;PFX ## _pixels_tab[IDX][15] = PFX ## NUM ## _mc33_altivec
+
+    dspfunc(put_h264_qpel, 0, 16);
+    dspfunc(avg_h264_qpel, 0, 16);
+#undef dspfunc
+
+  } else
+#endif /* HAVE_ALTIVEC */
+  {
+    // Non-AltiVec PPC optimisations
+
+    // ... pending ...
+  }
+}

Copied: branches/avidemux_2.4_branch/avidemux/ADM_libraries/ADM_lavcodec/ppc/h264_template_altivec.c (from rev 3719, branches/avidemux_2.4_branch/avidemux/ADM_libraries/ADM_lavcodec/ppc/dsputil_h264_template_altivec.c)
===================================================================
--- branches/avidemux_2.4_branch/avidemux/ADM_libraries/ADM_lavcodec/ppc/dsputil_h264_template_altivec.c	2007-12-03 12:38:06 UTC (rev 3719)
+++ branches/avidemux_2.4_branch/avidemux/ADM_libraries/ADM_lavcodec/ppc/h264_template_altivec.c	2007-12-04 00:19:48 UTC (rev 3720)
@@ -0,0 +1,719 @@
+/*
+ * Copyright (c) 2004 Romain Dolbeau &lt;<A HREF="https://lists.berlios.de/mailman/listinfo/avidemux-svn-commit">romain at dolbeau.org</A>&gt;
+ *
+ * This file is part of FFmpeg.
+ *
+ * FFmpeg is free software; you can redistribute it and/or
+ * modify it under the terms of the GNU Lesser General Public
+ * License as published by the Free Software Foundation; either
+ * version 2.1 of the License, or (at your option) any later version.
+ *
+ * FFmpeg is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+ * Lesser General Public License for more details.
+ *
+ * You should have received a copy of the GNU Lesser General Public
+ * License along with FFmpeg; if not, write to the Free Software
+ * Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA
+ */
+
+/* this code assume that stride % 16 == 0 */
+void PREFIX_h264_chroma_mc8_altivec(uint8_t * dst, uint8_t * src, int stride, int h, int x, int y) {
+  POWERPC_PERF_DECLARE(PREFIX_h264_chroma_mc8_num, 1);
+    DECLARE_ALIGNED_16(signed int, ABCD[4]) =
+                        {((8 - x) * (8 - y)),
+                          ((x) * (8 - y)),
+                          ((8 - x) * (y)),
+                          ((x) * (y))};
+    register int i;
+    vector unsigned char fperm;
+    const vector signed int vABCD = vec_ld(0, ABCD);
+    const vector signed short vA = vec_splat((vector signed short)vABCD, 1);
+    const vector signed short vB = vec_splat((vector signed short)vABCD, 3);
+    const vector signed short vC = vec_splat((vector signed short)vABCD, 5);
+    const vector signed short vD = vec_splat((vector signed short)vABCD, 7);
+    const vector signed int vzero = vec_splat_s32(0);
+    const vector signed short v32ss = vec_sl(vec_splat_s16(1),vec_splat_u16(5));
+    const vector unsigned short v6us = vec_splat_u16(6);
+    register int loadSecond = (((unsigned long)src) % 16) &lt;= 7 ? 0 : 1;
+    register int reallyBadAlign = (((unsigned long)src) % 16) == 15 ? 1 : 0;
+
+    vector unsigned char vsrcAuc, vsrcBuc, vsrcperm0, vsrcperm1;
+    vector unsigned char vsrc0uc, vsrc1uc;
+    vector signed short vsrc0ssH, vsrc1ssH;
+    vector unsigned char vsrcCuc, vsrc2uc, vsrc3uc;
+    vector signed short vsrc2ssH, vsrc3ssH, psum;
+    vector unsigned char vdst, ppsum, vfdst, fsum;
+
+  POWERPC_PERF_START_COUNT(PREFIX_h264_chroma_mc8_num, 1);
+
+    if (((unsigned long)dst) % 16 == 0) {
+      fperm = (vector unsigned char)AVV(0x10, 0x11, 0x12, 0x13,
+                                        0x14, 0x15, 0x16, 0x17,
+                                        0x08, 0x09, 0x0A, 0x0B,
+                                        0x0C, 0x0D, 0x0E, 0x0F);
+    } else {
+      fperm = (vector unsigned char)AVV(0x00, 0x01, 0x02, 0x03,
+                                        0x04, 0x05, 0x06, 0x07,
+                                        0x18, 0x19, 0x1A, 0x1B,
+                                        0x1C, 0x1D, 0x1E, 0x1F);
+    }
+
+    vsrcAuc = vec_ld(0, src);
+
+    if (loadSecond)
+      vsrcBuc = vec_ld(16, src);
+    vsrcperm0 = vec_lvsl(0, src);
+    vsrcperm1 = vec_lvsl(1, src);
+
+    vsrc0uc = vec_perm(vsrcAuc, vsrcBuc, vsrcperm0);
+    if (reallyBadAlign)
+      vsrc1uc = vsrcBuc;
+    else
+      vsrc1uc = vec_perm(vsrcAuc, vsrcBuc, vsrcperm1);
+
+    vsrc0ssH = (vector signed short)vec_mergeh((vector unsigned char)vzero,
+                                               (vector unsigned char)vsrc0uc);
+    vsrc1ssH = (vector signed short)vec_mergeh((vector unsigned char)vzero,
+                                               (vector unsigned char)vsrc1uc);
+
+    if (!loadSecond) {// -&gt; !reallyBadAlign
+      for (i = 0 ; i &lt; h ; i++) {
+
+
+        vsrcCuc = vec_ld(stride + 0, src);
+
+        vsrc2uc = vec_perm(vsrcCuc, vsrcCuc, vsrcperm0);
+        vsrc3uc = vec_perm(vsrcCuc, vsrcCuc, vsrcperm1);
+
+        vsrc2ssH = (vector signed short)vec_mergeh((vector unsigned char)vzero,
+                                                (vector unsigned char)vsrc2uc);
+        vsrc3ssH = (vector signed short)vec_mergeh((vector unsigned char)vzero,
+                                                (vector unsigned char)vsrc3uc);
+
+        psum = vec_mladd(vA, vsrc0ssH, vec_splat_s16(0));
+        psum = vec_mladd(vB, vsrc1ssH, psum);
+        psum = vec_mladd(vC, vsrc2ssH, psum);
+        psum = vec_mladd(vD, vsrc3ssH, psum);
+        psum = vec_add(v32ss, psum);
+        psum = vec_sra(psum, v6us);
+
+        vdst = vec_ld(0, dst);
+        ppsum = (vector unsigned char)vec_packsu(psum, psum);
+        vfdst = vec_perm(vdst, ppsum, fperm);
+
+        OP_U8_ALTIVEC(fsum, vfdst, vdst);
+
+        vec_st(fsum, 0, dst);
+
+        vsrc0ssH = vsrc2ssH;
+        vsrc1ssH = vsrc3ssH;
+
+        dst += stride;
+        src += stride;
+      }
+    } else {
+        vector unsigned char vsrcDuc;
+      for (i = 0 ; i &lt; h ; i++) {
+        vsrcCuc = vec_ld(stride + 0, src);
+        vsrcDuc = vec_ld(stride + 16, src);
+
+        vsrc2uc = vec_perm(vsrcCuc, vsrcDuc, vsrcperm0);
+        if (reallyBadAlign)
+          vsrc3uc = vsrcDuc;
+        else
+          vsrc3uc = vec_perm(vsrcCuc, vsrcDuc, vsrcperm1);
+
+        vsrc2ssH = (vector signed short)vec_mergeh((vector unsigned char)vzero,
+                                                (vector unsigned char)vsrc2uc);
+        vsrc3ssH = (vector signed short)vec_mergeh((vector unsigned char)vzero,
+                                                (vector unsigned char)vsrc3uc);
+
+        psum = vec_mladd(vA, vsrc0ssH, vec_splat_s16(0));
+        psum = vec_mladd(vB, vsrc1ssH, psum);
+        psum = vec_mladd(vC, vsrc2ssH, psum);
+        psum = vec_mladd(vD, vsrc3ssH, psum);
+        psum = vec_add(v32ss, psum);
+        psum = vec_sr(psum, v6us);
+
+        vdst = vec_ld(0, dst);
+        ppsum = (vector unsigned char)vec_pack(psum, psum);
+        vfdst = vec_perm(vdst, ppsum, fperm);
+
+        OP_U8_ALTIVEC(fsum, vfdst, vdst);
+
+        vec_st(fsum, 0, dst);
+
+        vsrc0ssH = vsrc2ssH;
+        vsrc1ssH = vsrc3ssH;
+
+        dst += stride;
+        src += stride;
+      }
+    }
+    POWERPC_PERF_STOP_COUNT(PREFIX_h264_chroma_mc8_num, 1);
+}
+
+/* this code assume stride % 16 == 0 */
+static void PREFIX_h264_qpel16_h_lowpass_altivec(uint8_t * dst, uint8_t * src, int dstStride, int srcStride) {
+  POWERPC_PERF_DECLARE(PREFIX_h264_qpel16_h_lowpass_num, 1);
+  register int i;
+
+  const vector signed int vzero = vec_splat_s32(0);
+  const vector unsigned char permM2 = vec_lvsl(-2, src);
+  const vector unsigned char permM1 = vec_lvsl(-1, src);
+  const vector unsigned char permP0 = vec_lvsl(+0, src);
+  const vector unsigned char permP1 = vec_lvsl(+1, src);
+  const vector unsigned char permP2 = vec_lvsl(+2, src);
+  const vector unsigned char permP3 = vec_lvsl(+3, src);
+  const vector signed short v5ss = vec_splat_s16(5);
+  const vector unsigned short v5us = vec_splat_u16(5);
+  const vector signed short v20ss = vec_sl(vec_splat_s16(5),vec_splat_u16(2));
+  const vector signed short v16ss = vec_sl(vec_splat_s16(1),vec_splat_u16(4));
+  const vector unsigned char dstperm = vec_lvsr(0, dst);
+  const vector unsigned char neg1 =
+                                (const vector unsigned char) vec_splat_s8(-1);
+
+  const vector unsigned char dstmask =
+                                vec_perm((const vector unsigned char)vzero,
+                                                               neg1, dstperm);
+
+  vector unsigned char srcM2, srcM1, srcP0, srcP1, srcP2, srcP3;
+
+  register int align = ((((unsigned long)src) - 2) % 16);
+
+  vector signed short srcP0A, srcP0B, srcP1A, srcP1B,
+                      srcP2A, srcP2B, srcP3A, srcP3B,
+                      srcM1A, srcM1B, srcM2A, srcM2B,
+                      sum1A, sum1B, sum2A, sum2B, sum3A, sum3B,
+                      pp1A, pp1B, pp2A, pp2B, pp3A, pp3B,
+                      psumA, psumB, sumA, sumB;
+
+  vector unsigned char sum, dst1, dst2, vdst, fsum,
+                       rsum, fdst1, fdst2;
+
+  POWERPC_PERF_START_COUNT(PREFIX_h264_qpel16_h_lowpass_num, 1);
+
+  for (i = 0 ; i &lt; 16 ; i ++) {
+    vector unsigned char srcR1 = vec_ld(-2, src);
+    vector unsigned char srcR2 = vec_ld(14, src);
+
+    switch (align) {
+    default: {
+      srcM2 = vec_perm(srcR1, srcR2, permM2);
+      srcM1 = vec_perm(srcR1, srcR2, permM1);
+      srcP0 = vec_perm(srcR1, srcR2, permP0);
+      srcP1 = vec_perm(srcR1, srcR2, permP1);
+      srcP2 = vec_perm(srcR1, srcR2, permP2);
+      srcP3 = vec_perm(srcR1, srcR2, permP3);
+    } break;
+    case 11: {
+      srcM2 = vec_perm(srcR1, srcR2, permM2);
+      srcM1 = vec_perm(srcR1, srcR2, permM1);
+      srcP0 = vec_perm(srcR1, srcR2, permP0);
+      srcP1 = vec_perm(srcR1, srcR2, permP1);
+      srcP2 = vec_perm(srcR1, srcR2, permP2);
+      srcP3 = srcR2;
+    } break;
+    case 12: {
+      vector unsigned char srcR3 = vec_ld(30, src);
+      srcM2 = vec_perm(srcR1, srcR2, permM2);
+      srcM1 = vec_perm(srcR1, srcR2, permM1);
+      srcP0 = vec_perm(srcR1, srcR2, permP0);
+      srcP1 = vec_perm(srcR1, srcR2, permP1);
+      srcP2 = srcR2;
+      srcP3 = vec_perm(srcR2, srcR3, permP3);
+    } break;
+    case 13: {
+      vector unsigned char srcR3 = vec_ld(30, src);
+      srcM2 = vec_perm(srcR1, srcR2, permM2);
+      srcM1 = vec_perm(srcR1, srcR2, permM1);
+      srcP0 = vec_perm(srcR1, srcR2, permP0);
+      srcP1 = srcR2;
+      srcP2 = vec_perm(srcR2, srcR3, permP2);
+      srcP3 = vec_perm(srcR2, srcR3, permP3);
+    } break;
+    case 14: {
+      vector unsigned char srcR3 = vec_ld(30, src);
+      srcM2 = vec_perm(srcR1, srcR2, permM2);
+      srcM1 = vec_perm(srcR1, srcR2, permM1);
+      srcP0 = srcR2;
+      srcP1 = vec_perm(srcR2, srcR3, permP1);
+      srcP2 = vec_perm(srcR2, srcR3, permP2);
+      srcP3 = vec_perm(srcR2, srcR3, permP3);
+    } break;
+    case 15: {
+      vector unsigned char srcR3 = vec_ld(30, src);
+      srcM2 = vec_perm(srcR1, srcR2, permM2);
+      srcM1 = srcR2;
+      srcP0 = vec_perm(srcR2, srcR3, permP0);
+      srcP1 = vec_perm(srcR2, srcR3, permP1);
+      srcP2 = vec_perm(srcR2, srcR3, permP2);
+      srcP3 = vec_perm(srcR2, srcR3, permP3);
+    } break;
+    }
+
+    srcP0A = (vector signed short)
+                vec_mergeh((vector unsigned char)vzero, srcP0);
+    srcP0B = (vector signed short)
+                vec_mergel((vector unsigned char)vzero, srcP0);
+    srcP1A = (vector signed short)
+                vec_mergeh((vector unsigned char)vzero, srcP1);
+    srcP1B = (vector signed short)
+                vec_mergel((vector unsigned char)vzero, srcP1);
+
+    srcP2A = (vector signed short)
+                vec_mergeh((vector unsigned char)vzero, srcP2);
+    srcP2B = (vector signed short)
+                vec_mergel((vector unsigned char)vzero, srcP2);
+    srcP3A = (vector signed short)
+                vec_mergeh((vector unsigned char)vzero, srcP3);
+    srcP3B = (vector signed short)
+                vec_mergel((vector unsigned char)vzero, srcP3);
+
+    srcM1A = (vector signed short)
+                vec_mergeh((vector unsigned char)vzero, srcM1);
+    srcM1B = (vector signed short)
+                vec_mergel((vector unsigned char)vzero, srcM1);
+    srcM2A = (vector signed short)
+                vec_mergeh((vector unsigned char)vzero, srcM2);
+    srcM2B = (vector signed short)
+                vec_mergel((vector unsigned char)vzero, srcM2);
+
+    sum1A = vec_adds(srcP0A, srcP1A);
+    sum1B = vec_adds(srcP0B, srcP1B);
+    sum2A = vec_adds(srcM1A, srcP2A);
+    sum2B = vec_adds(srcM1B, srcP2B);
+    sum3A = vec_adds(srcM2A, srcP3A);
+    sum3B = vec_adds(srcM2B, srcP3B);
+
+    pp1A = vec_mladd(sum1A, v20ss, v16ss);
+    pp1B = vec_mladd(sum1B, v20ss, v16ss);
+
+    pp2A = vec_mladd(sum2A, v5ss, (vector signed short)vzero);
+    pp2B = vec_mladd(sum2B, v5ss, (vector signed short)vzero);
+
+    pp3A = vec_add(sum3A, pp1A);
+    pp3B = vec_add(sum3B, pp1B);
+
+    psumA = vec_sub(pp3A, pp2A);
+    psumB = vec_sub(pp3B, pp2B);
+
+    sumA = vec_sra(psumA, v5us);
+    sumB = vec_sra(psumB, v5us);
+
+    sum = vec_packsu(sumA, sumB);
+
+    dst1 = vec_ld(0, dst);
+    dst2 = vec_ld(16, dst);
+    vdst = vec_perm(dst1, dst2, vec_lvsl(0, dst));
+
+    OP_U8_ALTIVEC(fsum, sum, vdst);
+
+    rsum = vec_perm(fsum, fsum, dstperm);
+    fdst1 = vec_sel(dst1, rsum, dstmask);
+    fdst2 = vec_sel(rsum, dst2, dstmask);
+
+    vec_st(fdst1, 0, dst);
+    vec_st(fdst2, 16, dst);
+
+    src += srcStride;
+    dst += dstStride;
+  }
+POWERPC_PERF_STOP_COUNT(PREFIX_h264_qpel16_h_lowpass_num, 1);
+}
+
+/* this code assume stride % 16 == 0 */
+static void PREFIX_h264_qpel16_v_lowpass_altivec(uint8_t * dst, uint8_t * src, int dstStride, int srcStride) {
+  POWERPC_PERF_DECLARE(PREFIX_h264_qpel16_v_lowpass_num, 1);
+
+  register int i;
+
+  const vector signed int vzero = vec_splat_s32(0);
+  const vector unsigned char perm = vec_lvsl(0, src);
+  const vector signed short v20ss = vec_sl(vec_splat_s16(5),vec_splat_u16(2));
+  const vector unsigned short v5us = vec_splat_u16(5);
+  const vector signed short v5ss = vec_splat_s16(5);
+  const vector signed short v16ss = vec_sl(vec_splat_s16(1),vec_splat_u16(4));
+  const vector unsigned char dstperm = vec_lvsr(0, dst);
+  const vector unsigned char neg1 = (const vector unsigned char)vec_splat_s8(-1);
+  const vector unsigned char dstmask = vec_perm((const vector unsigned char)vzero, neg1, dstperm);
+
+  uint8_t *srcbis = src - (srcStride * 2);
+
+  const vector unsigned char srcM2a = vec_ld(0, srcbis);
+  const vector unsigned char srcM2b = vec_ld(16, srcbis);
+  const vector unsigned char srcM2 = vec_perm(srcM2a, srcM2b, perm);
+//  srcbis += srcStride;
+  const vector unsigned char srcM1a = vec_ld(0, srcbis += srcStride);
+  const vector unsigned char srcM1b = vec_ld(16, srcbis);
+  const vector unsigned char srcM1 = vec_perm(srcM1a, srcM1b, perm);
+//  srcbis += srcStride;
+  const vector unsigned char srcP0a = vec_ld(0, srcbis += srcStride);
+  const vector unsigned char srcP0b = vec_ld(16, srcbis);
+  const vector unsigned char srcP0 = vec_perm(srcP0a, srcP0b, perm);
+//  srcbis += srcStride;
+  const vector unsigned char srcP1a = vec_ld(0, srcbis += srcStride);
+  const vector unsigned char srcP1b = vec_ld(16, srcbis);
+  const vector unsigned char srcP1 = vec_perm(srcP1a, srcP1b, perm);
+//  srcbis += srcStride;
+  const vector unsigned char srcP2a = vec_ld(0, srcbis += srcStride);
+  const vector unsigned char srcP2b = vec_ld(16, srcbis);
+  const vector unsigned char srcP2 = vec_perm(srcP2a, srcP2b, perm);
+//  srcbis += srcStride;
+
+  vector signed short srcM2ssA = (vector signed short)
+                                vec_mergeh((vector unsigned char)vzero, srcM2);
+  vector signed short srcM2ssB = (vector signed short)
+                                vec_mergel((vector unsigned char)vzero, srcM2);
+  vector signed short srcM1ssA = (vector signed short)
+                                vec_mergeh((vector unsigned char)vzero, srcM1);
+  vector signed short srcM1ssB = (vector signed short)
+                                vec_mergel((vector unsigned char)vzero, srcM1);
+  vector signed short srcP0ssA = (vector signed short)
+                                vec_mergeh((vector unsigned char)vzero, srcP0);
+  vector signed short srcP0ssB = (vector signed short)
+                                vec_mergel((vector unsigned char)vzero, srcP0);
+  vector signed short srcP1ssA = (vector signed short)
+                                vec_mergeh((vector unsigned char)vzero, srcP1);
+  vector signed short srcP1ssB = (vector signed short)
+                                vec_mergel((vector unsigned char)vzero, srcP1);
+  vector signed short srcP2ssA = (vector signed short)
+                                vec_mergeh((vector unsigned char)vzero, srcP2);
+  vector signed short srcP2ssB = (vector signed short)
+                                vec_mergel((vector unsigned char)vzero, srcP2);
+
+  vector signed short pp1A, pp1B, pp2A, pp2B, pp3A, pp3B,
+                      psumA, psumB, sumA, sumB,
+                      srcP3ssA, srcP3ssB,
+                      sum1A, sum1B, sum2A, sum2B, sum3A, sum3B;
+
+  vector unsigned char sum, dst1, dst2, vdst, fsum, rsum, fdst1, fdst2,
+                       srcP3a, srcP3b, srcP3;
+
+  POWERPC_PERF_START_COUNT(PREFIX_h264_qpel16_v_lowpass_num, 1);
+
+  for (i = 0 ; i &lt; 16 ; i++) {
+    srcP3a = vec_ld(0, srcbis += srcStride);
+    srcP3b = vec_ld(16, srcbis);
+    srcP3 = vec_perm(srcP3a, srcP3b, perm);
+    srcP3ssA = (vector signed short)
+                                vec_mergeh((vector unsigned char)vzero, srcP3);
+    srcP3ssB = (vector signed short)
+                                vec_mergel((vector unsigned char)vzero, srcP3);
+//    srcbis += srcStride;
+
+    sum1A = vec_adds(srcP0ssA, srcP1ssA);
+    sum1B = vec_adds(srcP0ssB, srcP1ssB);
+    sum2A = vec_adds(srcM1ssA, srcP2ssA);
+    sum2B = vec_adds(srcM1ssB, srcP2ssB);
+    sum3A = vec_adds(srcM2ssA, srcP3ssA);
+    sum3B = vec_adds(srcM2ssB, srcP3ssB);
+
+    srcM2ssA = srcM1ssA;
+    srcM2ssB = srcM1ssB;
+    srcM1ssA = srcP0ssA;
+    srcM1ssB = srcP0ssB;
+    srcP0ssA = srcP1ssA;
+    srcP0ssB = srcP1ssB;
+    srcP1ssA = srcP2ssA;
+    srcP1ssB = srcP2ssB;
+    srcP2ssA = srcP3ssA;
+    srcP2ssB = srcP3ssB;
+
+    pp1A = vec_mladd(sum1A, v20ss, v16ss);
+    pp1B = vec_mladd(sum1B, v20ss, v16ss);
+
+    pp2A = vec_mladd(sum2A, v5ss, (vector signed short)vzero);
+    pp2B = vec_mladd(sum2B, v5ss, (vector signed short)vzero);
+
+    pp3A = vec_add(sum3A, pp1A);
+    pp3B = vec_add(sum3B, pp1B);
+
+    psumA = vec_sub(pp3A, pp2A);
+    psumB = vec_sub(pp3B, pp2B);
+
+    sumA = vec_sra(psumA, v5us);
+    sumB = vec_sra(psumB, v5us);
+
+    sum = vec_packsu(sumA, sumB);
+
+    dst1 = vec_ld(0, dst);
+    dst2 = vec_ld(16, dst);
+    vdst = vec_perm(dst1, dst2, vec_lvsl(0, dst));
+
+    OP_U8_ALTIVEC(fsum, sum, vdst);
+
+    rsum = vec_perm(fsum, fsum, dstperm);
+    fdst1 = vec_sel(dst1, rsum, dstmask);
+    fdst2 = vec_sel(rsum, dst2, dstmask);
+
+    vec_st(fdst1, 0, dst);
+    vec_st(fdst2, 16, dst);
+
+    dst += dstStride;
+  }
+  POWERPC_PERF_STOP_COUNT(PREFIX_h264_qpel16_v_lowpass_num, 1);
+}
+
+/* this code assume stride % 16 == 0 *and* tmp is properly aligned */
+static void PREFIX_h264_qpel16_hv_lowpass_altivec(uint8_t * dst, int16_t * tmp, uint8_t * src, int dstStride, int tmpStride, int srcStride) {
+  POWERPC_PERF_DECLARE(PREFIX_h264_qpel16_hv_lowpass_num, 1);
+  register int i;
+  const vector signed int vzero = vec_splat_s32(0);
+  const vector unsigned char permM2 = vec_lvsl(-2, src);
+  const vector unsigned char permM1 = vec_lvsl(-1, src);
+  const vector unsigned char permP0 = vec_lvsl(+0, src);
+  const vector unsigned char permP1 = vec_lvsl(+1, src);
+  const vector unsigned char permP2 = vec_lvsl(+2, src);
+  const vector unsigned char permP3 = vec_lvsl(+3, src);
+  const vector signed short v20ss = vec_sl(vec_splat_s16(5),vec_splat_u16(2));
+  const vector unsigned int v10ui = vec_splat_u32(10);
+  const vector signed short v5ss = vec_splat_s16(5);
+  const vector signed short v1ss = vec_splat_s16(1);
+  const vector signed int v512si = vec_sl(vec_splat_s32(1),vec_splat_u32(9));
+  const vector unsigned int v16ui = vec_sl(vec_splat_u32(1),vec_splat_u32(4));
+
+  register int align = ((((unsigned long)src) - 2) % 16);
+
+  const vector unsigned char neg1 = (const vector unsigned char)
+                                                        vec_splat_s8(-1);
+
+  vector signed short srcP0A, srcP0B, srcP1A, srcP1B,
+                      srcP2A, srcP2B, srcP3A, srcP3B,
+                      srcM1A, srcM1B, srcM2A, srcM2B,
+                      sum1A, sum1B, sum2A, sum2B, sum3A, sum3B,
+                      pp1A, pp1B, pp2A, pp2B, psumA, psumB;
+
+  const vector unsigned char dstperm = vec_lvsr(0, dst);
+
+  const vector unsigned char dstmask = vec_perm((const vector unsigned char)vzero, neg1, dstperm);
+
+  const vector unsigned char mperm = (const vector unsigned char)
+    AVV(0x00, 0x08, 0x01, 0x09, 0x02, 0x0A, 0x03, 0x0B,
+        0x04, 0x0C, 0x05, 0x0D, 0x06, 0x0E, 0x07, 0x0F);
+  int16_t *tmpbis = tmp;
+
+  vector signed short tmpM1ssA, tmpM1ssB, tmpM2ssA, tmpM2ssB,
+                      tmpP0ssA, tmpP0ssB, tmpP1ssA, tmpP1ssB,
+                      tmpP2ssA, tmpP2ssB;
+
+  vector signed int pp1Ae, pp1Ao, pp1Be, pp1Bo, pp2Ae, pp2Ao, pp2Be, pp2Bo,
+                    pp3Ae, pp3Ao, pp3Be, pp3Bo, pp1cAe, pp1cAo, pp1cBe, pp1cBo,
+                    pp32Ae, pp32Ao, pp32Be, pp32Bo, sumAe, sumAo, sumBe, sumBo,
+                    ssumAe, ssumAo, ssumBe, ssumBo;
+  vector unsigned char fsum, sumv, sum, dst1, dst2, vdst,
+                       rsum, fdst1, fdst2;
+  vector signed short ssume, ssumo;
+
+  POWERPC_PERF_START_COUNT(PREFIX_h264_qpel16_hv_lowpass_num, 1);
+  src -= (2 * srcStride);
+  for (i = 0 ; i &lt; 21 ; i ++) {
+    vector unsigned char srcM2, srcM1, srcP0, srcP1, srcP2, srcP3;
+    vector unsigned char srcR1 = vec_ld(-2, src);
+    vector unsigned char srcR2 = vec_ld(14, src);
+
+    switch (align) {
+    default: {
+      srcM2 = vec_perm(srcR1, srcR2, permM2);
+      srcM1 = vec_perm(srcR1, srcR2, permM1);
+      srcP0 = vec_perm(srcR1, srcR2, permP0);
+      srcP1 = vec_perm(srcR1, srcR2, permP1);
+      srcP2 = vec_perm(srcR1, srcR2, permP2);
+      srcP3 = vec_perm(srcR1, srcR2, permP3);
+    } break;
+    case 11: {
+      srcM2 = vec_perm(srcR1, srcR2, permM2);
+      srcM1 = vec_perm(srcR1, srcR2, permM1);
+      srcP0 = vec_perm(srcR1, srcR2, permP0);
+      srcP1 = vec_perm(srcR1, srcR2, permP1);
+      srcP2 = vec_perm(srcR1, srcR2, permP2);
+      srcP3 = srcR2;
+    } break;
+    case 12: {
+      vector unsigned char srcR3 = vec_ld(30, src);
+      srcM2 = vec_perm(srcR1, srcR2, permM2);
+      srcM1 = vec_perm(srcR1, srcR2, permM1);
+      srcP0 = vec_perm(srcR1, srcR2, permP0);
+      srcP1 = vec_perm(srcR1, srcR2, permP1);
+      srcP2 = srcR2;
+      srcP3 = vec_perm(srcR2, srcR3, permP3);
+    } break;
+    case 13: {
+      vector unsigned char srcR3 = vec_ld(30, src);
+      srcM2 = vec_perm(srcR1, srcR2, permM2);
+      srcM1 = vec_perm(srcR1, srcR2, permM1);
+      srcP0 = vec_perm(srcR1, srcR2, permP0);
+      srcP1 = srcR2;
+      srcP2 = vec_perm(srcR2, srcR3, permP2);
+      srcP3 = vec_perm(srcR2, srcR3, permP3);
+    } break;
+    case 14: {
+      vector unsigned char srcR3 = vec_ld(30, src);
+      srcM2 = vec_perm(srcR1, srcR2, permM2);
+      srcM1 = vec_perm(srcR1, srcR2, permM1);
+      srcP0 = srcR2;
+      srcP1 = vec_perm(srcR2, srcR3, permP1);
+      srcP2 = vec_perm(srcR2, srcR3, permP2);
+      srcP3 = vec_perm(srcR2, srcR3, permP3);
+    } break;
+    case 15: {
+      vector unsigned char srcR3 = vec_ld(30, src);
+      srcM2 = vec_perm(srcR1, srcR2, permM2);
+      srcM1 = srcR2;
+      srcP0 = vec_perm(srcR2, srcR3, permP0);
+      srcP1 = vec_perm(srcR2, srcR3, permP1);
+      srcP2 = vec_perm(srcR2, srcR3, permP2);
+      srcP3 = vec_perm(srcR2, srcR3, permP3);
+    } break;
+    }
+
+    srcP0A = (vector signed short)
+                            vec_mergeh((vector unsigned char)vzero, srcP0);
+    srcP0B = (vector signed short)
+                            vec_mergel((vector unsigned char)vzero, srcP0);
+    srcP1A = (vector signed short)
+                            vec_mergeh((vector unsigned char)vzero, srcP1);
+    srcP1B = (vector signed short)
+                            vec_mergel((vector unsigned char)vzero, srcP1);
+
+    srcP2A = (vector signed short)
+                            vec_mergeh((vector unsigned char)vzero, srcP2);
+    srcP2B = (vector signed short)
+                            vec_mergel((vector unsigned char)vzero, srcP2);
+    srcP3A = (vector signed short)
+                            vec_mergeh((vector unsigned char)vzero, srcP3);
+    srcP3B = (vector signed short)
+                            vec_mergel((vector unsigned char)vzero, srcP3);
+
+    srcM1A = (vector signed short)
+                            vec_mergeh((vector unsigned char)vzero, srcM1);
+    srcM1B = (vector signed short)
+                            vec_mergel((vector unsigned char)vzero, srcM1);
+    srcM2A = (vector signed short)
+                            vec_mergeh((vector unsigned char)vzero, srcM2);
+    srcM2B = (vector signed short)
+                            vec_mergel((vector unsigned char)vzero, srcM2);
+
+    sum1A = vec_adds(srcP0A, srcP1A);
+    sum1B = vec_adds(srcP0B, srcP1B);
+    sum2A = vec_adds(srcM1A, srcP2A);
+    sum2B = vec_adds(srcM1B, srcP2B);
+    sum3A = vec_adds(srcM2A, srcP3A);
+    sum3B = vec_adds(srcM2B, srcP3B);
+
+    pp1A = vec_mladd(sum1A, v20ss, sum3A);
+    pp1B = vec_mladd(sum1B, v20ss, sum3B);
+
+    pp2A = vec_mladd(sum2A, v5ss, (vector signed short)vzero);
+    pp2B = vec_mladd(sum2B, v5ss, (vector signed short)vzero);
+
+    psumA = vec_sub(pp1A, pp2A);
+    psumB = vec_sub(pp1B, pp2B);
+
+    vec_st(psumA, 0, tmp);
+    vec_st(psumB, 16, tmp);
+
+    src += srcStride;
+    tmp += tmpStride; /* int16_t*, and stride is 16, so it's OK here */
+  }
+
+  tmpM2ssA = vec_ld(0, tmpbis);
+  tmpM2ssB = vec_ld(16, tmpbis);
+  tmpbis += tmpStride;
+  tmpM1ssA = vec_ld(0, tmpbis);
+  tmpM1ssB = vec_ld(16, tmpbis);
+  tmpbis += tmpStride;
+  tmpP0ssA = vec_ld(0, tmpbis);
+  tmpP0ssB = vec_ld(16, tmpbis);
+  tmpbis += tmpStride;
+  tmpP1ssA = vec_ld(0, tmpbis);
+  tmpP1ssB = vec_ld(16, tmpbis);
+  tmpbis += tmpStride;
+  tmpP2ssA = vec_ld(0, tmpbis);
+  tmpP2ssB = vec_ld(16, tmpbis);
+  tmpbis += tmpStride;
+
+  for (i = 0 ; i &lt; 16 ; i++) {
+    const vector signed short tmpP3ssA = vec_ld(0, tmpbis);
+    const vector signed short tmpP3ssB = vec_ld(16, tmpbis);
+
+    const vector signed short sum1A = vec_adds(tmpP0ssA, tmpP1ssA);
+    const vector signed short sum1B = vec_adds(tmpP0ssB, tmpP1ssB);
+    const vector signed short sum2A = vec_adds(tmpM1ssA, tmpP2ssA);
+    const vector signed short sum2B = vec_adds(tmpM1ssB, tmpP2ssB);
+    const vector signed short sum3A = vec_adds(tmpM2ssA, tmpP3ssA);
+    const vector signed short sum3B = vec_adds(tmpM2ssB, tmpP3ssB);
+
+    tmpbis += tmpStride;
+
+    tmpM2ssA = tmpM1ssA;
+    tmpM2ssB = tmpM1ssB;
+    tmpM1ssA = tmpP0ssA;
+    tmpM1ssB = tmpP0ssB;
+    tmpP0ssA = tmpP1ssA;
+    tmpP0ssB = tmpP1ssB;
+    tmpP1ssA = tmpP2ssA;
+    tmpP1ssB = tmpP2ssB;
+    tmpP2ssA = tmpP3ssA;
+    tmpP2ssB = tmpP3ssB;
+
+    pp1Ae = vec_mule(sum1A, v20ss);
+    pp1Ao = vec_mulo(sum1A, v20ss);
+    pp1Be = vec_mule(sum1B, v20ss);
+    pp1Bo = vec_mulo(sum1B, v20ss);
+
+    pp2Ae = vec_mule(sum2A, v5ss);
+    pp2Ao = vec_mulo(sum2A, v5ss);
+    pp2Be = vec_mule(sum2B, v5ss);
+    pp2Bo = vec_mulo(sum2B, v5ss);
+
+    pp3Ae = vec_sra((vector signed int)sum3A, v16ui);
+    pp3Ao = vec_mulo(sum3A, v1ss);
+    pp3Be = vec_sra((vector signed int)sum3B, v16ui);
+    pp3Bo = vec_mulo(sum3B, v1ss);
+
+    pp1cAe = vec_add(pp1Ae, v512si);
+    pp1cAo = vec_add(pp1Ao, v512si);
+    pp1cBe = vec_add(pp1Be, v512si);
+    pp1cBo = vec_add(pp1Bo, v512si);
+
+    pp32Ae = vec_sub(pp3Ae, pp2Ae);
+    pp32Ao = vec_sub(pp3Ao, pp2Ao);
+    pp32Be = vec_sub(pp3Be, pp2Be);
+    pp32Bo = vec_sub(pp3Bo, pp2Bo);
+
+    sumAe = vec_add(pp1cAe, pp32Ae);
+    sumAo = vec_add(pp1cAo, pp32Ao);
+    sumBe = vec_add(pp1cBe, pp32Be);
+    sumBo = vec_add(pp1cBo, pp32Bo);
+
+    ssumAe = vec_sra(sumAe, v10ui);
+    ssumAo = vec_sra(sumAo, v10ui);
+    ssumBe = vec_sra(sumBe, v10ui);
+    ssumBo = vec_sra(sumBo, v10ui);
+
+    ssume = vec_packs(ssumAe, ssumBe);
+    ssumo = vec_packs(ssumAo, ssumBo);
+
+    sumv = vec_packsu(ssume, ssumo);
+    sum = vec_perm(sumv, sumv, mperm);
+
+    dst1 = vec_ld(0, dst);
+    dst2 = vec_ld(16, dst);
+    vdst = vec_perm(dst1, dst2, vec_lvsl(0, dst));
+
+    OP_U8_ALTIVEC(fsum, sum, vdst);
+
+    rsum = vec_perm(fsum, fsum, dstperm);
+    fdst1 = vec_sel(dst1, rsum, dstmask);
+    fdst2 = vec_sel(rsum, dst2, dstmask);
+
+    vec_st(fdst1, 0, dst);
+    vec_st(fdst2, 16, dst);
+
+    dst += dstStride;
+  }
+  POWERPC_PERF_STOP_COUNT(PREFIX_h264_qpel16_hv_lowpass_num, 1);
+}

Modified: branches/avidemux_2.4_branch/avidemux/ADM_libraries/ADM_lavcodec/ppc/idct_altivec.c
===================================================================
--- branches/avidemux_2.4_branch/avidemux/ADM_libraries/ADM_lavcodec/ppc/idct_altivec.c	2007-12-03 12:38:06 UTC (rev 3719)
+++ branches/avidemux_2.4_branch/avidemux/ADM_libraries/ADM_lavcodec/ppc/idct_altivec.c	2007-12-04 00:19:48 UTC (rev 3720)
@@ -16,7 +16,6 @@
  * You should have received a copy of the GNU Lesser General Public
  * License along with FFmpeg; if not, write to the Free Software
  * Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA
- *
  */
 
 /*
@@ -39,7 +38,7 @@
 
 #include &lt;stdlib.h&gt;                                      /* malloc(), free() */
 #include &lt;string.h&gt;
-#include &quot;../dsputil.h&quot;
+#include &quot;dsputil.h&quot;
 
 #include &quot;gcc_fixes.h&quot;
 
@@ -171,7 +170,7 @@
 POWERPC_PERF_DECLARE(altivec_idct_put_num, 1);
     vector_u8_t tmp;
 
-#ifdef POWERPC_PERFORMANCE_REPORT
+#ifdef CONFIG_POWERPC_PERF
 POWERPC_PERF_START_COUNT(altivec_idct_put_num, 1);
 #endif
     IDCT
@@ -202,7 +201,7 @@
     vector_u8_t perm1;
     vector_u8_t p0, p1, p;
 
-#ifdef POWERPC_PERFORMANCE_REPORT
+#ifdef CONFIG_POWERPC_PERF
 POWERPC_PERF_START_COUNT(altivec_idct_add_num, 1);
 #endif
 

Added: branches/avidemux_2.4_branch/avidemux/ADM_libraries/ADM_lavcodec/ppc/int_altivec.c
===================================================================
--- branches/avidemux_2.4_branch/avidemux/ADM_libraries/ADM_lavcodec/ppc/int_altivec.c	2007-12-03 12:38:06 UTC (rev 3719)
+++ branches/avidemux_2.4_branch/avidemux/ADM_libraries/ADM_lavcodec/ppc/int_altivec.c	2007-12-04 00:19:48 UTC (rev 3720)
@@ -0,0 +1,80 @@
+/*
+ * Copyright (c) 2007 Luca Barbato &lt;<A HREF="https://lists.berlios.de/mailman/listinfo/avidemux-svn-commit">lu_zero at gentoo.org</A>&gt;
+ *
+ * This file is part of FFmpeg.
+ *
+ * FFmpeg is free software; you can redistribute it and/or
+ * modify it under the terms of the GNU Lesser General Public
+ * License as published by the Free Software Foundation; either
+ * version 2.1 of the License, or (at your option) any later version.
+ *
+ * FFmpeg is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+ * Lesser General Public License for more details.
+ *
+ * You should have received a copy of the GNU Lesser General Public
+ * License along with FFmpeg; if not, write to the Free Software
+ * Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA
+ */
+
+/**
+ ** @file int_altivec.c
+ ** integer misc ops.
+ **/
+
+#include &quot;dsputil.h&quot;
+
+#include &quot;gcc_fixes.h&quot;
+
+#include &quot;dsputil_altivec.h&quot;
+
+static int ssd_int8_vs_int16_altivec(const int8_t *pix1, const int16_t *pix2,
+                                     int size) {
+    int i, size16;
+    vector signed char vpix1;
+    vector signed short vpix2, vdiff, vpix1l,vpix1h;
+    union { vector signed int vscore;
+            int32_t score[4];
+           } u;
+    u.vscore = vec_splat_s32(0);
+//
+//XXX lazy way, fix it later
+
+#define vec_unaligned_load(b) \
+    vec_perm(vec_ld(0,b),vec_ld(15,b),vec_lvsl(0, b));
+
+    size16 = size &gt;&gt; 4;
+    while(size16) {
+//        score += (pix1[i]-pix2[i])*(pix1[i]-pix2[i]);
+        //load pix1 and the first batch of pix2
+
+        vpix1 = vec_unaligned_load(pix1);
+        vpix2 = vec_unaligned_load(pix2);
+        pix2 += 8;
+        //unpack
+        vpix1h = vec_unpackh(vpix1);
+        vdiff  = vec_sub(vpix1h, vpix2);
+        vpix1l = vec_unpackl(vpix1);
+        // load another batch from pix2
+        vpix2 = vec_unaligned_load(pix2);
+        u.vscore = vec_msum(vdiff, vdiff, u.vscore);
+        vdiff  = vec_sub(vpix1l, vpix2);
+        u.vscore = vec_msum(vdiff, vdiff, u.vscore);
+        pix1 += 16;
+        pix2 += 8;
+        size16--;
+    }
+    u.vscore = vec_sums(u.vscore, vec_splat_s32(0));
+
+    size %= 16;
+    for (i = 0; i &lt; size; i++) {
+        u.score[3] += (pix1[i]-pix2[i])*(pix1[i]-pix2[i]);
+    }
+    return u.score[3];
+}
+
+void int_init_altivec(DSPContext* c, AVCodecContext *avctx)
+{
+    c-&gt;ssd_int8_vs_int16 = ssd_int8_vs_int16_altivec;
+}

Added: branches/avidemux_2.4_branch/avidemux/ADM_libraries/ADM_lavcodec/ppc/mathops.h
===================================================================
--- branches/avidemux_2.4_branch/avidemux/ADM_libraries/ADM_lavcodec/ppc/mathops.h	2007-12-03 12:38:06 UTC (rev 3719)
+++ branches/avidemux_2.4_branch/avidemux/ADM_libraries/ADM_lavcodec/ppc/mathops.h	2007-12-04 00:19:48 UTC (rev 3720)
@@ -0,0 +1,38 @@
+/*
+ * simple math operations
+ * Copyright (c) 2001, 2002 Fabrice Bellard.
+ * Copyright (c) 2006 Michael Niedermayer &lt;<A HREF="https://lists.berlios.de/mailman/listinfo/avidemux-svn-commit">michaelni at gmx.at</A>&gt; et al
+ *
+ * This file is part of FFmpeg.
+ *
+ * FFmpeg is free software; you can redistribute it and/or
+ * modify it under the terms of the GNU Lesser General Public
+ * License as published by the Free Software Foundation; either
+ * version 2.1 of the License, or (at your option) any later version.
+ *
+ * FFmpeg is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+ * Lesser General Public License for more details.
+ *
+ * You should have received a copy of the GNU Lesser General Public
+ * License along with FFmpeg; if not, write to the Free Software
+ * Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA
+ */
+
+#ifndef AVCODEC_PPC_MATHOPS_H
+#define AVCODEC_PPC_MATHOPS_H
+
+#if defined(ARCH_POWERPC_405)
+/* signed 16x16 -&gt; 32 multiply add accumulate */
+#   define MAC16(rt, ra, rb) \
+        asm (&quot;maclhw %0, %2, %3&quot; : &quot;=r&quot; (rt) : &quot;0&quot; (rt), &quot;r&quot; (ra), &quot;r&quot; (rb));
+
+/* signed 16x16 -&gt; 32 multiply */
+#   define MUL16(ra, rb) \
+        ({ int __rt;
+         asm (&quot;mullhw %0, %1, %2&quot; : &quot;=r&quot; (__rt) : &quot;r&quot; (ra), &quot;r&quot; (rb));
+         __rt; })
+#endif
+
+#endif // AVCODEC_PPC_MATHOPS_H

Modified: branches/avidemux_2.4_branch/avidemux/ADM_libraries/ADM_lavcodec/ppc/mpegvideo_altivec.c
===================================================================
--- branches/avidemux_2.4_branch/avidemux/ADM_libraries/ADM_lavcodec/ppc/mpegvideo_altivec.c	2007-12-03 12:38:06 UTC (rev 3719)
+++ branches/avidemux_2.4_branch/avidemux/ADM_libraries/ADM_lavcodec/ppc/mpegvideo_altivec.c	2007-12-04 00:19:48 UTC (rev 3720)
@@ -23,8 +23,8 @@
 
 #include &lt;stdlib.h&gt;
 #include &lt;stdio.h&gt;
-#include &quot;../dsputil.h&quot;
-#include &quot;../mpegvideo.h&quot;
+#include &quot;dsputil.h&quot;
+#include &quot;mpegvideo.h&quot;
 
 #include &quot;gcc_fixes.h&quot;
 
@@ -515,7 +515,7 @@
         }else
             qadd = 0;
         i = 1;
-        nCoeffs= 63; //does not allways use zigzag table
+        nCoeffs= 63; //does not always use zigzag table
     } else {
         i = 0;
         nCoeffs= s-&gt;intra_scantable.raster_end[ s-&gt;block_last_index[n] ];
@@ -523,17 +523,17 @@
 
     {
       register const_vector signed short vczero = (const_vector signed short)vec_splat_s16(0);
-      short __attribute__ ((aligned(16))) qmul8[] =
+      DECLARE_ALIGNED_16(short, qmul8[]) =
           {
             qmul, qmul, qmul, qmul,
             qmul, qmul, qmul, qmul
           };
-      short __attribute__ ((aligned(16))) qadd8[] =
+      DECLARE_ALIGNED_16(short, qadd8[]) =
           {
             qadd, qadd, qadd, qadd,
             qadd, qadd, qadd, qadd
           };
-      short __attribute__ ((aligned(16))) nqadd8[] =
+      DECLARE_ALIGNED_16(short, nqadd8[]) =
           {
             -qadd, -qadd, -qadd, -qadd,
             -qadd, -qadd, -qadd, -qadd

Modified: branches/avidemux_2.4_branch/avidemux/ADM_libraries/ADM_lavcodec/ppc/mpegvideo_ppc.c
===================================================================
--- branches/avidemux_2.4_branch/avidemux/ADM_libraries/ADM_lavcodec/ppc/mpegvideo_ppc.c	2007-12-03 12:38:06 UTC (rev 3719)
+++ branches/avidemux_2.4_branch/avidemux/ADM_libraries/ADM_lavcodec/ppc/mpegvideo_ppc.c	2007-12-04 00:19:48 UTC (rev 3720)
@@ -18,8 +18,8 @@
  * Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA
  */
 
-#include &quot;../dsputil.h&quot;
-#include &quot;../mpegvideo.h&quot;
+#include &quot;dsputil.h&quot;
+#include &quot;mpegvideo.h&quot;
 #include &lt;time.h&gt;
 
 #ifdef HAVE_ALTIVEC

Modified: branches/avidemux_2.4_branch/avidemux/ADM_libraries/ADM_lavcodec/ppc/snow_altivec.c
===================================================================
--- branches/avidemux_2.4_branch/avidemux/ADM_libraries/ADM_lavcodec/ppc/snow_altivec.c	2007-12-03 12:38:06 UTC (rev 3719)
+++ branches/avidemux_2.4_branch/avidemux/ADM_libraries/ADM_lavcodec/ppc/snow_altivec.c	2007-12-04 00:19:48 UTC (rev 3720)
@@ -1,788 +1,786 @@
-/*
- * Altivec optimized snow DSP utils
- * Copyright (c) 2006 Luca Barbato &lt;<A HREF="https://lists.berlios.de/mailman/listinfo/avidemux-svn-commit">lu_zero at gentoo.org</A>&gt;
- *
- * This file is part of FFmpeg.
- *
- * FFmpeg is free software; you can redistribute it and/or
- * modify it under the terms of the GNU Lesser General Public
- * License as published by the Free Software Foundation; either
- * version 2.1 of the License, or (at your option) any later version.
- *
- * FFmpeg is distributed in the hope that it will be useful,
- * but WITHOUT ANY WARRANTY; without even the implied warranty of
- * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
- * Lesser General Public License for more details.
- *
- * You should have received a copy of the GNU Lesser General Public
- * License along with FFmpeg; if not, write to the Free Software
- * Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA
- *
- *
- */
-
-#include &quot;../dsputil.h&quot;
-
-#include &quot;gcc_fixes.h&quot;
-#include &quot;dsputil_altivec.h&quot;
-#include &quot;../snow.h&quot;
-
-#undef NDEBUG
-#include &lt;assert.h&gt;
-
-
-
-//FIXME remove this replication
-#define slice_buffer_get_line(slice_buf, line_num) ((slice_buf)-&gt;line[line_num] ? (slice_buf)-&gt;line[line_num] : slice_buffer_load_line((slice_buf), (line_num)))
-
-static DWTELEM * slice_buffer_load_line(slice_buffer * buf, int line)
-{
-    int offset;
-    DWTELEM * buffer;
-
-//  av_log(NULL, AV_LOG_DEBUG, &quot;Cache hit: %d\n&quot;, line);
-
-    assert(buf-&gt;data_stack_top &gt;= 0);
-//  assert(!buf-&gt;line[line]);
-    if (buf-&gt;line[line])
-        return buf-&gt;line[line];
-
-    offset = buf-&gt;line_width * line;
-    buffer = buf-&gt;data_stack[buf-&gt;data_stack_top];
-    buf-&gt;data_stack_top--;
-    buf-&gt;line[line] = buffer;
-
-//  av_log(NULL, AV_LOG_DEBUG, &quot;slice_buffer_load_line: line: %d remaining: %d\n&quot;, line, buf-&gt;data_stack_top + 1);
-
-    return buffer;
-}
-
-
-//altivec code
-
-void ff_snow_horizontal_compose97i_altivec(DWTELEM *b, int width)
-{
-    const int w2= (width+1)&gt;&gt;1;
-    DECLARE_ALIGNED_16(DWTELEM, temp[(width&gt;&gt;1)]);
-    const int w_l= (width&gt;&gt;1);
-    const int w_r= w2 - 1;
-    int i;
-    vector signed int t1, t2, x, y, tmp1, tmp2;
-    vector signed int *vbuf, *vtmp;
-    vector unsigned char align;
-
-
-
-    { // Lift 0
-        DWTELEM * const ref = b + w2 - 1;
-        DWTELEM b_0 = b[0];
-        vbuf = (vector signed int *)b;
-
-        tmp1 = vec_ld (0, ref);
-        align = vec_lvsl (0, ref);
-        tmp2 = vec_ld (15, ref);
-        t1= vec_perm(tmp1, tmp2, align);
-
-        i = 0;
-
-        for (i=0; i&lt;w_l-15; i+=16) {
-#if 0
-        b[i+0] = b[i+0] - ((3 * (ref[i+0] + ref[i+1]) + 4) &gt;&gt; 3);
-        b[i+1] = b[i+1] - ((3 * (ref[i+1] + ref[i+2]) + 4) &gt;&gt; 3);
-        b[i+2] = b[i+2] - ((3 * (ref[i+2] + ref[i+3]) + 4) &gt;&gt; 3);
-        b[i+3] = b[i+3] - ((3 * (ref[i+3] + ref[i+4]) + 4) &gt;&gt; 3);
-#else
-
-        tmp1 = vec_ld (0, ref+4+i);
-        tmp2 = vec_ld (15, ref+4+i);
-
-        t2 = vec_perm(tmp1, tmp2, align);
-
-        y = vec_add(t1,vec_sld(t1,t2,4));
-        y = vec_add(vec_add(y,y),y);
-
-        tmp1 = vec_ld (0, ref+8+i);
-
-        y = vec_add(y, vec_splat_s32(4));
-        y = vec_sra(y, vec_splat_u32(3));
-
-        tmp2 = vec_ld (15, ref+8+i);
-
-        *vbuf = vec_sub(*vbuf, y);
-
-        t1=t2;
-
-        vbuf++;
-
-        t2 = vec_perm(tmp1, tmp2, align);
-
-        y = vec_add(t1,vec_sld(t1,t2,4));
-        y = vec_add(vec_add(y,y),y);
-
-        tmp1 = vec_ld (0, ref+12+i);
-
-        y = vec_add(y, vec_splat_s32(4));
-        y = vec_sra(y, vec_splat_u32(3));
-
-        tmp2 = vec_ld (15, ref+12+i);
-
-        *vbuf = vec_sub(*vbuf, y);
-
-        t1=t2;
-
-        vbuf++;
-
-        t2 = vec_perm(tmp1, tmp2, align);
-
-        y = vec_add(t1,vec_sld(t1,t2,4));
-        y = vec_add(vec_add(y,y),y);
-
-        tmp1 = vec_ld (0, ref+16+i);
-
-        y = vec_add(y, vec_splat_s32(4));
-        y = vec_sra(y, vec_splat_u32(3));
-
-        tmp2 = vec_ld (15, ref+16+i);
-
-        *vbuf = vec_sub(*vbuf, y);
-
-        t1=t2;
-
-        t2 = vec_perm(tmp1, tmp2, align);
-
-        y = vec_add(t1,vec_sld(t1,t2,4));
-        y = vec_add(vec_add(y,y),y);
-
-        vbuf++;
-
-        y = vec_add(y, vec_splat_s32(4));
-        y = vec_sra(y, vec_splat_u32(3));
-        *vbuf = vec_sub(*vbuf, y);
-
-        t1=t2;
-
-        vbuf++;
-
-#endif
-        }
-
-        snow_horizontal_compose_lift_lead_out(i, b, b, ref, width, w_l, 0, W_DM, W_DO, W_DS);
-        b[0] = b_0 - ((W_DM * 2 * ref[1]+W_DO)&gt;&gt;W_DS);
-    }
-
-    { // Lift 1
-        DWTELEM * const dst = b+w2;
-
-        i = 0;
-        for(; (((long)&amp;dst[i]) &amp; 0xF) &amp;&amp; i&lt;w_r; i++){
-            dst[i] = dst[i] - (b[i] + b[i + 1]);
-        }
-
-        align = vec_lvsl(0, b+i);
-        tmp1 = vec_ld(0, b+i);
-        vbuf = (vector signed int*) (dst + i);
-        tmp2 = vec_ld(15, b+i);
-
-        t1 = vec_perm(tmp1, tmp2, align);
-
-        for (; i&lt;w_r-3; i+=4) {
-
-#if 0
-            dst[i]   = dst[i]   - (b[i]   + b[i + 1]);
-            dst[i+1] = dst[i+1] - (b[i+1] + b[i + 2]);
-            dst[i+2] = dst[i+2] - (b[i+2] + b[i + 3]);
-            dst[i+3] = dst[i+3] - (b[i+3] + b[i + 4]);
-#else
-
-        tmp1 = vec_ld(0, b+4+i);
-        tmp2 = vec_ld(15, b+4+i);
-
-        t2 = vec_perm(tmp1, tmp2, align);
-
-        y = vec_add(t1, vec_sld(t1,t2,4));
-        *vbuf = vec_sub (*vbuf, y);
-
-        vbuf++;
-
-        t1 = t2;
-
-#endif
-
-        }
-
-        snow_horizontal_compose_lift_lead_out(i, dst, dst, b, width, w_r, 1, W_CM, W_CO, W_CS);
-    }
-
-    { // Lift 2
-        DWTELEM * const ref = b+w2 - 1;
-        DWTELEM b_0 = b[0];
-        vbuf= (vector signed int *) b;
-
-        tmp1 = vec_ld (0, ref);
-        align = vec_lvsl (0, ref);
-        tmp2 = vec_ld (15, ref);
-        t1= vec_perm(tmp1, tmp2, align);
-
-        i = 0;
-        for (; i&lt;w_l-15; i+=16) {
-#if 0
-            b[i]   = b[i]   - (((8 -(ref[i]   + ref[i+1])) - (b[i]  &lt;&lt;2)) &gt;&gt; 4);
-            b[i+1] = b[i+1] - (((8 -(ref[i+1] + ref[i+2])) - (b[i+1]&lt;&lt;2)) &gt;&gt; 4);
-            b[i+2] = b[i+2] - (((8 -(ref[i+2] + ref[i+3])) - (b[i+2]&lt;&lt;2)) &gt;&gt; 4);
-            b[i+3] = b[i+3] - (((8 -(ref[i+3] + ref[i+4])) - (b[i+3]&lt;&lt;2)) &gt;&gt; 4);
-#else
-            tmp1 = vec_ld (0, ref+4+i);
-            tmp2 = vec_ld (15, ref+4+i);
-
-            t2 = vec_perm(tmp1, tmp2, align);
-
-            y = vec_add(t1,vec_sld(t1,t2,4));
-            y = vec_sub(vec_splat_s32(8),y);
-
-            tmp1 = vec_ld (0, ref+8+i);
-
-            x = vec_sl(*vbuf,vec_splat_u32(2));
-            y = vec_sra(vec_sub(y,x),vec_splat_u32(4));
-
-            tmp2 = vec_ld (15, ref+8+i);
-
-            *vbuf = vec_sub( *vbuf, y);
-
-            t1 = t2;
-
-            vbuf++;
-
-            t2 = vec_perm(tmp1, tmp2, align);
-
-            y = vec_add(t1,vec_sld(t1,t2,4));
-            y = vec_sub(vec_splat_s32(8),y);
-
-            tmp1 = vec_ld (0, ref+12+i);
-
-            x = vec_sl(*vbuf,vec_splat_u32(2));
-            y = vec_sra(vec_sub(y,x),vec_splat_u32(4));
-
-            tmp2 = vec_ld (15, ref+12+i);
-
-            *vbuf = vec_sub( *vbuf, y);
-
-            t1 = t2;
-
-            vbuf++;
-
-            t2 = vec_perm(tmp1, tmp2, align);
-
-            y = vec_add(t1,vec_sld(t1,t2,4));
-            y = vec_sub(vec_splat_s32(8),y);
-
-            tmp1 = vec_ld (0, ref+16+i);
-
-            x = vec_sl(*vbuf,vec_splat_u32(2));
-            y = vec_sra(vec_sub(y,x),vec_splat_u32(4));
-
-            tmp2 = vec_ld (15, ref+16+i);
-
-            *vbuf = vec_sub( *vbuf, y);
-
-            t1 = t2;
-
-            vbuf++;
-
-            t2 = vec_perm(tmp1, tmp2, align);
-
-            y = vec_add(t1,vec_sld(t1,t2,4));
-            y = vec_sub(vec_splat_s32(8),y);
-
-            t1 = t2;
-
-            x = vec_sl(*vbuf,vec_splat_u32(2));
-            y = vec_sra(vec_sub(y,x),vec_splat_u32(4));
-            *vbuf = vec_sub( *vbuf, y);
-
-            vbuf++;
-
-#endif
-        }
-
-        snow_horizontal_compose_liftS_lead_out(i, b, b, ref, width, w_l);
-        b[0] = b_0 - (((-2 * ref[1] + W_BO) - 4 * b_0) &gt;&gt; W_BS);
-    }
-
-    { // Lift 3
-        DWTELEM * const src = b+w2;
-
-        vbuf = (vector signed int *)b;
-        vtmp = (vector signed int *)temp;
-
-        i = 0;
-        align = vec_lvsl(0, src);
-
-        for (; i&lt;w_r-3; i+=4) {
-#if 0
-            temp[i] = src[i] - ((-3*(b[i] + b[i+1]))&gt;&gt;1);
-            temp[i+1] = src[i+1] - ((-3*(b[i+1] + b[i+2]))&gt;&gt;1);
-            temp[i+2] = src[i+2] - ((-3*(b[i+2] + b[i+3]))&gt;&gt;1);
-            temp[i+3] = src[i+3] - ((-3*(b[i+3] + b[i+4]))&gt;&gt;1);
-#else
-            tmp1 = vec_ld(0,src+i);
-            t1 = vec_add(vbuf[0],vec_sld(vbuf[0],vbuf[1],4));
-            tmp2 = vec_ld(15,src+i);
-            t1 = vec_sub(vec_splat_s32(0),t1); //bad!
-            t1 = vec_add(t1,vec_add(t1,t1));
-            t2 = vec_perm(tmp1 ,tmp2 ,align);
-            t1 = vec_sra(t1,vec_splat_u32(1));
-            vbuf++;
-            *vtmp = vec_sub(t2,t1);
-            vtmp++;
-
-#endif
-
-        }
-
-        snow_horizontal_compose_lift_lead_out(i, temp, src, b, width, w_r, 1, -3, 0, 1);
-    }
-
-    {
-    //Interleave
-        int a;
-        vector signed int *t = (vector signed int *)temp,
-                          *v = (vector signed int *)b;
-
-        snow_interleave_line_header(&amp;i, width, b, temp);
-
-        for (; (i &amp; 0xE) != 0xE; i-=2){
-            b[i+1] = temp[i&gt;&gt;1];
-            b[i] = b[i&gt;&gt;1];
-        }
-        for (i-=14; i&gt;=0; i-=16){
-           a=i/4;
-
-           v[a+3]=vec_mergel(v[(a&gt;&gt;1)+1],t[(a&gt;&gt;1)+1]);
-           v[a+2]=vec_mergeh(v[(a&gt;&gt;1)+1],t[(a&gt;&gt;1)+1]);
-           v[a+1]=vec_mergel(v[a&gt;&gt;1],t[a&gt;&gt;1]);
-           v[a]=vec_mergeh(v[a&gt;&gt;1],t[a&gt;&gt;1]);
-
-        }
-
-    }
-}
-
-void ff_snow_vertical_compose97i_altivec(DWTELEM *b0, DWTELEM *b1, DWTELEM *b2, DWTELEM *b3, DWTELEM *b4, DWTELEM *b5, int width)
-{
-    int i, w4 = width/4;
-    vector signed int *v0, *v1,*v2,*v3,*v4,*v5;
-    vector signed int t1, t2;
-
-    v0=(vector signed int *)b0;
-    v1=(vector signed int *)b1;
-    v2=(vector signed int *)b2;
-    v3=(vector signed int *)b3;
-    v4=(vector signed int *)b4;
-    v5=(vector signed int *)b5;
-
-    for (i=0; i&lt; w4;i++)
-    {
-
-    #if 0
-        b4[i] -= (3*(b3[i] + b5[i])+4)&gt;&gt;3;
-        b3[i] -= ((b2[i] + b4[i]));
-        b2[i] += ((b1[i] + b3[i])+4*b2[i]+8)&gt;&gt;4;
-        b1[i] += (3*(b0[i] + b2[i]))&gt;&gt;1;
-    #else
-        t1 = vec_add(v3[i], v5[i]);
-        t2 = vec_add(t1, vec_add(t1,t1));
-        t1 = vec_add(t2, vec_splat_s32(4));
-        v4[i] = vec_sub(v4[i], vec_sra(t1,vec_splat_u32(3)));
-
-        v3[i] = vec_sub(v3[i], vec_add(v2[i], v4[i]));
-
-        t1 = vec_add(vec_splat_s32(8), vec_add(v1[i], v3[i]));
-        t2 = vec_sl(v2[i], vec_splat_u32(2));
-        v2[i] = vec_add(v2[i], vec_sra(vec_add(t1,t2),vec_splat_u32(4)));
-        t1 = vec_add(v0[i], v2[i]);
-        t2 = vec_add(t1, vec_add(t1,t1));
-        v1[i] = vec_add(v1[i], vec_sra(t2,vec_splat_u32(1)));
-
-    #endif
-    }
-
-    for(i*=4; i &lt; width; i++)
-    {
-        b4[i] -= (W_DM*(b3[i] + b5[i])+W_DO)&gt;&gt;W_DS;
-        b3[i] -= (W_CM*(b2[i] + b4[i])+W_CO)&gt;&gt;W_CS;
-        b2[i] += (W_BM*(b1[i] + b3[i])+4*b2[i]+W_BO)&gt;&gt;W_BS;
-        b1[i] += (W_AM*(b0[i] + b2[i])+W_AO)&gt;&gt;W_AS;
-    }
-}
-
-#define LOAD_BLOCKS \
-            tmp1 = vec_ld(0, &amp;block[3][y*src_stride]);\
-            align = vec_lvsl(0, &amp;block[3][y*src_stride]);\
-            tmp2 = vec_ld(15, &amp;block[3][y*src_stride]);\
-\
-            b3 = vec_perm(tmp1,tmp2,align);\
-\
-            tmp1 = vec_ld(0, &amp;block[2][y*src_stride]);\
-            align = vec_lvsl(0, &amp;block[2][y*src_stride]);\
-            tmp2 = vec_ld(15, &amp;block[2][y*src_stride]);\
-\
-            b2 = vec_perm(tmp1,tmp2,align);\
-\
-            tmp1 = vec_ld(0, &amp;block[1][y*src_stride]);\
-            align = vec_lvsl(0, &amp;block[1][y*src_stride]);\
-            tmp2 = vec_ld(15, &amp;block[1][y*src_stride]);\
-\
-            b1 = vec_perm(tmp1,tmp2,align);\
-\
-            tmp1 = vec_ld(0, &amp;block[0][y*src_stride]);\
-            align = vec_lvsl(0, &amp;block[0][y*src_stride]);\
-            tmp2 = vec_ld(15, &amp;block[0][y*src_stride]);\
-\
-            b0 = vec_perm(tmp1,tmp2,align);
-
-#define LOAD_OBMCS \
-            tmp1 = vec_ld(0, obmc1);\
-            align = vec_lvsl(0, obmc1);\
-            tmp2 = vec_ld(15, obmc1);\
-\
-            ob1 = vec_perm(tmp1,tmp2,align);\
-\
-            tmp1 = vec_ld(0, obmc2);\
-            align = vec_lvsl(0, obmc2);\
-            tmp2 = vec_ld(15, obmc2);\
-\
-            ob2 = vec_perm(tmp1,tmp2,align);\
-\
-            tmp1 = vec_ld(0, obmc3);\
-            align = vec_lvsl(0, obmc3);\
-            tmp2 = vec_ld(15, obmc3);\
-\
-            ob3 = vec_perm(tmp1,tmp2,align);\
-\
-            tmp1 = vec_ld(0, obmc4);\
-            align = vec_lvsl(0, obmc4);\
-            tmp2 = vec_ld(15, obmc4);\
-\
-            ob4 = vec_perm(tmp1,tmp2,align);
-
-/* interleave logic
- * h1 &lt;- [ a,b,a,b, a,b,a,b, a,b,a,b, a,b,a,b ]
- * h2 &lt;- [ c,d,c,d, c,d,c,d, c,d,c,d, c,d,c,d ]
- * h  &lt;- [ a,b,c,d, a,b,c,d, a,b,c,d, a,b,c,d ]
- */
-
-#define STEPS_0_1\
-            h1 = (vector unsigned short)\
-                 vec_mergeh(ob1, ob2);\
-\
-            h2 = (vector unsigned short)\
-                 vec_mergeh(ob3, ob4);\
-\
-            ih = (vector unsigned char)\
-                 vec_mergeh(h1,h2);\
-\
-            l1 = (vector unsigned short) vec_mergeh(b3, b2);\
-\
-            ih1 = (vector unsigned char) vec_mergel(h1, h2);\
-\
-            l2 = (vector unsigned short) vec_mergeh(b1, b0);\
-\
-            il = (vector unsigned char) vec_mergeh(l1, l2);\
-\
-            v[0] = (vector signed int) vec_msum(ih, il, vec_splat_u32(0));\
-\
-            il1 = (vector unsigned char) vec_mergel(l1, l2);\
-\
-            v[1] = (vector signed int) vec_msum(ih1, il1, vec_splat_u32(0));
-
-#define FINAL_STEP_SCALAR\
-        for(x=0; x&lt;b_w; x++)\
-            if(add){\
-                vbuf[x] += dst[x + src_x];\
-                vbuf[x] = (vbuf[x] + (1&lt;&lt;(FRAC_BITS-1))) &gt;&gt; FRAC_BITS;\
-                if(vbuf[x]&amp;(~255)) vbuf[x]= ~(vbuf[x]&gt;&gt;31);\
-                dst8[x + y*src_stride] = vbuf[x];\
-            }else{\
-                dst[x + src_x] -= vbuf[x];\
-            }
-
-static void inner_add_yblock_bw_8_obmc_16_altivec(uint8_t *obmc,
-                                             const int obmc_stride,
-                                             uint8_t * * block, int b_w,
-                                             int b_h, int src_x, int src_y,
-                                             int src_stride, slice_buffer * sb,
-                                             int add, uint8_t * dst8)
-{
-    int y, x;
-    DWTELEM * dst;
-    vector unsigned short h1, h2, l1, l2;
-    vector unsigned char ih, il, ih1, il1, tmp1, tmp2, align;
-    vector unsigned char b0,b1,b2,b3;
-    vector unsigned char ob1,ob2,ob3,ob4;
-
-    DECLARE_ALIGNED_16(int, vbuf[16]);
-    vector signed int *v = (vector signed int *)vbuf, *d;
-
-    for(y=0; y&lt;b_h; y++){
-        //FIXME ugly missue of obmc_stride
-
-        uint8_t *obmc1= obmc + y*obmc_stride;
-        uint8_t *obmc2= obmc1+ (obmc_stride&gt;&gt;1);
-        uint8_t *obmc3= obmc1+ obmc_stride*(obmc_stride&gt;&gt;1);
-        uint8_t *obmc4= obmc3+ (obmc_stride&gt;&gt;1);
-
-        dst = slice_buffer_get_line(sb, src_y + y);
-        d = (vector signed int *)(dst + src_x);
-
-//FIXME i could avoid some loads!
-
-        // load blocks
-        LOAD_BLOCKS
-
-        // load obmcs
-        LOAD_OBMCS
-
-        // steps 0 1
-        STEPS_0_1
-
-        FINAL_STEP_SCALAR
-
-       }
-
-}
-
-#define STEPS_2_3\
-            h1 = (vector unsigned short) vec_mergel(ob1, ob2);\
-\
-            h2 = (vector unsigned short) vec_mergel(ob3, ob4);\
-\
-            ih = (vector unsigned char) vec_mergeh(h1,h2);\
-\
-            l1 = (vector unsigned short) vec_mergel(b3, b2);\
-\
-            l2 = (vector unsigned short) vec_mergel(b1, b0);\
-\
-            ih1 = (vector unsigned char) vec_mergel(h1,h2);\
-\
-            il = (vector unsigned char) vec_mergeh(l1,l2);\
-\
-            v[2] = (vector signed int) vec_msum(ih, il, vec_splat_u32(0));\
-\
-            il1 = (vector unsigned char) vec_mergel(l1,l2);\
-\
-            v[3] = (vector signed int) vec_msum(ih1, il1, vec_splat_u32(0));
-
-
-static void inner_add_yblock_bw_16_obmc_32_altivec(uint8_t *obmc,
-                                             const int obmc_stride,
-                                             uint8_t * * block, int b_w,
-                                             int b_h, int src_x, int src_y,
-                                             int src_stride, slice_buffer * sb,
-                                             int add, uint8_t * dst8)
-{
-    int y, x;
-    DWTELEM * dst;
-    vector unsigned short h1, h2, l1, l2;
-    vector unsigned char ih, il, ih1, il1, tmp1, tmp2, align;
-    vector unsigned char b0,b1,b2,b3;
-    vector unsigned char ob1,ob2,ob3,ob4;
-    DECLARE_ALIGNED_16(int, vbuf[b_w]);
-    vector signed int *v = (vector signed int *)vbuf, *d;
-
-    for(y=0; y&lt;b_h; y++){
-        //FIXME ugly missue of obmc_stride
-
-        uint8_t *obmc1= obmc + y*obmc_stride;
-        uint8_t *obmc2= obmc1+ (obmc_stride&gt;&gt;1);
-        uint8_t *obmc3= obmc1+ obmc_stride*(obmc_stride&gt;&gt;1);
-        uint8_t *obmc4= obmc3+ (obmc_stride&gt;&gt;1);
-
-        dst = slice_buffer_get_line(sb, src_y + y);
-        d = (vector signed int *)(dst + src_x);
-
-        // load blocks
-        LOAD_BLOCKS
-
-        // load obmcs
-        LOAD_OBMCS
-
-        // steps 0 1 2 3
-        STEPS_0_1
-
-        STEPS_2_3
-
-        FINAL_STEP_SCALAR
-
-    }
-}
-
-#define FINAL_STEP_VEC \
-\
-    if(add)\
-        {\
-            for(x=0; x&lt;b_w/4; x++)\
-            {\
-                v[x] = vec_add(v[x], d[x]);\
-                v[x] = vec_sra(vec_add(v[x],\
-                                       vec_sl( vec_splat_s32(1),\
-                                               vec_splat_u32(7))),\
-                               vec_splat_u32(8));\
-\
-                mask = (vector bool int) vec_sl((vector signed int)\
-                        vec_cmpeq(v[x],v[x]),vec_splat_u32(8));\
-                mask = (vector bool int) vec_and(v[x],vec_nor(mask,mask));\
-\
-                mask = (vector bool int)\
-                        vec_cmpeq((vector signed int)mask,\
-                                  (vector signed int)vec_splat_u32(0));\
-\
-                vs = vec_sra(v[x],vec_splat_u32(8));\
-                vs = vec_sra(v[x],vec_splat_u32(8));\
-                vs = vec_sra(v[x],vec_splat_u32(15));\
-\
-                vs = vec_nor(vs,vs);\
-\
-                v[x]= vec_sel(v[x],vs,mask);\
-            }\
-\
-            for(x=0; x&lt;b_w; x++)\
-                dst8[x + y*src_stride] = vbuf[x];\
-\
-        }\
-         else\
-            for(x=0; x&lt;b_w/4; x++)\
-                d[x] = vec_sub(d[x], v[x]);
-
-static void inner_add_yblock_a_bw_8_obmc_16_altivec(uint8_t *obmc,
-                                             const int obmc_stride,
-                                             uint8_t * * block, int b_w,
-                                             int b_h, int src_x, int src_y,
-                                             int src_stride, slice_buffer * sb,
-                                             int add, uint8_t * dst8)
-{
-    int y, x;
-    DWTELEM * dst;
-    vector bool int mask;
-    vector signed int vs;
-    vector unsigned short h1, h2, l1, l2;
-    vector unsigned char ih, il, ih1, il1, tmp1, tmp2, align;
-    vector unsigned char b0,b1,b2,b3;
-    vector unsigned char ob1,ob2,ob3,ob4;
-
-    DECLARE_ALIGNED_16(int, vbuf[16]);
-    vector signed int *v = (vector signed int *)vbuf, *d;
-
-    for(y=0; y&lt;b_h; y++){
-        //FIXME ugly missue of obmc_stride
-
-        uint8_t *obmc1= obmc + y*obmc_stride;
-        uint8_t *obmc2= obmc1+ (obmc_stride&gt;&gt;1);
-        uint8_t *obmc3= obmc1+ obmc_stride*(obmc_stride&gt;&gt;1);
-        uint8_t *obmc4= obmc3+ (obmc_stride&gt;&gt;1);
-
-        dst = slice_buffer_get_line(sb, src_y + y);
-        d = (vector signed int *)(dst + src_x);
-
-//FIXME i could avoid some loads!
-
-        // load blocks
-        LOAD_BLOCKS
-
-        // load obmcs
-        LOAD_OBMCS
-
-        // steps 0 1
-        STEPS_0_1
-
-        FINAL_STEP_VEC
-
-       }
-
-}
-
-static void inner_add_yblock_a_bw_16_obmc_32_altivec(uint8_t *obmc,
-                                             const int obmc_stride,
-                                             uint8_t * * block, int b_w,
-                                             int b_h, int src_x, int src_y,
-                                             int src_stride, slice_buffer * sb,
-                                             int add, uint8_t * dst8)
-{
-    int y, x;
-    DWTELEM * dst;
-    vector bool int mask;
-    vector signed int vs;
-    vector unsigned short h1, h2, l1, l2;
-    vector unsigned char ih, il, ih1, il1, tmp1, tmp2, align;
-    vector unsigned char b0,b1,b2,b3;
-    vector unsigned char ob1,ob2,ob3,ob4;
-    DECLARE_ALIGNED_16(int, vbuf[b_w]);
-    vector signed int *v = (vector signed int *)vbuf, *d;
-
-    for(y=0; y&lt;b_h; y++){
-        //FIXME ugly missue of obmc_stride
-
-        uint8_t *obmc1= obmc + y*obmc_stride;
-        uint8_t *obmc2= obmc1+ (obmc_stride&gt;&gt;1);
-        uint8_t *obmc3= obmc1+ obmc_stride*(obmc_stride&gt;&gt;1);
-        uint8_t *obmc4= obmc3+ (obmc_stride&gt;&gt;1);
-
-        dst = slice_buffer_get_line(sb, src_y + y);
-        d = (vector signed int *)(dst + src_x);
-
-        // load blocks
-        LOAD_BLOCKS
-
-        // load obmcs
-        LOAD_OBMCS
-
-        // steps 0 1 2 3
-        STEPS_0_1
-
-        STEPS_2_3
-
-        FINAL_STEP_VEC
-
-    }
-}
-
-
-void ff_snow_inner_add_yblock_altivec(uint8_t *obmc, const int obmc_stride,
-                                      uint8_t * * block, int b_w, int b_h,
-                                      int src_x, int src_y, int src_stride,
-                                      slice_buffer * sb, int add,
-                                      uint8_t * dst8)
-{
-    if (src_x&amp;15) {
-        if (b_w == 16)
-            inner_add_yblock_bw_16_obmc_32_altivec(obmc, obmc_stride, block,
-                                                   b_w, b_h, src_x, src_y,
-                                                   src_stride, sb, add, dst8);
-        else if (b_w == 8)
-            inner_add_yblock_bw_8_obmc_16_altivec(obmc, obmc_stride, block,
-                                                  b_w, b_h, src_x, src_y,
-                                                  src_stride, sb, add, dst8);
-        else
-            ff_snow_inner_add_yblock(obmc, obmc_stride, block, b_w, b_h, src_x,
-                                     src_y, src_stride, sb, add, dst8);
-    } else {
-        if (b_w == 16)
-            inner_add_yblock_a_bw_16_obmc_32_altivec(obmc, obmc_stride, block,
-                                                     b_w, b_h, src_x, src_y,
-                                                     src_stride, sb, add, dst8);
-        else if (b_w == 8)
-            inner_add_yblock_a_bw_8_obmc_16_altivec(obmc, obmc_stride, block,
-                                                    b_w, b_h, src_x, src_y,
-                                                    src_stride, sb, add, dst8);
-        else
-            ff_snow_inner_add_yblock(obmc, obmc_stride, block, b_w, b_h, src_x,
-                                     src_y, src_stride, sb, add, dst8);
-    }
-}
-
-
-void snow_init_altivec(DSPContext* c, AVCodecContext *avctx)
-{
-        c-&gt;horizontal_compose97i = ff_snow_horizontal_compose97i_altivec;
-        c-&gt;vertical_compose97i = ff_snow_vertical_compose97i_altivec;
-        c-&gt;inner_add_yblock = ff_snow_inner_add_yblock_altivec;
-}
+/*
+ * Altivec optimized snow DSP utils
+ * Copyright (c) 2006 Luca Barbato &lt;<A HREF="https://lists.berlios.de/mailman/listinfo/avidemux-svn-commit">lu_zero at gentoo.org</A>&gt;
+ *
+ * This file is part of FFmpeg.
+ *
+ * FFmpeg is free software; you can redistribute it and/or
+ * modify it under the terms of the GNU Lesser General Public
+ * License as published by the Free Software Foundation; either
+ * version 2.1 of the License, or (at your option) any later version.
+ *
+ * FFmpeg is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+ * Lesser General Public License for more details.
+ *
+ * You should have received a copy of the GNU Lesser General Public
+ * License along with FFmpeg; if not, write to the Free Software
+ * Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA
+ */
+
+#include &quot;dsputil.h&quot;
+
+#include &quot;gcc_fixes.h&quot;
+#include &quot;dsputil_altivec.h&quot;
+#include &quot;snow.h&quot;
+
+#undef NDEBUG
+#include &lt;assert.h&gt;
+
+
+
+//FIXME remove this replication
+#define slice_buffer_get_line(slice_buf, line_num) ((slice_buf)-&gt;line[line_num] ? (slice_buf)-&gt;line[line_num] : slice_buffer_load_line((slice_buf), (line_num)))
+
+static DWTELEM * slice_buffer_load_line(slice_buffer * buf, int line)
+{
+    int offset;
+    DWTELEM * buffer;
+
+//  av_log(NULL, AV_LOG_DEBUG, &quot;Cache hit: %d\n&quot;, line);
+
+    assert(buf-&gt;data_stack_top &gt;= 0);
+//  assert(!buf-&gt;line[line]);
+    if (buf-&gt;line[line])
+        return buf-&gt;line[line];
+
+    offset = buf-&gt;line_width * line;
+    buffer = buf-&gt;data_stack[buf-&gt;data_stack_top];
+    buf-&gt;data_stack_top--;
+    buf-&gt;line[line] = buffer;
+
+//  av_log(NULL, AV_LOG_DEBUG, &quot;slice_buffer_load_line: line: %d remaining: %d\n&quot;, line, buf-&gt;data_stack_top + 1);
+
+    return buffer;
+}
+
+
+//altivec code
+
+void ff_snow_horizontal_compose97i_altivec(DWTELEM *b, int width)
+{
+    const int w2= (width+1)&gt;&gt;1;
+    DECLARE_ALIGNED_16(DWTELEM, temp[(width&gt;&gt;1)]);
+    const int w_l= (width&gt;&gt;1);
+    const int w_r= w2 - 1;
+    int i;
+    vector signed int t1, t2, x, y, tmp1, tmp2;
+    vector signed int *vbuf, *vtmp;
+    vector unsigned char align;
+
+
+
+    { // Lift 0
+        DWTELEM * const ref = b + w2 - 1;
+        DWTELEM b_0 = b[0];
+        vbuf = (vector signed int *)b;
+
+        tmp1 = vec_ld (0, ref);
+        align = vec_lvsl (0, ref);
+        tmp2 = vec_ld (15, ref);
+        t1= vec_perm(tmp1, tmp2, align);
+
+        i = 0;
+
+        for (i=0; i&lt;w_l-15; i+=16) {
+#if 0
+        b[i+0] = b[i+0] - ((3 * (ref[i+0] + ref[i+1]) + 4) &gt;&gt; 3);
+        b[i+1] = b[i+1] - ((3 * (ref[i+1] + ref[i+2]) + 4) &gt;&gt; 3);
+        b[i+2] = b[i+2] - ((3 * (ref[i+2] + ref[i+3]) + 4) &gt;&gt; 3);
+        b[i+3] = b[i+3] - ((3 * (ref[i+3] + ref[i+4]) + 4) &gt;&gt; 3);
+#else
+
+        tmp1 = vec_ld (0, ref+4+i);
+        tmp2 = vec_ld (15, ref+4+i);
+
+        t2 = vec_perm(tmp1, tmp2, align);
+
+        y = vec_add(t1,vec_sld(t1,t2,4));
+        y = vec_add(vec_add(y,y),y);
+
+        tmp1 = vec_ld (0, ref+8+i);
+
+        y = vec_add(y, vec_splat_s32(4));
+        y = vec_sra(y, vec_splat_u32(3));
+
+        tmp2 = vec_ld (15, ref+8+i);
+
+        *vbuf = vec_sub(*vbuf, y);
+
+        t1=t2;
+
+        vbuf++;
+
+        t2 = vec_perm(tmp1, tmp2, align);
+
+        y = vec_add(t1,vec_sld(t1,t2,4));
+        y = vec_add(vec_add(y,y),y);
+
+        tmp1 = vec_ld (0, ref+12+i);
+
+        y = vec_add(y, vec_splat_s32(4));
+        y = vec_sra(y, vec_splat_u32(3));
+
+        tmp2 = vec_ld (15, ref+12+i);
+
+        *vbuf = vec_sub(*vbuf, y);
+
+        t1=t2;
+
+        vbuf++;
+
+        t2 = vec_perm(tmp1, tmp2, align);
+
+        y = vec_add(t1,vec_sld(t1,t2,4));
+        y = vec_add(vec_add(y,y),y);
+
+        tmp1 = vec_ld (0, ref+16+i);
+
+        y = vec_add(y, vec_splat_s32(4));
+        y = vec_sra(y, vec_splat_u32(3));
+
+        tmp2 = vec_ld (15, ref+16+i);
+
+        *vbuf = vec_sub(*vbuf, y);
+
+        t1=t2;
+
+        t2 = vec_perm(tmp1, tmp2, align);
+
+        y = vec_add(t1,vec_sld(t1,t2,4));
+        y = vec_add(vec_add(y,y),y);
+
+        vbuf++;
+
+        y = vec_add(y, vec_splat_s32(4));
+        y = vec_sra(y, vec_splat_u32(3));
+        *vbuf = vec_sub(*vbuf, y);
+
+        t1=t2;
+
+        vbuf++;
+
+#endif
+        }
+
+        snow_horizontal_compose_lift_lead_out(i, b, b, ref, width, w_l, 0, W_DM, W_DO, W_DS);
+        b[0] = b_0 - ((W_DM * 2 * ref[1]+W_DO)&gt;&gt;W_DS);
+    }
+
+    { // Lift 1
+        DWTELEM * const dst = b+w2;
+
+        i = 0;
+        for(; (((long)&amp;dst[i]) &amp; 0xF) &amp;&amp; i&lt;w_r; i++){
+            dst[i] = dst[i] - (b[i] + b[i + 1]);
+        }
+
+        align = vec_lvsl(0, b+i);
+        tmp1 = vec_ld(0, b+i);
+        vbuf = (vector signed int*) (dst + i);
+        tmp2 = vec_ld(15, b+i);
+
+        t1 = vec_perm(tmp1, tmp2, align);
+
+        for (; i&lt;w_r-3; i+=4) {
+
+#if 0
+            dst[i]   = dst[i]   - (b[i]   + b[i + 1]);
+            dst[i+1] = dst[i+1] - (b[i+1] + b[i + 2]);
+            dst[i+2] = dst[i+2] - (b[i+2] + b[i + 3]);
+            dst[i+3] = dst[i+3] - (b[i+3] + b[i + 4]);
+#else
+
+        tmp1 = vec_ld(0, b+4+i);
+        tmp2 = vec_ld(15, b+4+i);
+
+        t2 = vec_perm(tmp1, tmp2, align);
+
+        y = vec_add(t1, vec_sld(t1,t2,4));
+        *vbuf = vec_sub (*vbuf, y);
+
+        vbuf++;
+
+        t1 = t2;
+
+#endif
+
+        }
+
+        snow_horizontal_compose_lift_lead_out(i, dst, dst, b, width, w_r, 1, W_CM, W_CO, W_CS);
+    }
+
+    { // Lift 2
+        DWTELEM * const ref = b+w2 - 1;
+        DWTELEM b_0 = b[0];
+        vbuf= (vector signed int *) b;
+
+        tmp1 = vec_ld (0, ref);
+        align = vec_lvsl (0, ref);
+        tmp2 = vec_ld (15, ref);
+        t1= vec_perm(tmp1, tmp2, align);
+
+        i = 0;
+        for (; i&lt;w_l-15; i+=16) {
+#if 0
+            b[i]   = b[i]   - (((8 -(ref[i]   + ref[i+1])) - (b[i]  &lt;&lt;2)) &gt;&gt; 4);
+            b[i+1] = b[i+1] - (((8 -(ref[i+1] + ref[i+2])) - (b[i+1]&lt;&lt;2)) &gt;&gt; 4);
+            b[i+2] = b[i+2] - (((8 -(ref[i+2] + ref[i+3])) - (b[i+2]&lt;&lt;2)) &gt;&gt; 4);
+            b[i+3] = b[i+3] - (((8 -(ref[i+3] + ref[i+4])) - (b[i+3]&lt;&lt;2)) &gt;&gt; 4);
+#else
+            tmp1 = vec_ld (0, ref+4+i);
+            tmp2 = vec_ld (15, ref+4+i);
+
+            t2 = vec_perm(tmp1, tmp2, align);
+
+            y = vec_add(t1,vec_sld(t1,t2,4));
+            y = vec_sub(vec_splat_s32(8),y);
+
+            tmp1 = vec_ld (0, ref+8+i);
+
+            x = vec_sl(*vbuf,vec_splat_u32(2));
+            y = vec_sra(vec_sub(y,x),vec_splat_u32(4));
+
+            tmp2 = vec_ld (15, ref+8+i);
+
+            *vbuf = vec_sub( *vbuf, y);
+
+            t1 = t2;
+
+            vbuf++;
+
+            t2 = vec_perm(tmp1, tmp2, align);
+
+            y = vec_add(t1,vec_sld(t1,t2,4));
+            y = vec_sub(vec_splat_s32(8),y);
+
+            tmp1 = vec_ld (0, ref+12+i);
+
+            x = vec_sl(*vbuf,vec_splat_u32(2));
+            y = vec_sra(vec_sub(y,x),vec_splat_u32(4));
+
+            tmp2 = vec_ld (15, ref+12+i);
+
+            *vbuf = vec_sub( *vbuf, y);
+
+            t1 = t2;
+
+            vbuf++;
+
+            t2 = vec_perm(tmp1, tmp2, align);
+
+            y = vec_add(t1,vec_sld(t1,t2,4));
+            y = vec_sub(vec_splat_s32(8),y);
+
+            tmp1 = vec_ld (0, ref+16+i);
+
+            x = vec_sl(*vbuf,vec_splat_u32(2));
+            y = vec_sra(vec_sub(y,x),vec_splat_u32(4));
+
+            tmp2 = vec_ld (15, ref+16+i);
+
+            *vbuf = vec_sub( *vbuf, y);
+
+            t1 = t2;
+
+            vbuf++;
+
+            t2 = vec_perm(tmp1, tmp2, align);
+
+            y = vec_add(t1,vec_sld(t1,t2,4));
+            y = vec_sub(vec_splat_s32(8),y);
+
+            t1 = t2;
+
+            x = vec_sl(*vbuf,vec_splat_u32(2));
+            y = vec_sra(vec_sub(y,x),vec_splat_u32(4));
+            *vbuf = vec_sub( *vbuf, y);
+
+            vbuf++;
+
+#endif
+        }
+
+        snow_horizontal_compose_liftS_lead_out(i, b, b, ref, width, w_l);
+        b[0] = b_0 - (((-2 * ref[1] + W_BO) - 4 * b_0) &gt;&gt; W_BS);
+    }
+
+    { // Lift 3
+        DWTELEM * const src = b+w2;
+
+        vbuf = (vector signed int *)b;
+        vtmp = (vector signed int *)temp;
+
+        i = 0;
+        align = vec_lvsl(0, src);
+
+        for (; i&lt;w_r-3; i+=4) {
+#if 0
+            temp[i] = src[i] - ((-3*(b[i] + b[i+1]))&gt;&gt;1);
+            temp[i+1] = src[i+1] - ((-3*(b[i+1] + b[i+2]))&gt;&gt;1);
+            temp[i+2] = src[i+2] - ((-3*(b[i+2] + b[i+3]))&gt;&gt;1);
+            temp[i+3] = src[i+3] - ((-3*(b[i+3] + b[i+4]))&gt;&gt;1);
+#else
+            tmp1 = vec_ld(0,src+i);
+            t1 = vec_add(vbuf[0],vec_sld(vbuf[0],vbuf[1],4));
+            tmp2 = vec_ld(15,src+i);
+            t1 = vec_sub(vec_splat_s32(0),t1); //bad!
+            t1 = vec_add(t1,vec_add(t1,t1));
+            t2 = vec_perm(tmp1 ,tmp2 ,align);
+            t1 = vec_sra(t1,vec_splat_u32(1));
+            vbuf++;
+            *vtmp = vec_sub(t2,t1);
+            vtmp++;
+
+#endif
+
+        }
+
+        snow_horizontal_compose_lift_lead_out(i, temp, src, b, width, w_r, 1, -3, 0, 1);
+    }
+
+    {
+    //Interleave
+        int a;
+        vector signed int *t = (vector signed int *)temp,
+                          *v = (vector signed int *)b;
+
+        snow_interleave_line_header(&amp;i, width, b, temp);
+
+        for (; (i &amp; 0xE) != 0xE; i-=2){
+            b[i+1] = temp[i&gt;&gt;1];
+            b[i] = b[i&gt;&gt;1];
+        }
+        for (i-=14; i&gt;=0; i-=16){
+           a=i/4;
+
+           v[a+3]=vec_mergel(v[(a&gt;&gt;1)+1],t[(a&gt;&gt;1)+1]);
+           v[a+2]=vec_mergeh(v[(a&gt;&gt;1)+1],t[(a&gt;&gt;1)+1]);
+           v[a+1]=vec_mergel(v[a&gt;&gt;1],t[a&gt;&gt;1]);
+           v[a]=vec_mergeh(v[a&gt;&gt;1],t[a&gt;&gt;1]);
+
+        }
+
+    }
+}
+
+void ff_snow_vertical_compose97i_altivec(DWTELEM *b0, DWTELEM *b1, DWTELEM *b2, DWTELEM *b3, DWTELEM *b4, DWTELEM *b5, int width)
+{
+    int i, w4 = width/4;
+    vector signed int *v0, *v1,*v2,*v3,*v4,*v5;
+    vector signed int t1, t2;
+
+    v0=(vector signed int *)b0;
+    v1=(vector signed int *)b1;
+    v2=(vector signed int *)b2;
+    v3=(vector signed int *)b3;
+    v4=(vector signed int *)b4;
+    v5=(vector signed int *)b5;
+
+    for (i=0; i&lt; w4;i++)
+    {
+
+    #if 0
+        b4[i] -= (3*(b3[i] + b5[i])+4)&gt;&gt;3;
+        b3[i] -= ((b2[i] + b4[i]));
+        b2[i] += ((b1[i] + b3[i])+4*b2[i]+8)&gt;&gt;4;
+        b1[i] += (3*(b0[i] + b2[i]))&gt;&gt;1;
+    #else
+        t1 = vec_add(v3[i], v5[i]);
+        t2 = vec_add(t1, vec_add(t1,t1));
+        t1 = vec_add(t2, vec_splat_s32(4));
+        v4[i] = vec_sub(v4[i], vec_sra(t1,vec_splat_u32(3)));
+
+        v3[i] = vec_sub(v3[i], vec_add(v2[i], v4[i]));
+
+        t1 = vec_add(vec_splat_s32(8), vec_add(v1[i], v3[i]));
+        t2 = vec_sl(v2[i], vec_splat_u32(2));
+        v2[i] = vec_add(v2[i], vec_sra(vec_add(t1,t2),vec_splat_u32(4)));
+        t1 = vec_add(v0[i], v2[i]);
+        t2 = vec_add(t1, vec_add(t1,t1));
+        v1[i] = vec_add(v1[i], vec_sra(t2,vec_splat_u32(1)));
+
+    #endif
+    }
+
+    for(i*=4; i &lt; width; i++)
+    {
+        b4[i] -= (W_DM*(b3[i] + b5[i])+W_DO)&gt;&gt;W_DS;
+        b3[i] -= (W_CM*(b2[i] + b4[i])+W_CO)&gt;&gt;W_CS;
+        b2[i] += (W_BM*(b1[i] + b3[i])+4*b2[i]+W_BO)&gt;&gt;W_BS;
+        b1[i] += (W_AM*(b0[i] + b2[i])+W_AO)&gt;&gt;W_AS;
+    }
+}
+
+#define LOAD_BLOCKS \
+            tmp1 = vec_ld(0, &amp;block[3][y*src_stride]);\
+            align = vec_lvsl(0, &amp;block[3][y*src_stride]);\
+            tmp2 = vec_ld(15, &amp;block[3][y*src_stride]);\
+\
+            b3 = vec_perm(tmp1,tmp2,align);\
+\
+            tmp1 = vec_ld(0, &amp;block[2][y*src_stride]);\
+            align = vec_lvsl(0, &amp;block[2][y*src_stride]);\
+            tmp2 = vec_ld(15, &amp;block[2][y*src_stride]);\
+\
+            b2 = vec_perm(tmp1,tmp2,align);\
+\
+            tmp1 = vec_ld(0, &amp;block[1][y*src_stride]);\
+            align = vec_lvsl(0, &amp;block[1][y*src_stride]);\
+            tmp2 = vec_ld(15, &amp;block[1][y*src_stride]);\
+\
+            b1 = vec_perm(tmp1,tmp2,align);\
+\
+            tmp1 = vec_ld(0, &amp;block[0][y*src_stride]);\
+            align = vec_lvsl(0, &amp;block[0][y*src_stride]);\
+            tmp2 = vec_ld(15, &amp;block[0][y*src_stride]);\
+\
+            b0 = vec_perm(tmp1,tmp2,align);
+
+#define LOAD_OBMCS \
+            tmp1 = vec_ld(0, obmc1);\
+            align = vec_lvsl(0, obmc1);\
+            tmp2 = vec_ld(15, obmc1);\
+\
+            ob1 = vec_perm(tmp1,tmp2,align);\
+\
+            tmp1 = vec_ld(0, obmc2);\
+            align = vec_lvsl(0, obmc2);\
+            tmp2 = vec_ld(15, obmc2);\
+\
+            ob2 = vec_perm(tmp1,tmp2,align);\
+\
+            tmp1 = vec_ld(0, obmc3);\
+            align = vec_lvsl(0, obmc3);\
+            tmp2 = vec_ld(15, obmc3);\
+\
+            ob3 = vec_perm(tmp1,tmp2,align);\
+\
+            tmp1 = vec_ld(0, obmc4);\
+            align = vec_lvsl(0, obmc4);\
+            tmp2 = vec_ld(15, obmc4);\
+\
+            ob4 = vec_perm(tmp1,tmp2,align);
+
+/* interleave logic
+ * h1 &lt;- [ a,b,a,b, a,b,a,b, a,b,a,b, a,b,a,b ]
+ * h2 &lt;- [ c,d,c,d, c,d,c,d, c,d,c,d, c,d,c,d ]
+ * h  &lt;- [ a,b,c,d, a,b,c,d, a,b,c,d, a,b,c,d ]
+ */
+
+#define STEPS_0_1\
+            h1 = (vector unsigned short)\
+                 vec_mergeh(ob1, ob2);\
+\
+            h2 = (vector unsigned short)\
+                 vec_mergeh(ob3, ob4);\
+\
+            ih = (vector unsigned char)\
+                 vec_mergeh(h1,h2);\
+\
+            l1 = (vector unsigned short) vec_mergeh(b3, b2);\
+\
+            ih1 = (vector unsigned char) vec_mergel(h1, h2);\
+\
+            l2 = (vector unsigned short) vec_mergeh(b1, b0);\
+\
+            il = (vector unsigned char) vec_mergeh(l1, l2);\
+\
+            v[0] = (vector signed int) vec_msum(ih, il, vec_splat_u32(0));\
+\
+            il1 = (vector unsigned char) vec_mergel(l1, l2);\
+\
+            v[1] = (vector signed int) vec_msum(ih1, il1, vec_splat_u32(0));
+
+#define FINAL_STEP_SCALAR\
+        for(x=0; x&lt;b_w; x++)\
+            if(add){\
+                vbuf[x] += dst[x + src_x];\
+                vbuf[x] = (vbuf[x] + (1&lt;&lt;(FRAC_BITS-1))) &gt;&gt; FRAC_BITS;\
+                if(vbuf[x]&amp;(~255)) vbuf[x]= ~(vbuf[x]&gt;&gt;31);\
+                dst8[x + y*src_stride] = vbuf[x];\
+            }else{\
+                dst[x + src_x] -= vbuf[x];\
+            }
+
+static void inner_add_yblock_bw_8_obmc_16_altivec(uint8_t *obmc,
+                                             const int obmc_stride,
+                                             uint8_t * * block, int b_w,
+                                             int b_h, int src_x, int src_y,
+                                             int src_stride, slice_buffer * sb,
+                                             int add, uint8_t * dst8)
+{
+    int y, x;
+    DWTELEM * dst;
+    vector unsigned short h1, h2, l1, l2;
+    vector unsigned char ih, il, ih1, il1, tmp1, tmp2, align;
+    vector unsigned char b0,b1,b2,b3;
+    vector unsigned char ob1,ob2,ob3,ob4;
+
+    DECLARE_ALIGNED_16(int, vbuf[16]);
+    vector signed int *v = (vector signed int *)vbuf, *d;
+
+    for(y=0; y&lt;b_h; y++){
+        //FIXME ugly missue of obmc_stride
+
+        uint8_t *obmc1= obmc + y*obmc_stride;
+        uint8_t *obmc2= obmc1+ (obmc_stride&gt;&gt;1);
+        uint8_t *obmc3= obmc1+ obmc_stride*(obmc_stride&gt;&gt;1);
+        uint8_t *obmc4= obmc3+ (obmc_stride&gt;&gt;1);
+
+        dst = slice_buffer_get_line(sb, src_y + y);
+        d = (vector signed int *)(dst + src_x);
+
+//FIXME i could avoid some loads!
+
+        // load blocks
+        LOAD_BLOCKS
+
+        // load obmcs
+        LOAD_OBMCS
+
+        // steps 0 1
+        STEPS_0_1
+
+        FINAL_STEP_SCALAR
+
+       }
+
+}
+
+#define STEPS_2_3\
+            h1 = (vector unsigned short) vec_mergel(ob1, ob2);\
+\
+            h2 = (vector unsigned short) vec_mergel(ob3, ob4);\
+\
+            ih = (vector unsigned char) vec_mergeh(h1,h2);\
+\
+            l1 = (vector unsigned short) vec_mergel(b3, b2);\
+\
+            l2 = (vector unsigned short) vec_mergel(b1, b0);\
+\
+            ih1 = (vector unsigned char) vec_mergel(h1,h2);\
+\
+            il = (vector unsigned char) vec_mergeh(l1,l2);\
+\
+            v[2] = (vector signed int) vec_msum(ih, il, vec_splat_u32(0));\
+\
+            il1 = (vector unsigned char) vec_mergel(l1,l2);\
+\
+            v[3] = (vector signed int) vec_msum(ih1, il1, vec_splat_u32(0));
+
+
+static void inner_add_yblock_bw_16_obmc_32_altivec(uint8_t *obmc,
+                                             const int obmc_stride,
+                                             uint8_t * * block, int b_w,
+                                             int b_h, int src_x, int src_y,
+                                             int src_stride, slice_buffer * sb,
+                                             int add, uint8_t * dst8)
+{
+    int y, x;
+    DWTELEM * dst;
+    vector unsigned short h1, h2, l1, l2;
+    vector unsigned char ih, il, ih1, il1, tmp1, tmp2, align;
+    vector unsigned char b0,b1,b2,b3;
+    vector unsigned char ob1,ob2,ob3,ob4;
+    DECLARE_ALIGNED_16(int, vbuf[b_w]);
+    vector signed int *v = (vector signed int *)vbuf, *d;
+
+    for(y=0; y&lt;b_h; y++){
+        //FIXME ugly missue of obmc_stride
+
+        uint8_t *obmc1= obmc + y*obmc_stride;
+        uint8_t *obmc2= obmc1+ (obmc_stride&gt;&gt;1);
+        uint8_t *obmc3= obmc1+ obmc_stride*(obmc_stride&gt;&gt;1);
+        uint8_t *obmc4= obmc3+ (obmc_stride&gt;&gt;1);
+
+        dst = slice_buffer_get_line(sb, src_y + y);
+        d = (vector signed int *)(dst + src_x);
+
+        // load blocks
+        LOAD_BLOCKS
+
+        // load obmcs
+        LOAD_OBMCS
+
+        // steps 0 1 2 3
+        STEPS_0_1
+
+        STEPS_2_3
+
+        FINAL_STEP_SCALAR
+
+    }
+}
+
+#define FINAL_STEP_VEC \
+\
+    if(add)\
+        {\
+            for(x=0; x&lt;b_w/4; x++)\
+            {\
+                v[x] = vec_add(v[x], d[x]);\
+                v[x] = vec_sra(vec_add(v[x],\
+                                       vec_sl( vec_splat_s32(1),\
+                                               vec_splat_u32(7))),\
+                               vec_splat_u32(8));\
+\
+                mask = (vector bool int) vec_sl((vector signed int)\
+                        vec_cmpeq(v[x],v[x]),vec_splat_u32(8));\
+                mask = (vector bool int) vec_and(v[x],vec_nor(mask,mask));\
+\
+                mask = (vector bool int)\
+                        vec_cmpeq((vector signed int)mask,\
+                                  (vector signed int)vec_splat_u32(0));\
+\
+                vs = vec_sra(v[x],vec_splat_u32(8));\
+                vs = vec_sra(v[x],vec_splat_u32(8));\
+                vs = vec_sra(v[x],vec_splat_u32(15));\
+\
+                vs = vec_nor(vs,vs);\
+\
+                v[x]= vec_sel(v[x],vs,mask);\
+            }\
+\
+            for(x=0; x&lt;b_w; x++)\
+                dst8[x + y*src_stride] = vbuf[x];\
+\
+        }\
+         else\
+            for(x=0; x&lt;b_w/4; x++)\
+                d[x] = vec_sub(d[x], v[x]);
+
+static void inner_add_yblock_a_bw_8_obmc_16_altivec(uint8_t *obmc,
+                                             const int obmc_stride,
+                                             uint8_t * * block, int b_w,
+                                             int b_h, int src_x, int src_y,
+                                             int src_stride, slice_buffer * sb,
+                                             int add, uint8_t * dst8)
+{
+    int y, x;
+    DWTELEM * dst;
+    vector bool int mask;
+    vector signed int vs;
+    vector unsigned short h1, h2, l1, l2;
+    vector unsigned char ih, il, ih1, il1, tmp1, tmp2, align;
+    vector unsigned char b0,b1,b2,b3;
+    vector unsigned char ob1,ob2,ob3,ob4;
+
+    DECLARE_ALIGNED_16(int, vbuf[16]);
+    vector signed int *v = (vector signed int *)vbuf, *d;
+
+    for(y=0; y&lt;b_h; y++){
+        //FIXME ugly missue of obmc_stride
+
+        uint8_t *obmc1= obmc + y*obmc_stride;
+        uint8_t *obmc2= obmc1+ (obmc_stride&gt;&gt;1);
+        uint8_t *obmc3= obmc1+ obmc_stride*(obmc_stride&gt;&gt;1);
+        uint8_t *obmc4= obmc3+ (obmc_stride&gt;&gt;1);
+
+        dst = slice_buffer_get_line(sb, src_y + y);
+        d = (vector signed int *)(dst + src_x);
+
+//FIXME i could avoid some loads!
+
+        // load blocks
+        LOAD_BLOCKS
+
+        // load obmcs
+        LOAD_OBMCS
+
+        // steps 0 1
+        STEPS_0_1
+
+        FINAL_STEP_VEC
+
+       }
+
+}
+
+static void inner_add_yblock_a_bw_16_obmc_32_altivec(uint8_t *obmc,
+                                             const int obmc_stride,
+                                             uint8_t * * block, int b_w,
+                                             int b_h, int src_x, int src_y,
+                                             int src_stride, slice_buffer * sb,
+                                             int add, uint8_t * dst8)
+{
+    int y, x;
+    DWTELEM * dst;
+    vector bool int mask;
+    vector signed int vs;
+    vector unsigned short h1, h2, l1, l2;
+    vector unsigned char ih, il, ih1, il1, tmp1, tmp2, align;
+    vector unsigned char b0,b1,b2,b3;
+    vector unsigned char ob1,ob2,ob3,ob4;
+    DECLARE_ALIGNED_16(int, vbuf[b_w]);
+    vector signed int *v = (vector signed int *)vbuf, *d;
+
+    for(y=0; y&lt;b_h; y++){
+        //FIXME ugly missue of obmc_stride
+
+        uint8_t *obmc1= obmc + y*obmc_stride;
+        uint8_t *obmc2= obmc1+ (obmc_stride&gt;&gt;1);
+        uint8_t *obmc3= obmc1+ obmc_stride*(obmc_stride&gt;&gt;1);
+        uint8_t *obmc4= obmc3+ (obmc_stride&gt;&gt;1);
+
+        dst = slice_buffer_get_line(sb, src_y + y);
+        d = (vector signed int *)(dst + src_x);
+
+        // load blocks
+        LOAD_BLOCKS
+
+        // load obmcs
+        LOAD_OBMCS
+
+        // steps 0 1 2 3
+        STEPS_0_1
+
+        STEPS_2_3
+
+        FINAL_STEP_VEC
+
+    }
+}
+
+
+void ff_snow_inner_add_yblock_altivec(uint8_t *obmc, const int obmc_stride,
+                                      uint8_t * * block, int b_w, int b_h,
+                                      int src_x, int src_y, int src_stride,
+                                      slice_buffer * sb, int add,
+                                      uint8_t * dst8)
+{
+    if (src_x&amp;15) {
+        if (b_w == 16)
+            inner_add_yblock_bw_16_obmc_32_altivec(obmc, obmc_stride, block,
+                                                   b_w, b_h, src_x, src_y,
+                                                   src_stride, sb, add, dst8);
+        else if (b_w == 8)
+            inner_add_yblock_bw_8_obmc_16_altivec(obmc, obmc_stride, block,
+                                                  b_w, b_h, src_x, src_y,
+                                                  src_stride, sb, add, dst8);
+        else
+            ff_snow_inner_add_yblock(obmc, obmc_stride, block, b_w, b_h, src_x,
+                                     src_y, src_stride, sb, add, dst8);
+    } else {
+        if (b_w == 16)
+            inner_add_yblock_a_bw_16_obmc_32_altivec(obmc, obmc_stride, block,
+                                                     b_w, b_h, src_x, src_y,
+                                                     src_stride, sb, add, dst8);
+        else if (b_w == 8)
+            inner_add_yblock_a_bw_8_obmc_16_altivec(obmc, obmc_stride, block,
+                                                    b_w, b_h, src_x, src_y,
+                                                    src_stride, sb, add, dst8);
+        else
+            ff_snow_inner_add_yblock(obmc, obmc_stride, block, b_w, b_h, src_x,
+                                     src_y, src_stride, sb, add, dst8);
+    }
+}
+
+
+void snow_init_altivec(DSPContext* c, AVCodecContext *avctx)
+{
+        c-&gt;horizontal_compose97i = ff_snow_horizontal_compose97i_altivec;
+        c-&gt;vertical_compose97i = ff_snow_vertical_compose97i_altivec;
+        c-&gt;inner_add_yblock = ff_snow_inner_add_yblock_altivec;
+}

Added: branches/avidemux_2.4_branch/avidemux/ADM_libraries/ADM_lavcodec/ppc/types_altivec.h
===================================================================
--- branches/avidemux_2.4_branch/avidemux/ADM_libraries/ADM_lavcodec/ppc/types_altivec.h	2007-12-03 12:38:06 UTC (rev 3719)
+++ branches/avidemux_2.4_branch/avidemux/ADM_libraries/ADM_lavcodec/ppc/types_altivec.h	2007-12-04 00:19:48 UTC (rev 3720)
@@ -0,0 +1,46 @@
+/*
+ * Copyright (c) 2006 Guillaume Poirier &lt;<A HREF="https://lists.berlios.de/mailman/listinfo/avidemux-svn-commit">gpoirier at mplayerhq.hu</A>&gt;
+ *
+ * This file is part of FFmpeg.
+ *
+ * FFmpeg is free software; you can redistribute it and/or
+ * modify it under the terms of the GNU Lesser General Public
+ * License as published by the Free Software Foundation; either
+ * version 2.1 of the License, or (at your option) any later version.
+ *
+ * FFmpeg is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+ * Lesser General Public License for more details.
+ *
+ * You should have received a copy of the GNU Lesser General Public
+ * License along with FFmpeg; if not, write to the Free Software
+ * Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA
+ */
+
+#ifndef AVCODEC_TYPES_ALTIVEC_H
+#define AVCODEC_TYPES_ALTIVEC_H
+
+/***********************************************************************
+ * Vector types
+ **********************************************************************/
+#define vec_u8_t  vector unsigned char
+#define vec_s8_t  vector signed char
+#define vec_u16_t vector unsigned short
+#define vec_s16_t vector signed short
+#define vec_u32_t vector unsigned int
+#define vec_s32_t vector signed int
+
+/***********************************************************************
+ * Null vector
+ **********************************************************************/
+#define LOAD_ZERO const vec_u8_t zerov = vec_splat_u8( 0 )
+
+#define zero_u8v  (vec_u8_t)  zerov
+#define zero_s8v  (vec_s8_t)  zerov
+#define zero_u16v (vec_u16_t) zerov
+#define zero_s16v (vec_s16_t) zerov
+#define zero_u32v (vec_u32_t) zerov
+#define zero_s32v (vec_s32_t) zerov
+
+#endif // AVCODEC_TYPES_ALTIVEC_H

Added: branches/avidemux_2.4_branch/avidemux/ADM_libraries/ADM_lavcodec/ppc/vc1dsp_altivec.c
===================================================================
--- branches/avidemux_2.4_branch/avidemux/ADM_libraries/ADM_lavcodec/ppc/vc1dsp_altivec.c	2007-12-03 12:38:06 UTC (rev 3719)
+++ branches/avidemux_2.4_branch/avidemux/ADM_libraries/ADM_lavcodec/ppc/vc1dsp_altivec.c	2007-12-04 00:19:48 UTC (rev 3720)
@@ -0,0 +1,337 @@
+/*
+ * VC-1 and WMV3 decoder - DSP functions AltiVec-optimized
+ * Copyright (c) 2006 Konstantin Shishkov
+ *
+ * This file is part of FFmpeg.
+ *
+ * FFmpeg is free software; you can redistribute it and/or
+ * modify it under the terms of the GNU Lesser General Public
+ * License as published by the Free Software Foundation; either
+ * version 2.1 of the License, or (at your option) any later version.
+ *
+ * FFmpeg is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+ * Lesser General Public License for more details.
+ *
+ * You should have received a copy of the GNU Lesser General Public
+ * License along with FFmpeg; if not, write to the Free Software
+ * Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA
+ */
+
+#include &quot;dsputil.h&quot;
+
+#include &quot;gcc_fixes.h&quot;
+
+#include &quot;dsputil_altivec.h&quot;
+
+// main steps of 8x8 transform
+#define STEP8(s0, s1, s2, s3, s4, s5, s6, s7, vec_rnd) \
+do { \
+    t0 = vec_sl(vec_add(s0, s4), vec_2); \
+    t0 = vec_add(vec_sl(t0, vec_1), t0); \
+    t0 = vec_add(t0, vec_rnd); \
+    t1 = vec_sl(vec_sub(s0, s4), vec_2); \
+    t1 = vec_add(vec_sl(t1, vec_1), t1); \
+    t1 = vec_add(t1, vec_rnd); \
+    t2 = vec_add(vec_sl(s6, vec_2), vec_sl(s6, vec_1)); \
+    t2 = vec_add(t2, vec_sl(s2, vec_4)); \
+    t3 = vec_add(vec_sl(s2, vec_2), vec_sl(s2, vec_1)); \
+    t3 = vec_sub(t3, vec_sl(s6, vec_4)); \
+    t4 = vec_add(t0, t2); \
+    t5 = vec_add(t1, t3); \
+    t6 = vec_sub(t1, t3); \
+    t7 = vec_sub(t0, t2); \
+\
+    t0 = vec_sl(vec_add(s1, s3), vec_4); \
+    t0 = vec_add(t0, vec_sl(s5, vec_3)); \
+    t0 = vec_add(t0, vec_sl(s7, vec_2)); \
+    t0 = vec_add(t0, vec_sub(s5, s3)); \
+\
+    t1 = vec_sl(vec_sub(s1, s5), vec_4); \
+    t1 = vec_sub(t1, vec_sl(s7, vec_3)); \
+    t1 = vec_sub(t1, vec_sl(s3, vec_2)); \
+    t1 = vec_sub(t1, vec_add(s1, s7)); \
+\
+    t2 = vec_sl(vec_sub(s7, s3), vec_4); \
+    t2 = vec_add(t2, vec_sl(s1, vec_3)); \
+    t2 = vec_add(t2, vec_sl(s5, vec_2)); \
+    t2 = vec_add(t2, vec_sub(s1, s7)); \
+\
+    t3 = vec_sl(vec_sub(s5, s7), vec_4); \
+    t3 = vec_sub(t3, vec_sl(s3, vec_3)); \
+    t3 = vec_add(t3, vec_sl(s1, vec_2)); \
+    t3 = vec_sub(t3, vec_add(s3, s5)); \
+\
+    s0 = vec_add(t4, t0); \
+    s1 = vec_add(t5, t1); \
+    s2 = vec_add(t6, t2); \
+    s3 = vec_add(t7, t3); \
+    s4 = vec_sub(t7, t3); \
+    s5 = vec_sub(t6, t2); \
+    s6 = vec_sub(t5, t1); \
+    s7 = vec_sub(t4, t0); \
+}while(0)
+
+#define SHIFT_HOR8(s0, s1, s2, s3, s4, s5, s6, s7) \
+do { \
+    s0 = vec_sra(s0, vec_3); \
+    s1 = vec_sra(s1, vec_3); \
+    s2 = vec_sra(s2, vec_3); \
+    s3 = vec_sra(s3, vec_3); \
+    s4 = vec_sra(s4, vec_3); \
+    s5 = vec_sra(s5, vec_3); \
+    s6 = vec_sra(s6, vec_3); \
+    s7 = vec_sra(s7, vec_3); \
+}while(0)
+
+#define SHIFT_VERT8(s0, s1, s2, s3, s4, s5, s6, s7) \
+do { \
+    s0 = vec_sra(s0, vec_7); \
+    s1 = vec_sra(s1, vec_7); \
+    s2 = vec_sra(s2, vec_7); \
+    s3 = vec_sra(s3, vec_7); \
+    s4 = vec_sra(vec_add(s4, vec_1s), vec_7); \
+    s5 = vec_sra(vec_add(s5, vec_1s), vec_7); \
+    s6 = vec_sra(vec_add(s6, vec_1s), vec_7); \
+    s7 = vec_sra(vec_add(s7, vec_1s), vec_7); \
+}while(0)
+
+/* main steps of 4x4 transform */
+#define STEP4(s0, s1, s2, s3, vec_rnd) \
+do { \
+    t1 = vec_add(vec_sl(s0, vec_4), s0); \
+    t1 = vec_add(t1, vec_rnd); \
+    t2 = vec_add(vec_sl(s2, vec_4), s2); \
+    t0 = vec_add(t1, t2); \
+    t1 = vec_sub(t1, t2); \
+    t3 = vec_sl(vec_sub(s3, s1), vec_1); \
+    t3 = vec_add(t3, vec_sl(t3, vec_2)); \
+    t2 = vec_add(t3, vec_sl(s1, vec_5)); \
+    t3 = vec_add(t3, vec_sl(s3, vec_3)); \
+    t3 = vec_add(t3, vec_sl(s3, vec_2)); \
+    s0 = vec_add(t0, t2); \
+    s1 = vec_sub(t1, t3); \
+    s2 = vec_add(t1, t3); \
+    s3 = vec_sub(t0, t2); \
+}while (0)
+
+#define SHIFT_HOR4(s0, s1, s2, s3) \
+    s0 = vec_sra(s0, vec_3); \
+    s1 = vec_sra(s1, vec_3); \
+    s2 = vec_sra(s2, vec_3); \
+    s3 = vec_sra(s3, vec_3);
+
+#define SHIFT_VERT4(s0, s1, s2, s3) \
+    s0 = vec_sra(s0, vec_7); \
+    s1 = vec_sra(s1, vec_7); \
+    s2 = vec_sra(s2, vec_7); \
+    s3 = vec_sra(s3, vec_7);
+
+/** Do inverse transform on 8x8 block
+*/
+static void vc1_inv_trans_8x8_altivec(DCTELEM block[64])
+{
+    vector signed short src0, src1, src2, src3, src4, src5, src6, src7;
+    vector signed int s0, s1, s2, s3, s4, s5, s6, s7;
+    vector signed int s8, s9, sA, sB, sC, sD, sE, sF;
+    vector signed int t0, t1, t2, t3, t4, t5, t6, t7;
+    const vector signed int vec_64 = vec_sl(vec_splat_s32(4), vec_splat_u32(4));
+    const vector unsigned int vec_7 = vec_splat_u32(7);
+    const vector unsigned int vec_5 = vec_splat_u32(5);
+    const vector unsigned int vec_4 = vec_splat_u32(4);
+    const vector  signed int vec_4s = vec_splat_s32(4);
+    const vector unsigned int vec_3 = vec_splat_u32(3);
+    const vector unsigned int vec_2 = vec_splat_u32(2);
+    const vector  signed int vec_1s = vec_splat_s32(1);
+    const vector unsigned int vec_1 = vec_splat_u32(1);
+
+
+    src0 = vec_ld(  0, block);
+    src1 = vec_ld( 16, block);
+    src2 = vec_ld( 32, block);
+    src3 = vec_ld( 48, block);
+    src4 = vec_ld( 64, block);
+    src5 = vec_ld( 80, block);
+    src6 = vec_ld( 96, block);
+    src7 = vec_ld(112, block);
+
+    TRANSPOSE8(src0, src1, src2, src3, src4, src5, src6, src7);
+    s0 = vec_unpackl(src0);
+    s1 = vec_unpackl(src1);
+    s2 = vec_unpackl(src2);
+    s3 = vec_unpackl(src3);
+    s4 = vec_unpackl(src4);
+    s5 = vec_unpackl(src5);
+    s6 = vec_unpackl(src6);
+    s7 = vec_unpackl(src7);
+    s8 = vec_unpackh(src0);
+    s9 = vec_unpackh(src1);
+    sA = vec_unpackh(src2);
+    sB = vec_unpackh(src3);
+    sC = vec_unpackh(src4);
+    sD = vec_unpackh(src5);
+    sE = vec_unpackh(src6);
+    sF = vec_unpackh(src7);
+    STEP8(s0, s1, s2, s3, s4, s5, s6, s7, vec_4s);
+    SHIFT_HOR8(s0, s1, s2, s3, s4, s5, s6, s7);
+    STEP8(s8, s9, sA, sB, sC, sD, sE, sF, vec_4s);
+    SHIFT_HOR8(s8, s9, sA, sB, sC, sD, sE, sF);
+    src0 = vec_pack(s8, s0);
+    src1 = vec_pack(s9, s1);
+    src2 = vec_pack(sA, s2);
+    src3 = vec_pack(sB, s3);
+    src4 = vec_pack(sC, s4);
+    src5 = vec_pack(sD, s5);
+    src6 = vec_pack(sE, s6);
+    src7 = vec_pack(sF, s7);
+    TRANSPOSE8(src0, src1, src2, src3, src4, src5, src6, src7);
+
+    s0 = vec_unpackl(src0);
+    s1 = vec_unpackl(src1);
+    s2 = vec_unpackl(src2);
+    s3 = vec_unpackl(src3);
+    s4 = vec_unpackl(src4);
+    s5 = vec_unpackl(src5);
+    s6 = vec_unpackl(src6);
+    s7 = vec_unpackl(src7);
+    s8 = vec_unpackh(src0);
+    s9 = vec_unpackh(src1);
+    sA = vec_unpackh(src2);
+    sB = vec_unpackh(src3);
+    sC = vec_unpackh(src4);
+    sD = vec_unpackh(src5);
+    sE = vec_unpackh(src6);
+    sF = vec_unpackh(src7);
+    STEP8(s0, s1, s2, s3, s4, s5, s6, s7, vec_64);
+    SHIFT_VERT8(s0, s1, s2, s3, s4, s5, s6, s7);
+    STEP8(s8, s9, sA, sB, sC, sD, sE, sF, vec_64);
+    SHIFT_VERT8(s8, s9, sA, sB, sC, sD, sE, sF);
+    src0 = vec_pack(s8, s0);
+    src1 = vec_pack(s9, s1);
+    src2 = vec_pack(sA, s2);
+    src3 = vec_pack(sB, s3);
+    src4 = vec_pack(sC, s4);
+    src5 = vec_pack(sD, s5);
+    src6 = vec_pack(sE, s6);
+    src7 = vec_pack(sF, s7);
+
+    vec_st(src0,  0, block);
+    vec_st(src1, 16, block);
+    vec_st(src2, 32, block);
+    vec_st(src3, 48, block);
+    vec_st(src4, 64, block);
+    vec_st(src5, 80, block);
+    vec_st(src6, 96, block);
+    vec_st(src7,112, block);
+}
+
+/** Do inverse transform on 8x4 part of block
+*/
+static void vc1_inv_trans_8x4_altivec(DCTELEM block[64], int n)
+{
+    vector signed short src0, src1, src2, src3, src4, src5, src6, src7;
+    vector signed int s0, s1, s2, s3, s4, s5, s6, s7;
+    vector signed int s8, s9, sA, sB, sC, sD, sE, sF;
+    vector signed int t0, t1, t2, t3, t4, t5, t6, t7;
+    const vector signed int vec_64 = vec_sl(vec_splat_s32(4), vec_splat_u32(4));
+    const vector unsigned int vec_7 = vec_splat_u32(7);
+    const vector unsigned int vec_5 = vec_splat_u32(5);
+    const vector unsigned int vec_4 = vec_splat_u32(4);
+    const vector  signed int vec_4s = vec_splat_s32(4);
+    const vector unsigned int vec_3 = vec_splat_u32(3);
+    const vector unsigned int vec_2 = vec_splat_u32(2);
+    const vector unsigned int vec_1 = vec_splat_u32(1);
+
+    src0 = vec_ld(  0, block);
+    src1 = vec_ld( 16, block);
+    src2 = vec_ld( 32, block);
+    src3 = vec_ld( 48, block);
+    src4 = vec_ld( 64, block);
+    src5 = vec_ld( 80, block);
+    src6 = vec_ld( 96, block);
+    src7 = vec_ld(112, block);
+
+    TRANSPOSE8(src0, src1, src2, src3, src4, src5, src6, src7);
+    s0 = vec_unpackl(src0);
+    s1 = vec_unpackl(src1);
+    s2 = vec_unpackl(src2);
+    s3 = vec_unpackl(src3);
+    s4 = vec_unpackl(src4);
+    s5 = vec_unpackl(src5);
+    s6 = vec_unpackl(src6);
+    s7 = vec_unpackl(src7);
+    s8 = vec_unpackh(src0);
+    s9 = vec_unpackh(src1);
+    sA = vec_unpackh(src2);
+    sB = vec_unpackh(src3);
+    sC = vec_unpackh(src4);
+    sD = vec_unpackh(src5);
+    sE = vec_unpackh(src6);
+    sF = vec_unpackh(src7);
+    STEP8(s0, s1, s2, s3, s4, s5, s6, s7, vec_4s);
+    SHIFT_HOR8(s0, s1, s2, s3, s4, s5, s6, s7);
+    STEP8(s8, s9, sA, sB, sC, sD, sE, sF, vec_4s);
+    SHIFT_HOR8(s8, s9, sA, sB, sC, sD, sE, sF);
+    src0 = vec_pack(s8, s0);
+    src1 = vec_pack(s9, s1);
+    src2 = vec_pack(sA, s2);
+    src3 = vec_pack(sB, s3);
+    src4 = vec_pack(sC, s4);
+    src5 = vec_pack(sD, s5);
+    src6 = vec_pack(sE, s6);
+    src7 = vec_pack(sF, s7);
+    TRANSPOSE8(src0, src1, src2, src3, src4, src5, src6, src7);
+
+    if(!n){ // upper half of block
+        s0 = vec_unpackh(src0);
+        s1 = vec_unpackh(src1);
+        s2 = vec_unpackh(src2);
+        s3 = vec_unpackh(src3);
+        s8 = vec_unpackl(src0);
+        s9 = vec_unpackl(src1);
+        sA = vec_unpackl(src2);
+        sB = vec_unpackl(src3);
+        STEP4(s0, s1, s2, s3, vec_64);
+        SHIFT_VERT4(s0, s1, s2, s3);
+        STEP4(s8, s9, sA, sB, vec_64);
+        SHIFT_VERT4(s8, s9, sA, sB);
+        src0 = vec_pack(s0, s8);
+        src1 = vec_pack(s1, s9);
+        src2 = vec_pack(s2, sA);
+        src3 = vec_pack(s3, sB);
+
+        vec_st(src0,  0, block);
+        vec_st(src1, 16, block);
+        vec_st(src2, 32, block);
+        vec_st(src3, 48, block);
+    } else { //lower half of block
+        s0 = vec_unpackh(src4);
+        s1 = vec_unpackh(src5);
+        s2 = vec_unpackh(src6);
+        s3 = vec_unpackh(src7);
+        s8 = vec_unpackl(src4);
+        s9 = vec_unpackl(src5);
+        sA = vec_unpackl(src6);
+        sB = vec_unpackl(src7);
+        STEP4(s0, s1, s2, s3, vec_64);
+        SHIFT_VERT4(s0, s1, s2, s3);
+        STEP4(s8, s9, sA, sB, vec_64);
+        SHIFT_VERT4(s8, s9, sA, sB);
+        src4 = vec_pack(s0, s8);
+        src5 = vec_pack(s1, s9);
+        src6 = vec_pack(s2, sA);
+        src7 = vec_pack(s3, sB);
+
+        vec_st(src4, 64, block);
+        vec_st(src5, 80, block);
+        vec_st(src6, 96, block);
+        vec_st(src7,112, block);
+    }
+}
+
+
+void vc1dsp_init_altivec(DSPContext* dsp, AVCodecContext *avctx) {
+    dsp-&gt;vc1_inv_trans_8x8 = vc1_inv_trans_8x8_altivec;
+    dsp-&gt;vc1_inv_trans_8x4 = vc1_inv_trans_8x4_altivec;
+}

Modified: branches/avidemux_2.4_branch/avidemux/ADM_libraries/ADM_libmpeg2enc/CMakeLists.txt
===================================================================
--- branches/avidemux_2.4_branch/avidemux/ADM_libraries/ADM_libmpeg2enc/CMakeLists.txt	2007-12-03 12:38:06 UTC (rev 3719)
+++ branches/avidemux_2.4_branch/avidemux/ADM_libraries/ADM_libmpeg2enc/CMakeLists.txt	2007-12-04 00:19:48 UTC (rev 3720)
@@ -14,6 +14,18 @@
 fdct_mmx.cc           ioio.c                mblock_sumsq_mmx.cc         mpegconsts.cc    puthdr.cc         quant_mmx2.cc    writepic.cc
 fdctref.cc            macroblock.cc         motion.cc                   predcomp_mmx.cc  putmpg.cc         ratectl.cc       ADM_mpe2enc.cpp
 )
+
+IF (BUILD_ALTIVEC)
+	SET(${ADM_LIB}_SRCS ${${ADM_LIB}_SRCS}
+		altivec/add_pred.c   altivec/amber.c  altivec/benchmark.c  altivec/bsad.c  altivec/bsumsq.c  altivec/bsumsq_sub22.c  altivec/build_sub22_mests.c
+		altivec/build_sub44_mests.c  altivec/detect.c  altivec/fdct.c  altivec/fdct_idct.c  altivec/field_dct_best.c  altivec/find_best_one_pel.c
+		altivec/idct.c  altivec/iquant_intra.c  altivec/iquant_non_intra.c  altivec/motion.c  altivec/pred_comp.c  altivec/quant_non_intra.c
+		altivec/quant_weight_coeff_sum.c  altivec/quantize.c  altivec/sad_00.c  altivec/sad_01.c  altivec/sad_10.c  altivec/sad_11.c
+		altivec/sub_mean_reduction.c  altivec/sub_mean_reduction_ppc.S  altivec/sub_pred.c  altivec/subsample_image.c  altivec/sumsq.c
+		altivec/sumsq_sub22.c  altivec/variance.c
+	)
+ENDIF (BUILD_ALTIVEC)
+
 ADD_LIBRARY(${ADM_LIB} STATIC ${${ADM_LIB}_SRCS})
 ADD_ADM_LIB(${ADM_LIB} ADM_libraries)
 

Modified: branches/avidemux_2.4_branch/config.h.cmake
===================================================================
--- branches/avidemux_2.4_branch/config.h.cmake	2007-12-03 12:38:06 UTC (rev 3719)
+++ branches/avidemux_2.4_branch/config.h.cmake	2007-12-04 00:19:48 UTC (rev 3720)
@@ -1,276 +1,277 @@
-/* config.h.in.  Generated from configure.in by autoheader.  */
-
-/* Jog Shuttle */
-#cmakedefine USE_JOG
-
-/* MPEG2DEC */
-#cmakedefine ACCEL_DETECT
-
-/* Big endian CPU - SPARC or PowerPC */
-#cmakedefine ADM_BIG_ENDIAN
-
-#define PACKAGE   &quot;avidemux&quot;
-#define ADMLOCALE &quot;${ADM_LOCALE}&quot;
-
-/* BSD OS specific ifdef */
-#cmakedefine ADM_BSD_FAMILY
-
-#cmakedefine HAVE_AUDIO
-
-/* Sparc workstations */
-#cmakedefine ADM_SPARC
-
-/* Build for Windows 32bits */
-#cmakedefine ADM_WIN32
-
-/* ALSA is 1.0 */
-#cmakedefine ALSA_1_0_SUPPORT
-
-/* use ALSA as possible audio device */
-#cmakedefine ALSA_SUPPORT
-
-/* AMR_NB */
-#cmakedefine AMR_NB
-
-/* X86_64 AMD64 assembly */
-#cmakedefine ARCH_64_BITS
-
-/* Enable PowerPC optim */
-#cmakedefine ARCH_POWERPC
-
-/* AltiVec for libmpeg2 */
-#cmakedefine ARCH_PPC
-
-/* post proc */
-#cmakedefine ARCH_X86
-
-/* X86_32 assembly */
-#cmakedefine ARCH_X86_32
-
-/* X86_64 AMD64 assembly */
-#cmakedefine ARCH_X86_64
-
-/* FFMPEG */
-#cmakedefine CONFIG_ENCODERS
-#cmakedefine CONFIG_DVVIDEO_ENCODER
-
-#cmakedefine CONFIG_DECODERS
-#cmakedefine CONFIG_DVVIDEO_DECODER
-#cmakedefine CONFIG_H263_DECODER
-#cmakedefine CONFIG_MPEG4_DECODER
-#cmakedefine CONFIG_MPEGAUDIO_HP
-#cmakedefine CONFIG_SNOW_DECODER
-#cmakedefine CONFIG_VC1_DECODER
-#cmakedefine CONFIG_WMV2_DECODER
-#cmakedefine CONFIG_WMV3_DECODER
-#cmakedefine CONFIG_ZLIB
-
-#cmakedefine CONFIG_MUXERS
-#cmakedefine CONFIG_MOV_MUXER
-#cmakedefine CONFIG_MP4_MUXER
-#cmakedefine CONFIG_PSP_MUXER
-#cmakedefine CONFIG_TG2_MUXER
-#cmakedefine CONFIG_TGP_MUXER
-
-#define ENABLE_MMX ${ENABLE_MMX}
-#cmakedefine ENABLE_THREADS ${ENABLE_THREADS}
-#cmakedefine HAVE_FAST_UNALIGNED
-#cmakedefine HAVE_LRINTF
-#cmakedefine HAVE_MMX
-#cmakedefine HAVE_THREADS
-#cmakedefine RUNTIME_CPUDETECT
-
-#cmakedefine HAVE_FAST_64BIT
-#cmakedefine HAVE_SSSE3
-#cmakedefine CONFIG_DARWIN
-
-/* Name mangling */
-#cmakedefine CYG_MANGLING
-
-/* Mad */
-#cmakedefine FPM_DEFAULT
-#cmakedefine FPM_INTEL
-#cmakedefine FPM_PPC
-#cmakedefine FPM_SPARC
-
-/* Using GCC 2.9x.x */
-#cmakedefine GCC_2_95_X
-
-/* gettext package name */
-#cmakedefine GETTEXT_PACKAGE
-
-/* AltiVec for mpeg2enc */
-#cmakedefine HAVE_ALTIVEC
-
-/* Enable AltiVec by default */
-#cmakedefine HAVE_ALTIVEC_H
-
-/* Enable AltiVec by default */
-#cmakedefine HAVE_BUILTIN_VECTOR
-
-/* FontConfig detected */
-#cmakedefine HAVE_FONTCONFIG
-
-/* Define if the GNU gettext() function is already present or preinstalled. */
-#cmakedefine HAVE_GETTEXT
-
-/* Define to 1 if you have the `gettimeofday' function. */
-#cmakedefine HAVE_GETTIMEOFDAY
-
-/* Define to 1 if you have the &lt;inttypes.h&gt; header file. */
-#cmakedefine HAVE_INTTYPES_H
-
-/* Define to 1 if you have the `mp3lame' library (-lmp3lame). */
-#cmakedefine HAVE_LIBMP3LAME
-
-/* Use malloc.h */
-#cmakedefine HAVE_MALLOC_H
-
-/* Define to 1 if you have the &lt;stdint.h&gt; header file. */
-#cmakedefine HAVE_STDINT_H
-
-/* Define to 1 if you have the &lt;stdlib.h&gt; header file. */
-#cmakedefine HAVE_STDLIB_H
-
-/* Define to 1 if you have the &lt;string.h&gt; header file. */
-#cmakedefine HAVE_STRING_H
-
-/* Define to 1 if you have the &lt;sys/stat.h&gt; header file. */
-#cmakedefine HAVE_SYS_STAT_H
-
-/* Define to 1 if you have the &lt;sys/types.h&gt; header file. */
-#cmakedefine HAVE_SYS_TYPES_H
-
-/* Define to 1 if you have the &lt;unistd.h&gt; header file. */
-#cmakedefine HAVE_UNISTD_H
-
-/* stricter prototyping */
-#cmakedefine ICONV_NEED_CONST
-
-/* use classing FAAD support */
-#cmakedefine OLD_FAAD_PROTO
-
-/* OSS detected */
-#cmakedefine OSS_SUPPORT
-
-/* Name of package */
-#cmakedefine PACKAGE
-
-/* Define to the address where bug reports for this package should be sent. */
-#cmakedefine PACKAGE_BUGREPORT
-
-/* Define to the full name of this package. */
-#cmakedefine PACKAGE_NAME
-
-/* Define to the full name and version of this package. */
-#cmakedefine PACKAGE_STRING
-
-/* Define to the one symbol short name of this package. */
-#cmakedefine PACKAGE_TARNAME
-
-/* Define to the version of this package. */
-#cmakedefine PACKAGE_VERSION
-
-/* use liba52 */
-#cmakedefine USE_AC3
-
-/* use Aften AC3 encoder */
-#cmakedefine USE_AFTEN
-
-/* Use Aften 0.07 */
-#cmakedefine USE_AFTEN_07
-
-/* Use Aften 0.08 */
-#cmakedefine USE_AFTEN_08
-
-/* AltiVec for mpeg2enc */
-#cmakedefine USE_ALTIVEC
-
-/* Tell avidemux to use libamrnb */
-#cmakedefine USE_AMR_NB
-
-/* aRts detected */
-#cmakedefine USE_ARTS
-
-/* ESD detected */
-#cmakedefine USE_ESD
-
-/* Jack detected */
-#cmakedefine USE_JACK
-
-/* Use faac audio enccoder */
-#cmakedefine USE_FAAC
-
-/* FAAD2 detected */
-#cmakedefine USE_FAAD
-
-/* FFmpeg */
-#cmakedefine USE_FFMPEG
-
-/* FontConfig detected */
-#cmakedefine USE_FONTCONFIG
-
-/* FreeType2 detected */
-#cmakedefine USE_FREETYPE
-
-/* use late binding of selected libraries */
-#cmakedefine USE_LATE_BINDING
-
-/* libdca detected */
-#cmakedefine USE_LIBDCA
-
-/* Libxml2 is available */
-#cmakedefine USE_LIBXML2
-
-/* MJPEG */
-#cmakedefine USE_MJPEG
-
-/* use libmad */
-#cmakedefine USE_MP3
-
-/* libpng is available */
-#cmakedefine USE_PNG
-
-/* use libsamplerate */
-#cmakedefine USE_SRC
-
-/* SDL detected */
-#cmakedefine USE_SDL
-
-/* Vorbis detected */
-#cmakedefine USE_VORBIS
-
-/* use x264 encoder */
-#cmakedefine USE_X264
-
-/* XVideo detected */
-#cmakedefine USE_XV
-
-/* use Xvid 1.x API */
-#cmakedefine USE_XVID_4
-
-/* use Xvid 0.9 API */
-#cmakedefine USE_XX_XVID
-
-/* Version number of package */
-#define  VERSION &quot;${VERSION}&quot;
-
-/* Big endian CPU - SPARC or PowerPC */
-#cmakedefine WORDS_BIGENDIAN
-
-/* use Nvwa memory leak detector */
-#cmakedefine FIND_LEAKS
-
-#cmakedefine ADM_OS_APPLE
-#cmakedefine ADM_OS_UNIX
-#cmakedefine ADM_OS_WINDOWS
-
-#cmakedefine ADM_CPU_X86
-#cmakedefine ADM_CPU_X86_64
-
-#ifdef ADM_OS_WINDOWS
-#define rindex strrchr
-#define index strchr
-#define ftello ftello64
-#define fseeko fseeko64
-#endif
+/* config.h.in.  Generated from configure.in by autoheader.  */
+
+/* Jog Shuttle */
+#cmakedefine USE_JOG
+
+/* MPEG2DEC */
+#cmakedefine ACCEL_DETECT
+
+/* Big endian CPU - SPARC or PowerPC */
+#cmakedefine ADM_BIG_ENDIAN
+
+#define PACKAGE   &quot;avidemux&quot;
+#define ADMLOCALE &quot;${ADM_LOCALE}&quot;
+
+/* BSD OS specific ifdef */
+#cmakedefine ADM_BSD_FAMILY
+
+#cmakedefine HAVE_AUDIO
+
+/* Sparc workstations */
+#cmakedefine ADM_SPARC
+
+/* Build for Windows 32bits */
+#cmakedefine ADM_WIN32
+
+/* ALSA is 1.0 */
+#cmakedefine ALSA_1_0_SUPPORT
+
+/* use ALSA as possible audio device */
+#cmakedefine ALSA_SUPPORT
+
+/* AMR_NB */
+#cmakedefine AMR_NB
+
+/* X86_64 AMD64 assembly */
+#cmakedefine ARCH_64_BITS
+
+/* Enable PowerPC optim */
+#cmakedefine ARCH_POWERPC
+
+/* AltiVec for libmpeg2 */
+#cmakedefine ARCH_PPC
+
+/* post proc */
+#cmakedefine ARCH_X86
+
+/* X86_32 assembly */
+#cmakedefine ARCH_X86_32
+
+/* X86_64 AMD64 assembly */
+#cmakedefine ARCH_X86_64
+
+/* FFMPEG */
+#cmakedefine CONFIG_ENCODERS
+#cmakedefine CONFIG_DVVIDEO_ENCODER
+
+#cmakedefine CONFIG_DECODERS
+#cmakedefine CONFIG_DVVIDEO_DECODER
+#cmakedefine CONFIG_H263_DECODER
+#cmakedefine CONFIG_MPEG4_DECODER
+#cmakedefine CONFIG_MPEGAUDIO_HP
+#cmakedefine CONFIG_SNOW_DECODER
+#cmakedefine CONFIG_VC1_DECODER
+#cmakedefine CONFIG_WMV2_DECODER
+#cmakedefine CONFIG_WMV3_DECODER
+#cmakedefine CONFIG_ZLIB
+
+#cmakedefine CONFIG_MUXERS
+#cmakedefine CONFIG_MOV_MUXER
+#cmakedefine CONFIG_MP4_MUXER
+#cmakedefine CONFIG_PSP_MUXER
+#cmakedefine CONFIG_TG2_MUXER
+#cmakedefine CONFIG_TGP_MUXER
+
+#define ENABLE_MMX ${ENABLE_MMX}
+#cmakedefine ENABLE_THREADS ${ENABLE_THREADS}
+#cmakedefine HAVE_FAST_UNALIGNED
+#cmakedefine HAVE_LRINTF
+#cmakedefine HAVE_MMX
+#cmakedefine HAVE_THREADS
+#cmakedefine RUNTIME_CPUDETECT
+
+#cmakedefine HAVE_FAST_64BIT
+#cmakedefine HAVE_SSSE3
+#cmakedefine CONFIG_DARWIN
+
+/* Name mangling */
+#cmakedefine CYG_MANGLING
+
+/* Mad */
+#cmakedefine FPM_DEFAULT
+#cmakedefine FPM_INTEL
+#cmakedefine FPM_PPC
+#cmakedefine FPM_SPARC
+
+/* Using GCC 2.9x.x */
+#cmakedefine GCC_2_95_X
+
+/* gettext package name */
+#cmakedefine GETTEXT_PACKAGE
+
+/* AltiVec for mpeg2enc */
+#cmakedefine HAVE_ALTIVEC
+
+/* Enable AltiVec by default */
+#cmakedefine HAVE_ALTIVEC_H
+
+/* Enable AltiVec by default */
+#cmakedefine HAVE_BUILTIN_VECTOR
+
+/* FontConfig detected */
+#cmakedefine HAVE_FONTCONFIG
+
+/* Define if the GNU gettext() function is already present or preinstalled. */
+#cmakedefine HAVE_GETTEXT
+
+/* Define to 1 if you have the `gettimeofday' function. */
+#cmakedefine HAVE_GETTIMEOFDAY
+
+/* Define to 1 if you have the &lt;inttypes.h&gt; header file. */
+#cmakedefine HAVE_INTTYPES_H
+
+/* Define to 1 if you have the `mp3lame' library (-lmp3lame). */
+#cmakedefine HAVE_LIBMP3LAME
+
+/* Use malloc.h */
+#cmakedefine HAVE_MALLOC_H
+
+/* Define to 1 if you have the &lt;stdint.h&gt; header file. */
+#cmakedefine HAVE_STDINT_H
+
+/* Define to 1 if you have the &lt;stdlib.h&gt; header file. */
+#cmakedefine HAVE_STDLIB_H
+
+/* Define to 1 if you have the &lt;string.h&gt; header file. */
+#cmakedefine HAVE_STRING_H
+
+/* Define to 1 if you have the &lt;sys/stat.h&gt; header file. */
+#cmakedefine HAVE_SYS_STAT_H
+
+/* Define to 1 if you have the &lt;sys/types.h&gt; header file. */
+#cmakedefine HAVE_SYS_TYPES_H
+
+/* Define to 1 if you have the &lt;unistd.h&gt; header file. */
+#cmakedefine HAVE_UNISTD_H
+
+/* stricter prototyping */
+#cmakedefine ICONV_NEED_CONST
+
+/* use classing FAAD support */
+#cmakedefine OLD_FAAD_PROTO
+
+/* OSS detected */
+#cmakedefine OSS_SUPPORT
+
+/* Name of package */
+#cmakedefine PACKAGE
+
+/* Define to the address where bug reports for this package should be sent. */
+#cmakedefine PACKAGE_BUGREPORT
+
+/* Define to the full name of this package. */
+#cmakedefine PACKAGE_NAME
+
+/* Define to the full name and version of this package. */
+#cmakedefine PACKAGE_STRING
+
+/* Define to the one symbol short name of this package. */
+#cmakedefine PACKAGE_TARNAME
+
+/* Define to the version of this package. */
+#cmakedefine PACKAGE_VERSION
+
+/* use liba52 */
+#cmakedefine USE_AC3
+
+/* use Aften AC3 encoder */
+#cmakedefine USE_AFTEN
+
+/* Use Aften 0.07 */
+#cmakedefine USE_AFTEN_07
+
+/* Use Aften 0.08 */
+#cmakedefine USE_AFTEN_08
+
+/* AltiVec for mpeg2enc */
+#cmakedefine USE_ALTIVEC
+
+/* Tell avidemux to use libamrnb */
+#cmakedefine USE_AMR_NB
+
+/* aRts detected */
+#cmakedefine USE_ARTS
+
+/* ESD detected */
+#cmakedefine USE_ESD
+
+/* Jack detected */
+#cmakedefine USE_JACK
+
+/* Use faac audio enccoder */
+#cmakedefine USE_FAAC
+
+/* FAAD2 detected */
+#cmakedefine USE_FAAD
+
+/* FFmpeg */
+#cmakedefine USE_FFMPEG
+
+/* FontConfig detected */
+#cmakedefine USE_FONTCONFIG
+
+/* FreeType2 detected */
+#cmakedefine USE_FREETYPE
+
+/* use late binding of selected libraries */
+#cmakedefine USE_LATE_BINDING
+
+/* libdca detected */
+#cmakedefine USE_LIBDCA
+
+/* Libxml2 is available */
+#cmakedefine USE_LIBXML2
+
+/* MJPEG */
+#cmakedefine USE_MJPEG
+
+/* use libmad */
+#cmakedefine USE_MP3
+
+/* libpng is available */
+#cmakedefine USE_PNG
+
+/* use libsamplerate */
+#cmakedefine USE_SRC
+
+/* SDL detected */
+#cmakedefine USE_SDL
+
+/* Vorbis detected */
+#cmakedefine USE_VORBIS
+
+/* use x264 encoder */
+#cmakedefine USE_X264
+
+/* XVideo detected */
+#cmakedefine USE_XV
+
+/* use Xvid 1.x API */
+#cmakedefine USE_XVID_4
+
+/* use Xvid 0.9 API */
+#cmakedefine USE_XX_XVID
+
+/* Version number of package */
+#define  VERSION &quot;${VERSION}&quot;
+
+/* Big endian CPU - SPARC or PowerPC */
+#cmakedefine WORDS_BIGENDIAN
+
+/* use Nvwa memory leak detector */
+#cmakedefine FIND_LEAKS
+
+#cmakedefine ADM_OS_APPLE
+#cmakedefine ADM_OS_UNIX
+#cmakedefine ADM_OS_WINDOWS
+
+#cmakedefine ADM_CPU_PPC
+#cmakedefine ADM_CPU_X86
+#cmakedefine ADM_CPU_X86_64
+
+#ifdef ADM_OS_WINDOWS
+#define rindex strrchr
+#define index strchr
+#define ftello ftello64
+#define fseeko fseeko64
+#endif


</PRE>

<!--endarticle-->
    <HR>
    <P><UL>
        <!--threads-->
	<LI>Previous message: <A HREF="000987.html">[Avidemux-svn-commit] r3719 -	branches/avidemux_2.4_branch/avidemux/ADM_audiodevice
</A></li>
	<LI>Next message: <A HREF="000989.html">[Avidemux-svn-commit] r3721 - in branches/avidemux_2.4_branch: .	avidemux/ADM_libraries/ADM_libswscale	avidemux/ADM_libraries/ADM_mplex
</A></li>
         <LI> <B>Messages sorted by:</B> 
              <a href="date.html#988">[ date ]</a>
              <a href="thread.html#988">[ thread ]</a>
              <a href="subject.html#988">[ subject ]</a>
              <a href="author.html#988">[ author ]</a>
         </LI>
       </UL>

<hr>
<a href="https://lists.berlios.de/mailman/listinfo/avidemux-svn-commit">More information about the Avidemux-svn-commit
mailing list</a><br>
</body></html>
