From gruntster at mail.berlios.de  Sat Dec  1 15:47:51 2007
From: gruntster at mail.berlios.de (gruntster at mail.berlios.de)
Date: Sat, 1 Dec 2007 15:47:51 +0100
Subject: [Avidemux-svn-commit] r3714 -
	branches/avidemux_2.4_branch/avidemux/ADM_libraries/ADM_lavcodec
Message-ID: <200712011447.lB1ElpT9032347@sheep.berlios.de>

Author: gruntster
Date: 2007-12-01 15:47:44 +0100 (Sat, 01 Dec 2007)
New Revision: 3714

Modified:
   branches/avidemux_2.4_branch/avidemux/ADM_libraries/ADM_lavcodec/CMakeLists.txt
Log:
[CMake] remove unnecessary file from libavcodec build

Modified: branches/avidemux_2.4_branch/avidemux/ADM_libraries/ADM_lavcodec/CMakeLists.txt
===================================================================
--- branches/avidemux_2.4_branch/avidemux/ADM_libraries/ADM_lavcodec/CMakeLists.txt	2007-11-30 14:10:32 UTC (rev 3713)
+++ branches/avidemux_2.4_branch/avidemux/ADM_libraries/ADM_lavcodec/CMakeLists.txt	2007-12-01 14:47:44 UTC (rev 3714)
@@ -6,7 +6,7 @@
 #
 SET(${ADM_LIB}_SRCS 
 		adpcm.c 	allcodecs.c 	cyuv.c 	dsputil.c 	dv.c 
-		error_resilience.c 		fdctref.c 	fft.c 	h263.c 	h263dec.c 	huffyuv.c 	imgconvert.c 
+		error_resilience.c 		fft.c 	h263.c 	h263dec.c 	huffyuv.c 	imgconvert.c 
 		jfdctfst.c 	jfdctint.c 	jrevdct.c 	mace.c 	mdct.c 	 	mjpeg.c 
 		motion_est.c 	mpeg12.c 	mpegaudio.c 	mpegaudiodec.c 	mpegvideo.c 	msmpeg4.c 	 
 		pcm.c 	ratecontrol.c 	raw.c 	resample.c 	rv10.c 	simple_idct.c 	svq1.c 



From mean at mail.berlios.de  Sat Dec  1 18:12:44 2007
From: mean at mail.berlios.de (mean at BerliOS)
Date: Sat, 1 Dec 2007 18:12:44 +0100
Subject: [Avidemux-svn-commit] r3715 - in
	branches/avidemux_2.4_branch/avidemux/ADM_userInterfaces/ADM_GTK:
	ADM_gui2 glade
Message-ID: <200712011712.lB1HCiYh011996@sheep.berlios.de>

Author: mean
Date: 2007-12-01 18:12:43 +0100 (Sat, 01 Dec 2007)
New Revision: 3715

Modified:
   branches/avidemux_2.4_branch/avidemux/ADM_userInterfaces/ADM_GTK/ADM_gui2/GUI_main2.cpp
   branches/avidemux_2.4_branch/avidemux/ADM_userInterfaces/ADM_GTK/glade/gtk2_build.glade
Log:
[GTK] Fix jog (oops)

Modified: branches/avidemux_2.4_branch/avidemux/ADM_userInterfaces/ADM_GTK/ADM_gui2/GUI_main2.cpp
===================================================================
--- branches/avidemux_2.4_branch/avidemux/ADM_userInterfaces/ADM_GTK/ADM_gui2/GUI_main2.cpp	2007-12-01 14:47:44 UTC (rev 3714)
+++ branches/avidemux_2.4_branch/avidemux/ADM_userInterfaces/ADM_GTK/ADM_gui2/GUI_main2.cpp	2007-12-01 17:12:43 UTC (rev 3715)
@@ -220,7 +220,6 @@
   GtkWidget *comboboxFormat;
   GtkWidget *guiDrawing;
   GtkWidget *table2;
-  GtkWidget *jogg;
   GtkWidget *hbox16;
   GtkWidget *label1;
   GtkWidget *boxCurFrame;
@@ -263,6 +262,7 @@
   GtkWidget *buttonGotoA;
   GtkWidget *labelMarkB;
   GtkWidget *labelMarkA;
+  GtkWidget *jogg;
   extern GtkAccelGroup *accel_group;
   GtkTooltips *tooltips;
 
@@ -1195,14 +1195,6 @@
   gtk_container_set_border_width (GTK_CONTAINER (table2), 3);
   gtk_table_set_col_spacings (GTK_TABLE (table2), 6);
 
-  jogg = jog_shuttle_new ();gtk_widget_set_size_request (jogg, -1, 16); 
-  gtk_widget_show (jogg);
-  gtk_table_attach (GTK_TABLE (table2), jogg, 1, 2, 0, 1,
-                    (GtkAttachOptions) (0),
-                    (GtkAttachOptions) (0), 0, 0);
-  GTK_WIDGET_UNSET_FLAGS (jogg, GTK_CAN_FOCUS);
-  GTK_WIDGET_UNSET_FLAGS (jogg, GTK_CAN_DEFAULT);
-
   hbox16 = gtk_hbox_new (FALSE, 6);
   gtk_widget_show (hbox16);
   gtk_table_attach (GTK_TABLE (table2), hbox16, 0, 1, 2, 3,
@@ -1475,6 +1467,14 @@
   GTK_WIDGET_SET_FLAGS (labelMarkA, GTK_CAN_FOCUS);
   gtk_label_set_selectable (GTK_LABEL (labelMarkA), TRUE);
 
+  jogg = jog_shuttle_new ();gtk_widget_set_size_request (jogg, -1, 16); 
+  gtk_widget_show (jogg);
+  gtk_table_attach (GTK_TABLE (table2), jogg, 1, 2, 0, 1,
+                    (GtkAttachOptions) (GTK_FILL),
+                    (GtkAttachOptions) (GTK_EXPAND), 0, 0);
+  GTK_WIDGET_UNSET_FLAGS (jogg, GTK_CAN_FOCUS);
+  GTK_WIDGET_UNSET_FLAGS (jogg, GTK_CAN_DEFAULT);
+
   /* Store pointers to all widgets, for use by lookup_widget(). */
   GLADE_HOOKUP_OBJECT_NO_REF (mainWindow, mainWindow, "mainWindow");
   GLADE_HOOKUP_OBJECT (mainWindow, vbox1, "vbox1");
@@ -1663,7 +1663,6 @@
   GLADE_HOOKUP_OBJECT (mainWindow, comboboxFormat, "comboboxFormat");
   GLADE_HOOKUP_OBJECT (mainWindow, guiDrawing, "guiDrawing");
   GLADE_HOOKUP_OBJECT (mainWindow, table2, "table2");
-  GLADE_HOOKUP_OBJECT (mainWindow, jogg, "jogg");
   GLADE_HOOKUP_OBJECT (mainWindow, hbox16, "hbox16");
   GLADE_HOOKUP_OBJECT (mainWindow, label1, "label1");
   GLADE_HOOKUP_OBJECT (mainWindow, boxCurFrame, "boxCurFrame");
@@ -1706,6 +1705,7 @@
   GLADE_HOOKUP_OBJECT (mainWindow, buttonGotoA, "buttonGotoA");
   GLADE_HOOKUP_OBJECT (mainWindow, labelMarkB, "labelMarkB");
   GLADE_HOOKUP_OBJECT (mainWindow, labelMarkA, "labelMarkA");
+  GLADE_HOOKUP_OBJECT (mainWindow, jogg, "jogg");
   GLADE_HOOKUP_OBJECT_NO_REF (mainWindow, tooltips, "tooltips");
 
   gtk_window_add_accel_group (GTK_WINDOW (mainWindow), accel_group);

Modified: branches/avidemux_2.4_branch/avidemux/ADM_userInterfaces/ADM_GTK/glade/gtk2_build.glade
===================================================================
--- branches/avidemux_2.4_branch/avidemux/ADM_userInterfaces/ADM_GTK/glade/gtk2_build.glade	2007-12-01 14:47:44 UTC (rev 3714)
+++ branches/avidemux_2.4_branch/avidemux/ADM_userInterfaces/ADM_GTK/glade/gtk2_build.glade	2007-12-01 17:12:43 UTC (rev 3715)
@@ -1898,24 +1898,6 @@
 	      <property name="column_spacing">6</property>
 
 	      <child>
-		<widget class="Custom" id="jogg">
-		  <property name="visible">True</property>
-		  <property name="creation_function">jog_shuttle_new</property>
-		  <property name="int1">0</property>
-		  <property name="int2">0</property>
-		  <property name="last_modification_time">Sun, 07 Oct 2007 12:58:40 GMT</property>
-		</widget>
-		<packing>
-		  <property name="left_attach">1</property>
-		  <property name="right_attach">2</property>
-		  <property name="top_attach">0</property>
-		  <property name="bottom_attach">1</property>
-		  <property name="x_options"></property>
-		  <property name="y_options"></property>
-		</packing>
-	      </child>
-
-	      <child>
 		<widget class="GtkHBox" id="hbox16">
 		  <property name="visible">True</property>
 		  <property name="homogeneous">False</property>
@@ -2612,6 +2594,24 @@
 		  <property name="y_options"></property>
 		</packing>
 	      </child>
+
+	      <child>
+		<widget class="Custom" id="jogg">
+		  <property name="visible">True</property>
+		  <property name="creation_function">jog_shuttle_new</property>
+		  <property name="int1">0</property>
+		  <property name="int2">0</property>
+		  <property name="last_modification_time">Sun, 07 Oct 2007 12:58:40 GMT</property>
+		</widget>
+		<packing>
+		  <property name="left_attach">1</property>
+		  <property name="right_attach">2</property>
+		  <property name="top_attach">0</property>
+		  <property name="bottom_attach">1</property>
+		  <property name="x_options">fill</property>
+		  <property name="y_options">expand</property>
+		</packing>
+	      </child>
 	    </widget>
 	    <packing>
 	      <property name="padding">0</property>



From mean at mail.berlios.de  Sun Dec  2 16:41:33 2007
From: mean at mail.berlios.de (mean at BerliOS)
Date: Sun, 2 Dec 2007 16:41:33 +0100
Subject: [Avidemux-svn-commit] r3716 -
	branches/avidemux_2.4_branch/avidemux/ADM_libraries/ADM_lavcodec
Message-ID: <200712021541.lB2FfX34002420@sheep.berlios.de>

Author: mean
Date: 2007-12-02 16:41:32 +0100 (Sun, 02 Dec 2007)
New Revision: 3716

Modified:
   branches/avidemux_2.4_branch/avidemux/ADM_libraries/ADM_lavcodec/h263.c
Log:
[Lav] Try to avoid corruption decoding mpeg4/xvid

Modified: branches/avidemux_2.4_branch/avidemux/ADM_libraries/ADM_lavcodec/h263.c
===================================================================
--- branches/avidemux_2.4_branch/avidemux/ADM_libraries/ADM_lavcodec/h263.c	2007-12-01 17:12:43 UTC (rev 3715)
+++ branches/avidemux_2.4_branch/avidemux/ADM_libraries/ADM_lavcodec/h263.c	2007-12-02 15:41:32 UTC (rev 3716)
@@ -3123,7 +3123,7 @@
     int v= show_bits(&s->gb, 16);
 
     // MEANX if( s->workaround_bugs&FF_BUG_NO_PADDING){
-    if(s->lavc_build==4655) // Breaks multithreaded decoding meanx
+    if(s->lavc_build==4655 || s->xvid_build ) // Breaks multithreaded decoding meanx
     {
     #if 0
     #undef printf



From mean at mail.berlios.de  Sun Dec  2 17:02:08 2007
From: mean at mail.berlios.de (mean at BerliOS)
Date: Sun, 2 Dec 2007 17:02:08 +0100
Subject: [Avidemux-svn-commit] r3717 - branches/avidemux_2.4_branch/avidemux
Message-ID: <200712021602.lB2G289h003269@sheep.berlios.de>

Author: mean
Date: 2007-12-02 17:02:08 +0100 (Sun, 02 Dec 2007)
New Revision: 3717

Modified:
   branches/avidemux_2.4_branch/avidemux/gui_navigate.cpp
Log:
[UI] Fix seeking out of boundaries

Modified: branches/avidemux_2.4_branch/avidemux/gui_navigate.cpp
===================================================================
--- branches/avidemux_2.4_branch/avidemux/gui_navigate.cpp	2007-12-02 15:41:32 UTC (rev 3716)
+++ branches/avidemux_2.4_branch/avidemux/gui_navigate.cpp	2007-12-02 16:02:08 UTC (rev 3717)
@@ -363,8 +363,15 @@
 
     if (avifileinfo)
     {
+        if(frame>=avifileinfo->nb_frames)
+        {
+            curframe=avifileinfo->nb_frames-1;
+        }
+        else
+        {
             curframe=frame;
             GUI_PreviousKeyFrame();	
+        }
     }
 }
 
@@ -379,13 +386,14 @@
     if (!avifileinfo)
               return 0;
     
+      if(frame>=avifileinfo->nb_frames) return 0;
 
       if( !GUI_getFrame(frame ,&flags))
       {
         GUI_Error_HIG(QT_TR_NOOP("Decompressing error"),QT_TR_NOOP( "Cannot decode the frame."));
               return 0;
       }
-
+        
       curframe = frame;
       admPreview::update( curframe) ;
       update_status_bar();



From gruntster at mail.berlios.de  Sun Dec  2 20:47:05 2007
From: gruntster at mail.berlios.de (gruntster at mail.berlios.de)
Date: Sun, 2 Dec 2007 20:47:05 +0100
Subject: [Avidemux-svn-commit] r3718 - in branches/avidemux_2.4_branch: .
	avidemux/ADM_colorspace avidemux/ADM_libraries/ADM_utilities
	avidemux/ADM_osSupport cmake
Message-ID: <200712021947.lB2Jl5aO002951@sheep.berlios.de>

Author: gruntster
Date: 2007-12-02 20:46:22 +0100 (Sun, 02 Dec 2007)
New Revision: 3718

Modified:
   branches/avidemux_2.4_branch/ConfigureChecks.cmake
   branches/avidemux_2.4_branch/avidemux/ADM_colorspace/ADM_rgb.cpp
   branches/avidemux_2.4_branch/avidemux/ADM_libraries/ADM_utilities/ADM_imageLoader.cpp
   branches/avidemux_2.4_branch/avidemux/ADM_osSupport/ADM_cpuCap.cpp
   branches/avidemux_2.4_branch/cmake/lavcodec.cmake
   branches/avidemux_2.4_branch/config.h.cmake
Log:
[PowerPC] CMake support for PowerPC + fixes

Modified: branches/avidemux_2.4_branch/ConfigureChecks.cmake
===================================================================
--- branches/avidemux_2.4_branch/ConfigureChecks.cmake	2007-12-02 16:02:08 UTC (rev 3717)
+++ branches/avidemux_2.4_branch/ConfigureChecks.cmake	2007-12-02 19:46:22 UTC (rev 3718)
@@ -6,23 +6,27 @@
 
 IF (WIN32)
 	SET(ADM_OS_WINDOWS 1)
-	
+
 	IF (${CMAKE_SYSTEM_PROCESSOR} STREQUAL "x86")
 		SET(ADM_CPU_X86 1)
 	ENDIF (${CMAKE_SYSTEM_PROCESSOR} STREQUAL "x86")
 ELSEIF (APPLE)
 	SET(ADM_OS_APPLE 1)
-	
+
 	IF (${CMAKE_SYSTEM_PROCESSOR} STREQUAL "i386")
 		SET(ADM_CPU_X86 1)
+	ELSEIF (${CMAKE_SYSTEM_PROCESSOR} STREQUAL "powerpc" OR ${CMAKE_SYSTEM_PROCESSOR} STREQUAL "Power Macintosh")
+		SET(ADM_CPU_PPC 1)
 	ENDIF (${CMAKE_SYSTEM_PROCESSOR} STREQUAL "i386")
 ELSEIF (UNIX)
 	SET(ADM_OS_UNIX 1)
-	
+
 	IF (${CMAKE_SYSTEM_PROCESSOR} STREQUAL "i586" OR ${CMAKE_SYSTEM_PROCESSOR} STREQUAL "i686")
 		SET(ADM_CPU_X86 1)
-	ELSEIF(${CMAKE_SYSTEM_PROCESSOR} STREQUAL "x86_64" OR ${CMAKE_SYSTEM_PROCESSOR} STREQUAL "amd64")
+	ELSEIF (${CMAKE_SYSTEM_PROCESSOR} STREQUAL "x86_64" OR ${CMAKE_SYSTEM_PROCESSOR} STREQUAL "amd64")
 		SET(ADM_CPU_X86_64 1)
+	ELSEIF (${CMAKE_SYSTEM_PROCESSOR} STREQUAL "ppc")
+		SET(ADM_CPU_PPC 1)
 	ENDIF (${CMAKE_SYSTEM_PROCESSOR} STREQUAL "i586" OR ${CMAKE_SYSTEM_PROCESSOR} STREQUAL "i686")
 ENDIF (WIN32)
 
@@ -34,6 +38,9 @@
 	SET(ARCH_X86 1)
 	SET(ARCH_X86_64 1)
 	SET(ARCH_64_BITS 1)
+ELSEIF (ADM_CPU_PPC)
+	SET(ADM_BIG_ENDIAN 1)
+	SET(WORDS_BIGENDIAN 1)
 ENDIF (ADM_CPU_X86)
 
 ########################################
@@ -120,6 +127,8 @@
 	SET(FPM_INTEL 1)
 ELSEIF (ADM_CPU_X86_64)
 	SET(FPM_DEFAULT 1)
+ELSEIF (ADM_CPU_PPC)
+	SET(FPM_PPC 1)
 ENDIF (ADM_CPU_X86)
 
 ########################################

Modified: branches/avidemux_2.4_branch/avidemux/ADM_colorspace/ADM_rgb.cpp
===================================================================
--- branches/avidemux_2.4_branch/avidemux/ADM_colorspace/ADM_rgb.cpp	2007-12-02 16:02:08 UTC (rev 3717)
+++ branches/avidemux_2.4_branch/avidemux/ADM_colorspace/ADM_rgb.cpp	2007-12-02 19:46:22 UTC (rev 3718)
@@ -262,18 +262,20 @@
             }
      }
 #endif
-#if  defined( ADM_BIG_ENDIAN)
-        uint8_t r,g,b,a;
-        uint8_t *ptr=target;
-        int pel=h*w;
-        for(int yy=0;yy<th;yy++)
-                {
-                  *ptr=target+(startx*4)+(starty+yy)*totalW*4;;
-                  invertRGB(ptr,tw);
-                }
-        
+
+#if defined(ADM_BIG_ENDIAN)
+     uint8_t r, g, b, a;
+     uint8_t *ptr = target;
+     int pel = h * w;
+
+     for (int yy = 0; yy < th; yy++)
+     {
+          ptr = target + (startx * 4) + (starty + yy) * totalW * 4;
+          ADM_RGBA2BGRA(ptr, tw);
+     }
 #endif
-  return 1;
+
+     return 1;
  }
 
  

Modified: branches/avidemux_2.4_branch/avidemux/ADM_libraries/ADM_utilities/ADM_imageLoader.cpp
===================================================================
--- branches/avidemux_2.4_branch/avidemux/ADM_libraries/ADM_utilities/ADM_imageLoader.cpp	2007-12-02 16:02:08 UTC (rev 3717)
+++ branches/avidemux_2.4_branch/avidemux/ADM_libraries/ADM_utilities/ADM_imageLoader.cpp	2007-12-02 19:46:22 UTC (rev 3718)
@@ -33,6 +33,8 @@
 #include "ADM_codecs/ADM_png.h"
 #include "ADM_toolkit/toolkit.hxx"
 #include "ADM_toolkit/bitmap.h"
+#include "ADM_editor/ADM_edit.hxx"
+
 //**********************************
 static ADMImage *createImageFromFile_jpeg(const char *filename);
 static ADMImage *createImageFromFile_Bmp(const char *filename);
@@ -262,7 +264,7 @@
 ADMImage *createImageFromFile_Bmp2(const char *filename)
 {
     
-	BITMAPHEADER bmph;
+	BITMAPINFOHEADER bmph;
     uint8_t fcc_tab[4];
     uint32_t offset;
     FILE *fd=NULL;
@@ -281,14 +283,14 @@
  #ifdef ADM_BIG_ENDIAN
  	    Endian_BitMapInfo(&bmph);
  #endif
- 	    if (bmph.compressionScheme != 0) 
+ 	    if (bmph.biCompression != 0) 
  	    {
  	    	printf("[imageLoader] BMP2:Cannot handle compressed bmp\n");
  	    	fclose(fd);
  	    	return NULL;
  	    }
- 	    w = bmph.width;
- 	    h = bmph.height;
+ 	    w = bmph.biWidth;
+ 	    h = bmph.biHeight;
  	    printf("[imageLoader] BMP2 W: %d H: %d offset : %d\n", w, h, offset);
 // Load the binary coded image
  	fseek(fd,offset,SEEK_SET);
@@ -359,7 +361,6 @@
 		    uint8_t fcc_tab[4];
 		    FILE *fd;
 		    uint32_t off,tag,count,size;
-		    BITMAPHEADER bmph;
 
 		    // 1- identity the file type
 		    //
@@ -429,7 +430,8 @@
 		    // BMP2?
 		    if (fcc_tab[0] == 'B' && fcc_tab[1] == 'M') 
 		    {
-		    	  
+		    	    BITMAPINFOHEADER bmph;
+
 		     	    fseek(fd, 10, SEEK_SET);
 		     	    fread(fcc_tab, 4, 1, fd);
 		     	    // size, width height follow as int32 
@@ -437,14 +439,14 @@
 		     #ifdef ADM_BIG_ENDIAN
 		     	    Endian_BitMapInfo(&bmph);
 		     #endif
-		     	    if (bmph.compressionScheme != 0) 
+		     	    if (bmph.biCompression != 0) 
 		     	    {
 		     	    	printf("[imageIdentify] BMP2:Cannot handle compressed bmp\n");
 		     	    	fclose(fd);
 		     	    	return ADM_IMAGE_UNKNOWN;
 		     	    }
-		     	    *w = bmph.width;
-		     	    *h = bmph.height;
+		     	    *w = bmph.biWidth;
+		     	    *h = bmph.biHeight;
 	     	    	fclose(fd);
 	     	    	return ADM_IMAGE_BMP2;
 		    }

Modified: branches/avidemux_2.4_branch/avidemux/ADM_osSupport/ADM_cpuCap.cpp
===================================================================
--- branches/avidemux_2.4_branch/avidemux/ADM_osSupport/ADM_cpuCap.cpp	2007-12-02 16:02:08 UTC (rev 3717)
+++ branches/avidemux_2.4_branch/avidemux/ADM_osSupport/ADM_cpuCap.cpp	2007-12-02 19:46:22 UTC (rev 3718)
@@ -32,7 +32,7 @@
 }
 
 
-#if defined( ARCH_X86)  || defined(ARCH_X86_64)
+#if defined(ARCH_X86) || defined(ARCH_X86_64)
 extern "C" 
 {
 #include "ADM_lavcodec/dsputil_cpu.h"
@@ -60,7 +60,7 @@
 	myCpuMask=0xffffffff;
 	prefs->get(FEATURE_CPU_CAPS,&myCpuMask);
 
-#if defined( ARCH_X86)  || defined(ARCH_X86_64)	
+#if defined(ARCH_X86) || defined(ARCH_X86_64)
 int rval = 0;
  int eax, ebx, ecx, edx;
  int max_std_level, max_ext_level, std_caps=0, ext_caps=0;
@@ -141,20 +141,29 @@
 }
 
 /************************************************************************/
+#if defined(ARCH_X86) || defined(ARCH_X86_64)
 #include "ADM_libraries/ADM_libMpeg2Dec/mpeg2_cpu.h"
+#endif
+
 int ADM_mpeg2dec_mm_support(void)
 {
 int rval=0;
+
+#if defined(ARCH_X86) || defined(ARCH_X86_64)
 #undef MATCH
 #define MATCH(x,y) if(CpuCaps::myCpuCaps & CpuCaps::myCpuMask & ADM_CPU_##x) rval|=MPEG2_ACCEL_X86_##x;
 	
 	MATCH(MMX,MMX);
 	MATCH(MMXEXT,MMXEXT);
 	MATCH(3DNOW,3DNOW);
+#endif
+
 	return rval;
 }
 //******************************************************
+#if defined(ARCH_X86) || defined(ARCH_X86_64)
 #include "ADM_lavcodec.h"
+#endif
 /**
  * 		\fn lavcodec_mm_support
  * 		\brief Give lavcodec CPU supported ( FF_MM_MMX)
@@ -162,9 +171,11 @@
 int ADM_lavcodec_mm_support(void)
 {
 int rval=0;
+
+#if defined(ARCH_X86) || defined(ARCH_X86_64)
 #undef MATCH
 #define MATCH(x,y) if(CpuCaps::myCpuCaps &  CpuCaps::myCpuMask & ADM_CPU_##x) rval|=MM_##x;
-	
+
 	MATCH(MMX,MMX);
 	MATCH(MMXEXT,MMXEXT);
 	MATCH(SSE,SSE);
@@ -173,6 +184,8 @@
 	MATCH(SSSE3,SSSE3);
 	MATCH(3DNOW,3DNOW);
 	MATCH(3DNOWEXT,3DNOWEXT);
+#endif
+
 	return rval;
 }
 // EOF

Modified: branches/avidemux_2.4_branch/cmake/lavcodec.cmake
===================================================================
--- branches/avidemux_2.4_branch/cmake/lavcodec.cmake	2007-12-02 16:02:08 UTC (rev 3717)
+++ branches/avidemux_2.4_branch/cmake/lavcodec.cmake	2007-12-02 19:46:22 UTC (rev 3718)
@@ -3,41 +3,46 @@
 	SET(CONFIG_DVVIDEO_ENCODER 1)
 
 	SET(CONFIG_DECODERS 1)
-	SET(CONFIG_DVVIDEO_DECODER 1)
-	SET(CONFIG_H263_DECODER 1)
-	SET(CONFIG_MPEG4_DECODER 1)
-	SET(CONFIG_MPEGAUDIO_HP 1)
-	SET(CONFIG_SNOW_DECODER 1)
-	SET(CONFIG_VC1_DECODER 1)
-	SET(CONFIG_WMV2_DECODER 1)
-	SET(CONFIG_WMV3_DECODER 1)
-	SET(CONFIG_ZLIB 1)
+	SET(CONFIG_DVVIDEO_DECODER 1)
+	SET(CONFIG_H263_DECODER 1)
+	SET(CONFIG_MPEG4_DECODER 1)
+	SET(CONFIG_MPEGAUDIO_HP 1)
+	SET(CONFIG_SNOW_DECODER 1)
+	SET(CONFIG_VC1_DECODER 1)
+	SET(CONFIG_WMV2_DECODER 1)
+	SET(CONFIG_WMV3_DECODER 1)
+	SET(CONFIG_ZLIB 1)
 
 	SET(CONFIG_MUXERS 1)
-	SET(CONFIG_MOV_MUXER 1)
-	SET(CONFIG_MP4_MUXER 1)
-	SET(CONFIG_PSP_MUXER 1)
-	SET(CONFIG_TG2_MUXER 1)
-	SET(CONFIG_TGP_MUXER 1)
+	SET(CONFIG_MOV_MUXER 1)
+	SET(CONFIG_MP4_MUXER 1)
+	SET(CONFIG_PSP_MUXER 1)
+	SET(CONFIG_TG2_MUXER 1)
+	SET(CONFIG_TGP_MUXER 1)
 
-	SET(ENABLE_MMX 1)
-	SET(ENABLE_THREADS 1)
-	SET(HAVE_FAST_UNALIGNED 1)
-	SET(HAVE_LRINTF 1)
-	SET(HAVE_MMX 1)
-	SET(HAVE_THREADS 1)
+	SET(ENABLE_THREADS 1)
+	SET(HAVE_LRINTF 1)
+	SET(HAVE_THREADS 1)
 	SET(RUNTIME_CPUDETECT 1)
-	
-	IF (ADM_CPU_X86_64)
-		SET(HAVE_FAST_64BIT 1)
-	ENDIF (ADM_CPU_X86_64)
-
-	IF (NOT NO_SSSE3)
-		SET(HAVE_SSSE3 1)
-	ENDIF (NOT NO_SSSE3)
-
-	IF (ADM_OS_APPLE)
-		SET(CONFIG_DARWIN 1)
+
+	IF (ADM_CPU_X86 OR ADM_CPU_X86_64)
+		SET(ENABLE_MMX 1)
+		SET(HAVE_MMX 1)
+		SET(HAVE_FAST_UNALIGNED 1)
+
+		IF (NOT NO_SSSE3)
+			SET(HAVE_SSSE3 1)
+		ENDIF (NOT NO_SSSE3)
+
+		IF (ADM_CPU_X86_64)
+			SET(HAVE_FAST_64BIT 1)
+		ENDIF (ADM_CPU_X86_64)
+	ELSE (ADM_CPU_X86 OR ADM_CPU_X86_64)
+		SET(ENABLE_MMX 0)
+	ENDIF (ADM_CPU_X86 OR ADM_CPU_X86_64)
+
+	IF (ADM_OS_APPLE)
+		SET(CONFIG_DARWIN 1)
 	ENDIF (ADM_OS_APPLE)
 ENDMACRO(SET_LAVCODEC_FLAGS)
 

Modified: branches/avidemux_2.4_branch/config.h.cmake
===================================================================
--- branches/avidemux_2.4_branch/config.h.cmake	2007-12-02 16:02:08 UTC (rev 3717)
+++ branches/avidemux_2.4_branch/config.h.cmake	2007-12-02 19:46:22 UTC (rev 3718)
@@ -1,56 +1,56 @@
-/* config.h.in.  Generated from configure.in by autoheader.  */
-
-/* Jog Shuttle */
-#cmakedefine USE_JOG
-
-/* MPEG2DEC */
-#cmakedefine ACCEL_DETECT
-
-/* Big endian CPU - SPARC or PowerPC */
-#cmakedefine ADM_BIG_ENDIAN
-
-#define PACKAGE   "avidemux"
-#define ADMLOCALE "${ADM_LOCALE}"
-
-/* BSD OS specific ifdef */
-#cmakedefine ADM_BSD_FAMILY
-
-#cmakedefine HAVE_AUDIO
-
-/* Sparc workstations */
-#cmakedefine ADM_SPARC
-
-/* Build for Windows 32bits */
-#cmakedefine ADM_WIN32
-
-/* ALSA is 1.0 */
-#cmakedefine ALSA_1_0_SUPPORT
-
-/* use ALSA as possible audio device */
-#cmakedefine ALSA_SUPPORT
-
-/* AMR_NB */
-#cmakedefine AMR_NB
-
-/* X86_64 AMD64 assembly */
-#cmakedefine ARCH_64_BITS
-
-/* Enable PowerPC optim */
-#cmakedefine ARCH_POWERPC
-
-/* AltiVec for libmpeg2 */
-#cmakedefine ARCH_PPC
-
-/* post proc */
-#cmakedefine ARCH_X86
-
-/* X86_32 assembly */
-#cmakedefine ARCH_X86_32
-
-/* X86_64 AMD64 assembly */
-#cmakedefine ARCH_X86_64
-
-/* FFMPEG */
+/* config.h.in.  Generated from configure.in by autoheader.  */
+
+/* Jog Shuttle */
+#cmakedefine USE_JOG
+
+/* MPEG2DEC */
+#cmakedefine ACCEL_DETECT
+
+/* Big endian CPU - SPARC or PowerPC */
+#cmakedefine ADM_BIG_ENDIAN
+
+#define PACKAGE   "avidemux"
+#define ADMLOCALE "${ADM_LOCALE}"
+
+/* BSD OS specific ifdef */
+#cmakedefine ADM_BSD_FAMILY
+
+#cmakedefine HAVE_AUDIO
+
+/* Sparc workstations */
+#cmakedefine ADM_SPARC
+
+/* Build for Windows 32bits */
+#cmakedefine ADM_WIN32
+
+/* ALSA is 1.0 */
+#cmakedefine ALSA_1_0_SUPPORT
+
+/* use ALSA as possible audio device */
+#cmakedefine ALSA_SUPPORT
+
+/* AMR_NB */
+#cmakedefine AMR_NB
+
+/* X86_64 AMD64 assembly */
+#cmakedefine ARCH_64_BITS
+
+/* Enable PowerPC optim */
+#cmakedefine ARCH_POWERPC
+
+/* AltiVec for libmpeg2 */
+#cmakedefine ARCH_PPC
+
+/* post proc */
+#cmakedefine ARCH_X86
+
+/* X86_32 assembly */
+#cmakedefine ARCH_X86_32
+
+/* X86_64 AMD64 assembly */
+#cmakedefine ARCH_X86_64
+
+/* FFMPEG */
 #cmakedefine CONFIG_ENCODERS
 #cmakedefine CONFIG_DVVIDEO_ENCODER
 
@@ -71,206 +71,206 @@
 #cmakedefine CONFIG_PSP_MUXER
 #cmakedefine CONFIG_TG2_MUXER
 #cmakedefine CONFIG_TGP_MUXER
-
-#cmakedefine ENABLE_MMX ${ENABLE_MMX}
-#cmakedefine ENABLE_THREADS ${ENABLE_THREADS}
-#cmakedefine HAVE_FAST_UNALIGNED
-#cmakedefine HAVE_LRINTF
-#cmakedefine HAVE_MMX
-#cmakedefine HAVE_THREADS
-#cmakedefine RUNTIME_CPUDETECT
-
-#cmakedefine HAVE_FAST_64BIT
-#cmakedefine HAVE_SSSE3
-#cmakedefine CONFIG_DARWIN
-
-/* Name mangling */
-#cmakedefine CYG_MANGLING
-
-/* Mad */
-#cmakedefine FPM_DEFAULT
-#cmakedefine FPM_INTEL
-#cmakedefine FPM_PPC
-#cmakedefine FPM_SPARC
-
-/* Using GCC 2.9x.x */
-#cmakedefine GCC_2_95_X
-
-/* gettext package name */
-#cmakedefine GETTEXT_PACKAGE
-
-/* AltiVec for mpeg2enc */
-#cmakedefine HAVE_ALTIVEC
-
-/* Enable AltiVec by default */
-#cmakedefine HAVE_ALTIVEC_H
-
-/* Enable AltiVec by default */
-#cmakedefine HAVE_BUILTIN_VECTOR
-
-/* FontConfig detected */
-#cmakedefine HAVE_FONTCONFIG
-
-/* Define if the GNU gettext() function is already present or preinstalled. */
-#cmakedefine HAVE_GETTEXT
-
-/* Define to 1 if you have the `gettimeofday' function. */
-#cmakedefine HAVE_GETTIMEOFDAY
-
-/* Define to 1 if you have the <inttypes.h> header file. */
-#cmakedefine HAVE_INTTYPES_H
-
-/* Define to 1 if you have the `mp3lame' library (-lmp3lame). */
-#cmakedefine HAVE_LIBMP3LAME
-
-/* Use malloc.h */
-#cmakedefine HAVE_MALLOC_H
-
-/* Define to 1 if you have the <stdint.h> header file. */
-#cmakedefine HAVE_STDINT_H
-
-/* Define to 1 if you have the <stdlib.h> header file. */
-#cmakedefine HAVE_STDLIB_H
-
-/* Define to 1 if you have the <string.h> header file. */
-#cmakedefine HAVE_STRING_H
-
-/* Define to 1 if you have the <sys/stat.h> header file. */
-#cmakedefine HAVE_SYS_STAT_H
-
-/* Define to 1 if you have the <sys/types.h> header file. */
-#cmakedefine HAVE_SYS_TYPES_H
-
-/* Define to 1 if you have the <unistd.h> header file. */
-#cmakedefine HAVE_UNISTD_H
-
-/* stricter prototyping */
-#cmakedefine ICONV_NEED_CONST
-
-/* use classing FAAD support */
-#cmakedefine OLD_FAAD_PROTO
-
-/* OSS detected */
-#cmakedefine OSS_SUPPORT
-
-/* Name of package */
-#cmakedefine PACKAGE
-
-/* Define to the address where bug reports for this package should be sent. */
-#cmakedefine PACKAGE_BUGREPORT
-
-/* Define to the full name of this package. */
-#cmakedefine PACKAGE_NAME
-
-/* Define to the full name and version of this package. */
-#cmakedefine PACKAGE_STRING
-
-/* Define to the one symbol short name of this package. */
-#cmakedefine PACKAGE_TARNAME
-
-/* Define to the version of this package. */
-#cmakedefine PACKAGE_VERSION
-
-/* use liba52 */
-#cmakedefine USE_AC3
-
-/* use Aften AC3 encoder */
-#cmakedefine USE_AFTEN
-
-/* Use Aften 0.07 */
-#cmakedefine USE_AFTEN_07
-
-/* Use Aften 0.08 */
-#cmakedefine USE_AFTEN_08
-
-/* AltiVec for mpeg2enc */
-#cmakedefine USE_ALTIVEC
-
-/* Tell avidemux to use libamrnb */
-#cmakedefine USE_AMR_NB
-
-/* aRts detected */
-#cmakedefine USE_ARTS
-
-/* ESD detected */
-#cmakedefine USE_ESD
-
-/* Jack detected */
-#cmakedefine USE_JACK
-
-/* Use faac audio enccoder */
-#cmakedefine USE_FAAC
-
-/* FAAD2 detected */
-#cmakedefine USE_FAAD
-
-/* FFmpeg */
-#cmakedefine USE_FFMPEG
-
-/* FontConfig detected */
-#cmakedefine USE_FONTCONFIG
-
-/* FreeType2 detected */
-#cmakedefine USE_FREETYPE
-
-/* use late binding of selected libraries */
-#cmakedefine USE_LATE_BINDING
-
-/* libdca detected */
-#cmakedefine USE_LIBDCA
-
-/* Libxml2 is available */
-#cmakedefine USE_LIBXML2
-
-/* MJPEG */
-#cmakedefine USE_MJPEG
-
-/* use libmad */
-#cmakedefine USE_MP3
-
-/* libpng is available */
-#cmakedefine USE_PNG
-
-/* use libsamplerate */
-#cmakedefine USE_SRC
-
-/* SDL detected */
-#cmakedefine USE_SDL
-
-/* Vorbis detected */
-#cmakedefine USE_VORBIS
-
-/* use x264 encoder */
-#cmakedefine USE_X264
-
-/* XVideo detected */
-#cmakedefine USE_XV
-
-/* use Xvid 1.x API */
-#cmakedefine USE_XVID_4
-
-/* use Xvid 0.9 API */
-#cmakedefine USE_XX_XVID
-
-/* Version number of package */
-#define  VERSION "${VERSION}"
-
-/* Big endian CPU - SPARC or PowerPC */
-#cmakedefine WORDS_BIGENDIAN
-
-/* use Nvwa memory leak detector */
-#cmakedefine FIND_LEAKS
-
-#cmakedefine ADM_OS_APPLE
-#cmakedefine ADM_OS_UNIX
-#cmakedefine ADM_OS_WINDOWS
-
-#cmakedefine ADM_CPU_X86
-#cmakedefine ADM_CPU_X86_64
-
-#ifdef ADM_OS_WINDOWS
-#define rindex strrchr
-#define index strchr
-#define ftello ftello64
-#define fseeko fseeko64
-#endif
+
+#define ENABLE_MMX ${ENABLE_MMX}
+#cmakedefine ENABLE_THREADS ${ENABLE_THREADS}
+#cmakedefine HAVE_FAST_UNALIGNED
+#cmakedefine HAVE_LRINTF
+#cmakedefine HAVE_MMX
+#cmakedefine HAVE_THREADS
+#cmakedefine RUNTIME_CPUDETECT
+
+#cmakedefine HAVE_FAST_64BIT
+#cmakedefine HAVE_SSSE3
+#cmakedefine CONFIG_DARWIN
+
+/* Name mangling */
+#cmakedefine CYG_MANGLING
+
+/* Mad */
+#cmakedefine FPM_DEFAULT
+#cmakedefine FPM_INTEL
+#cmakedefine FPM_PPC
+#cmakedefine FPM_SPARC
+
+/* Using GCC 2.9x.x */
+#cmakedefine GCC_2_95_X
+
+/* gettext package name */
+#cmakedefine GETTEXT_PACKAGE
+
+/* AltiVec for mpeg2enc */
+#cmakedefine HAVE_ALTIVEC
+
+/* Enable AltiVec by default */
+#cmakedefine HAVE_ALTIVEC_H
+
+/* Enable AltiVec by default */
+#cmakedefine HAVE_BUILTIN_VECTOR
+
+/* FontConfig detected */
+#cmakedefine HAVE_FONTCONFIG
+
+/* Define if the GNU gettext() function is already present or preinstalled. */
+#cmakedefine HAVE_GETTEXT
+
+/* Define to 1 if you have the `gettimeofday' function. */
+#cmakedefine HAVE_GETTIMEOFDAY
+
+/* Define to 1 if you have the <inttypes.h> header file. */
+#cmakedefine HAVE_INTTYPES_H
+
+/* Define to 1 if you have the `mp3lame' library (-lmp3lame). */
+#cmakedefine HAVE_LIBMP3LAME
+
+/* Use malloc.h */
+#cmakedefine HAVE_MALLOC_H
+
+/* Define to 1 if you have the <stdint.h> header file. */
+#cmakedefine HAVE_STDINT_H
+
+/* Define to 1 if you have the <stdlib.h> header file. */
+#cmakedefine HAVE_STDLIB_H
+
+/* Define to 1 if you have the <string.h> header file. */
+#cmakedefine HAVE_STRING_H
+
+/* Define to 1 if you have the <sys/stat.h> header file. */
+#cmakedefine HAVE_SYS_STAT_H
+
+/* Define to 1 if you have the <sys/types.h> header file. */
+#cmakedefine HAVE_SYS_TYPES_H
+
+/* Define to 1 if you have the <unistd.h> header file. */
+#cmakedefine HAVE_UNISTD_H
+
+/* stricter prototyping */
+#cmakedefine ICONV_NEED_CONST
+
+/* use classing FAAD support */
+#cmakedefine OLD_FAAD_PROTO
+
+/* OSS detected */
+#cmakedefine OSS_SUPPORT
+
+/* Name of package */
+#cmakedefine PACKAGE
+
+/* Define to the address where bug reports for this package should be sent. */
+#cmakedefine PACKAGE_BUGREPORT
+
+/* Define to the full name of this package. */
+#cmakedefine PACKAGE_NAME
+
+/* Define to the full name and version of this package. */
+#cmakedefine PACKAGE_STRING
+
+/* Define to the one symbol short name of this package. */
+#cmakedefine PACKAGE_TARNAME
+
+/* Define to the version of this package. */
+#cmakedefine PACKAGE_VERSION
+
+/* use liba52 */
+#cmakedefine USE_AC3
+
+/* use Aften AC3 encoder */
+#cmakedefine USE_AFTEN
+
+/* Use Aften 0.07 */
+#cmakedefine USE_AFTEN_07
+
+/* Use Aften 0.08 */
+#cmakedefine USE_AFTEN_08
+
+/* AltiVec for mpeg2enc */
+#cmakedefine USE_ALTIVEC
+
+/* Tell avidemux to use libamrnb */
+#cmakedefine USE_AMR_NB
+
+/* aRts detected */
+#cmakedefine USE_ARTS
+
+/* ESD detected */
+#cmakedefine USE_ESD
+
+/* Jack detected */
+#cmakedefine USE_JACK
+
+/* Use faac audio enccoder */
+#cmakedefine USE_FAAC
+
+/* FAAD2 detected */
+#cmakedefine USE_FAAD
+
+/* FFmpeg */
+#cmakedefine USE_FFMPEG
+
+/* FontConfig detected */
+#cmakedefine USE_FONTCONFIG
+
+/* FreeType2 detected */
+#cmakedefine USE_FREETYPE
+
+/* use late binding of selected libraries */
+#cmakedefine USE_LATE_BINDING
+
+/* libdca detected */
+#cmakedefine USE_LIBDCA
+
+/* Libxml2 is available */
+#cmakedefine USE_LIBXML2
+
+/* MJPEG */
+#cmakedefine USE_MJPEG
+
+/* use libmad */
+#cmakedefine USE_MP3
+
+/* libpng is available */
+#cmakedefine USE_PNG
+
+/* use libsamplerate */
+#cmakedefine USE_SRC
+
+/* SDL detected */
+#cmakedefine USE_SDL
+
+/* Vorbis detected */
+#cmakedefine USE_VORBIS
+
+/* use x264 encoder */
+#cmakedefine USE_X264
+
+/* XVideo detected */
+#cmakedefine USE_XV
+
+/* use Xvid 1.x API */
+#cmakedefine USE_XVID_4
+
+/* use Xvid 0.9 API */
+#cmakedefine USE_XX_XVID
+
+/* Version number of package */
+#define  VERSION "${VERSION}"
+
+/* Big endian CPU - SPARC or PowerPC */
+#cmakedefine WORDS_BIGENDIAN
+
+/* use Nvwa memory leak detector */
+#cmakedefine FIND_LEAKS
+
+#cmakedefine ADM_OS_APPLE
+#cmakedefine ADM_OS_UNIX
+#cmakedefine ADM_OS_WINDOWS
+
+#cmakedefine ADM_CPU_X86
+#cmakedefine ADM_CPU_X86_64
+
+#ifdef ADM_OS_WINDOWS
+#define rindex strrchr
+#define index strchr
+#define ftello ftello64
+#define fseeko fseeko64
+#endif



From gruntster at mail.berlios.de  Mon Dec  3 13:38:21 2007
From: gruntster at mail.berlios.de (gruntster at mail.berlios.de)
Date: Mon, 3 Dec 2007 13:38:21 +0100
Subject: [Avidemux-svn-commit] r3719 -
	branches/avidemux_2.4_branch/avidemux/ADM_audiodevice
Message-ID: <200712031238.lB3CcLLp030673@sheep.berlios.de>

Author: gruntster
Date: 2007-12-03 13:38:06 +0100 (Mon, 03 Dec 2007)
New Revision: 3719

Modified:
   branches/avidemux_2.4_branch/avidemux/ADM_audiodevice/ADM_deviceAudioCore.cpp
Log:
[Mac] vip007: Core Audio fix for Leopard

Modified: branches/avidemux_2.4_branch/avidemux/ADM_audiodevice/ADM_deviceAudioCore.cpp
===================================================================
--- branches/avidemux_2.4_branch/avidemux/ADM_audiodevice/ADM_deviceAudioCore.cpp	2007-12-02 19:46:22 UTC (rev 3718)
+++ branches/avidemux_2.4_branch/avidemux/ADM_audiodevice/ADM_deviceAudioCore.cpp	2007-12-03 12:38:06 UTC (rev 3719)
@@ -39,8 +39,8 @@
 static uint32_t wr_ptr = 0;
 static pthread_mutex_t lock;
 
-static OSStatus MyRenderer(void *inRefCon, AudioUnitRenderActionFlags inActionFlags, 
-	const AudioTimeStamp *inTimeStamp, UInt32 inBusNumber, AudioBuffer *ioData);
+static OSStatus MyRenderer(void *inRefCon, AudioUnitRenderActionFlags *inActionFlags,
+	const AudioTimeStamp *inTimeStamp, UInt32 inBusNumber, UInt32 inNumberFrames, AudioBufferList *ioData);
 static OSStatus OverloadListenerProc(AudioDeviceID inDevice, UInt32 inChannel, Boolean isInput,
 	AudioDevicePropertyID inPropertyID, void* inClientData);
 
@@ -73,16 +73,16 @@
 	return 1;
 }
 
-OSStatus MyRenderer(void *inRefCon, AudioUnitRenderActionFlags inActionFlags, const AudioTimeStamp *inTimeStamp, 
-	UInt32 inBusNumber, AudioBuffer *ioData)
+OSStatus MyRenderer(void *inRefCon, AudioUnitRenderActionFlags *inActionFlags, const AudioTimeStamp *inTimeStamp,
+	UInt32 inBusNumber, UInt32 inChannel, AudioBufferList *ioData)
 {
 	pthread_mutex_lock(&lock);
-	uint32_t nb_sample = ioData->mDataByteSize >> 1;
+	uint32_t nb_sample = ioData->mBuffers[0].mDataByteSize >> 1;
 	uint32_t left = 0;
 	uint8_t *in, *out;
 
 	in = (uint8_t*)&audioBuffer[rd_ptr];
-	out = (uint8_t*)ioData->mData;
+	out = (uint8_t*)ioData->mBuffers[0].mData;
 	aprintf("[CoreAudio] Fill: rd %lu, wr %lu, nb asked %lu\n", rd_ptr, wr_ptr, nb_sample);
 
 	if(wr_ptr>rd_ptr)
@@ -141,13 +141,13 @@
 
 	OSStatus err;
 	ComponentDescription desc;
-	AudioUnitInputCallback input;
+	AURenderCallbackStruct input;
 	AudioStreamBasicDescription streamFormat;
 	AudioDeviceID theDevice;
 
-	desc.componentType = kAudioUnitComponentType;
-	desc.componentSubType = kAudioUnitSubType_Output;
-	desc.componentManufacturer = kAudioUnitID_DefaultOutput;
+	desc.componentType = kAudioUnitType_Output;
+	desc.componentSubType = kAudioUnitSubType_HALOutput;
+	desc.componentManufacturer = kAudioUnitManufacturer_Apple;
 	desc.componentFlags = 0;
 	desc.componentFlagsMask = 0;
 
@@ -170,7 +170,7 @@
 	input.inputProcRefCon = NULL;
 	
 	err = AudioUnitSetProperty(theOutputUnit, 
-					kAudioUnitProperty_SetInputCallback, 
+					kAudioUnitProperty_SetRenderCallback,
 					kAudioUnitScope_Global,
 					0,
 					&input, 



From gruntster at mail.berlios.de  Tue Dec  4 01:20:11 2007
From: gruntster at mail.berlios.de (gruntster at mail.berlios.de)
Date: Tue, 4 Dec 2007 01:20:11 +0100
Subject: [Avidemux-svn-commit] r3720 - in branches/avidemux_2.4_branch: .
	avidemux/ADM_libraries/ADM_lavcodec
	avidemux/ADM_libraries/ADM_lavcodec/ppc
	avidemux/ADM_libraries/ADM_libmpeg2enc
Message-ID: <200712040020.lB40KBuM019102@sheep.berlios.de>

Author: gruntster
Date: 2007-12-04 01:19:48 +0100 (Tue, 04 Dec 2007)
New Revision: 3720

Added:
   branches/avidemux_2.4_branch/avidemux/ADM_libraries/ADM_lavcodec/ppc/float_altivec.c
   branches/avidemux_2.4_branch/avidemux/ADM_libraries/ADM_lavcodec/ppc/h264_altivec.c
   branches/avidemux_2.4_branch/avidemux/ADM_libraries/ADM_lavcodec/ppc/h264_template_altivec.c
   branches/avidemux_2.4_branch/avidemux/ADM_libraries/ADM_lavcodec/ppc/int_altivec.c
   branches/avidemux_2.4_branch/avidemux/ADM_libraries/ADM_lavcodec/ppc/mathops.h
   branches/avidemux_2.4_branch/avidemux/ADM_libraries/ADM_lavcodec/ppc/types_altivec.h
   branches/avidemux_2.4_branch/avidemux/ADM_libraries/ADM_lavcodec/ppc/vc1dsp_altivec.c
Removed:
   branches/avidemux_2.4_branch/avidemux/ADM_libraries/ADM_lavcodec/ppc/dsputil_h264_altivec.c
   branches/avidemux_2.4_branch/avidemux/ADM_libraries/ADM_lavcodec/ppc/dsputil_h264_template_altivec.c
Modified:
   branches/avidemux_2.4_branch/ConfigureChecks.cmake
   branches/avidemux_2.4_branch/avidemux/ADM_libraries/ADM_lavcodec/CMakeLists.txt
   branches/avidemux_2.4_branch/avidemux/ADM_libraries/ADM_lavcodec/ppc/dsputil_altivec.c
   branches/avidemux_2.4_branch/avidemux/ADM_libraries/ADM_lavcodec/ppc/dsputil_altivec.h
   branches/avidemux_2.4_branch/avidemux/ADM_libraries/ADM_lavcodec/ppc/dsputil_ppc.c
   branches/avidemux_2.4_branch/avidemux/ADM_libraries/ADM_lavcodec/ppc/dsputil_ppc.h
   branches/avidemux_2.4_branch/avidemux/ADM_libraries/ADM_lavcodec/ppc/fdct_altivec.c
   branches/avidemux_2.4_branch/avidemux/ADM_libraries/ADM_lavcodec/ppc/fft_altivec.c
   branches/avidemux_2.4_branch/avidemux/ADM_libraries/ADM_lavcodec/ppc/gmc_altivec.c
   branches/avidemux_2.4_branch/avidemux/ADM_libraries/ADM_lavcodec/ppc/idct_altivec.c
   branches/avidemux_2.4_branch/avidemux/ADM_libraries/ADM_lavcodec/ppc/mpegvideo_altivec.c
   branches/avidemux_2.4_branch/avidemux/ADM_libraries/ADM_lavcodec/ppc/mpegvideo_ppc.c
   branches/avidemux_2.4_branch/avidemux/ADM_libraries/ADM_lavcodec/ppc/snow_altivec.c
   branches/avidemux_2.4_branch/avidemux/ADM_libraries/ADM_libmpeg2enc/CMakeLists.txt
   branches/avidemux_2.4_branch/config.h.cmake
Log:
[AltiVec] update libavcodec's AltiVec code + add AltiVec support for CMake build

Modified: branches/avidemux_2.4_branch/ConfigureChecks.cmake
===================================================================
--- branches/avidemux_2.4_branch/ConfigureChecks.cmake	2007-12-03 12:38:06 UTC (rev 3719)
+++ branches/avidemux_2.4_branch/ConfigureChecks.cmake	2007-12-04 00:19:48 UTC (rev 3720)
@@ -1,525 +1,543 @@
-########################################
-# CPU and Host
-########################################
-MESSAGE(STATUS "Checking CPU and OS")
-INCLUDE(CMakeDetermineSystem)
-
-IF (WIN32)
-	SET(ADM_OS_WINDOWS 1)
-
-	IF (${CMAKE_SYSTEM_PROCESSOR} STREQUAL "x86")
-		SET(ADM_CPU_X86 1)
-	ENDIF (${CMAKE_SYSTEM_PROCESSOR} STREQUAL "x86")
-ELSEIF (APPLE)
-	SET(ADM_OS_APPLE 1)
-
-	IF (${CMAKE_SYSTEM_PROCESSOR} STREQUAL "i386")
-		SET(ADM_CPU_X86 1)
-	ELSEIF (${CMAKE_SYSTEM_PROCESSOR} STREQUAL "powerpc" OR ${CMAKE_SYSTEM_PROCESSOR} STREQUAL "Power Macintosh")
-		SET(ADM_CPU_PPC 1)
-	ENDIF (${CMAKE_SYSTEM_PROCESSOR} STREQUAL "i386")
-ELSEIF (UNIX)
-	SET(ADM_OS_UNIX 1)
-
-	IF (${CMAKE_SYSTEM_PROCESSOR} STREQUAL "i586" OR ${CMAKE_SYSTEM_PROCESSOR} STREQUAL "i686")
-		SET(ADM_CPU_X86 1)
-	ELSEIF (${CMAKE_SYSTEM_PROCESSOR} STREQUAL "x86_64" OR ${CMAKE_SYSTEM_PROCESSOR} STREQUAL "amd64")
-		SET(ADM_CPU_X86_64 1)
-	ELSEIF (${CMAKE_SYSTEM_PROCESSOR} STREQUAL "ppc")
-		SET(ADM_CPU_PPC 1)
-	ENDIF (${CMAKE_SYSTEM_PROCESSOR} STREQUAL "i586" OR ${CMAKE_SYSTEM_PROCESSOR} STREQUAL "i686")
-ENDIF (WIN32)
-
-# Various defines (needs to be removed from Avidemux code one day and be library specific...)
-IF (ADM_CPU_X86)
-	SET(ARCH_X86 1)
-	SET(ARCH_X86_32 1)
-ELSEIF (ADM_CPU_X86_64)
-	SET(ARCH_X86 1)
-	SET(ARCH_X86_64 1)
-	SET(ARCH_64_BITS 1)
-ELSEIF (ADM_CPU_PPC)
-	SET(ADM_BIG_ENDIAN 1)
-	SET(WORDS_BIGENDIAN 1)
-ENDIF (ADM_CPU_X86)
-
-########################################
-# Include CMake scripts
-########################################
-INCLUDE(CheckIncludeFiles)
-INCLUDE(CheckSymbolExists)
-INCLUDE(CheckFunctionExists)
-INCLUDE(CheckLibraryExists)
-INCLUDE(lavcodec)
-INCLUDE(adm_checkHeaderLib)
-INCLUDE(adm_compile)
-
-########################################
-# Mangling
-########################################
-IF (ADM_OS_WINDOWS OR ADM_OS_APPLE)
-	SET(CYG_MANGLING 1)
-ENDIF (ADM_OS_WINDOWS OR ADM_OS_APPLE)
-
-########################################
-# Avidemux OS specific tweaks
-########################################
-IF (ADM_OS_WINDOWS)
-	SET(ADM_WIN32 1)		# needs to be removed one day...
-	ADD_DEFINITIONS(-mms-bitfields -mno-cygwin)
-ELSE (ADM_OS_WINDOWS)
-	IF (ADM_OS_APPLE)
-		SET(CFLAGS_ORIG $ENV{CFLAGS})
-		SET(CXXFLAGS_ORIG $ENV{CXXFLAGS})
-		
-		SET(ENV{CFLAGS} "-I/opt/local/include -L/opt/local/lib $ENV{CFLAGS}")
-		SET(ENV{CXXFLAGS} "-I/opt/local/include -L/opt/local/lib $ENV{CXXFLAGS}")
-
-		LINK_DIRECTORIES(/opt/local/lib)
-
-		SET(ADM_BSD_FAMILY 1)
-	ENDIF (ADM_OS_APPLE)
-
-	CHECK_FUNCTION_EXISTS(chmod HAVE_CHMOD)         # __homedir/homedir.cpp, gpg/gpg.cpp
-ENDIF (ADM_OS_WINDOWS)
-
-# Jog shuttle is only available on linux due to its interface
-IF(CMAKE_SYSTEM_NAME STREQUAL "Linux")
-        SET(USE_JOG 1)
-ENDIF(CMAKE_SYSTEM_NAME STREQUAL "Linux")
-
-########################################
-# Standard Avidemux defines
-########################################
-SET(VERSION 2.4)
-SET(PACKAGE_VERSION 2.4)
-
-SET(HAVE_BUILTIN_VECTOR 1)
-SET(HAVE_AUDIO 1)
-
-SET(USE_MP3 1)
-SET(USE_AC3 1)
-SET(USE_FFMPEG 1)
-SET(USE_MJPEG 1)
-SET(USE_LIBXML2 1)
-
-########################################
-# Check functions, includes, symbols
-########################################
-CHECK_FUNCTION_EXISTS(gettimeofday HAVE_GETTIMEOFDAY)
-
-CHECK_INCLUDE_FILES(inttypes.h      HAVE_INTTYPES_H)                    # simapi.h
-CHECK_INCLUDE_FILES(stddef.h        HAVE_STDDEF_H)                      # simapi.h
-CHECK_INCLUDE_FILES(stdint.h        HAVE_STDINT_H)                      # simapi.h
-CHECK_INCLUDE_FILES(stdlib.h        HAVE_STDLIB_H)                      # simapi.h
-CHECK_INCLUDE_FILES(string.h        HAVE_STRING_H)                      # _core/libintl.cpp
-CHECK_INCLUDE_FILES(sys/stat.h      HAVE_SYS_STAT_H)                    # gpg/gpg.cpp
-CHECK_INCLUDE_FILES(sys/types.h     HAVE_SYS_TYPES_H)                   # simapi.h
-CHECK_INCLUDE_FILES(unistd.h        HAVE_UNISTD_H)                      # simapi.h
-CHECK_INCLUDE_FILES(malloc.h        HAVE_MALLOC_H)                      # simapi.h
-
-CHECK_SYMBOL_EXISTS(strcasecmp  "strings.h"         HAVE_STRCASECMP)    # simapi.h, various
-
-########################################
-# LibMad
-########################################
-IF (ADM_CPU_X86)
-	SET(FPM_INTEL 1)
-ELSEIF (ADM_CPU_X86_64)
-	SET(FPM_DEFAULT 1)
-ELSEIF (ADM_CPU_PPC)
-	SET(FPM_PPC 1)
-ENDIF (ADM_CPU_X86)
-
-########################################
-# Libavcodec
-########################################
-SET_LAVCODEC_FLAGS()
-
-########################################
-# LibMpeg2Dec
-########################################
-SET(ACCEL_DETECT 1)
-
-########################################
-# Gettext
-########################################
-MESSAGE(STATUS "<Checking gettext>")
-MESSAGE(STATUS "<****************>")
-
-IF (NO_NLS)
-	MESSAGE(STATUS "<disabled per request>")
-ELSE (NO_NLS)
-	FIND_PATH(LIBINTL_H_DIR libintl.h $ENV{CXXFLAGS})
-	MESSAGE(STATUS "libintl Header Path: ${LIBINTL_H_DIR}")
-
-	IF (NOT LIBINTL_H_DIR STREQUAL "LIBINTL_H-NOTFOUND")
-		FIND_LIBRARY(LIBINTL_LIB_DIR intl $ENV{CXXFLAGS})
-		MESSAGE(STATUS "libintl Library Path: ${LIBINTL_LIB_DIR}")
-
-		# Try linking without -lintl
-		ADM_COMPILE(gettext.cpp -I${LIBINTL_H_DIR} "" WITHOUT_LIBINTL outputWithoutLibintl)
-		
-		IF (WITHOUT_LIBINTL)
-			SET(HAVE_GETTEXT 1)
-			MESSAGE(STATUS "Ok, No lib needed (${ADM_GETTEXT_LIB})")
-		ELSE (WITHOUT_LIBINTL)
-			ADM_COMPILE(gettext.cpp -I${LIBINTL_H_DIR} ${LIBINTL_LIB_DIR} WITH_LIBINTL outputWithLibintl)
-			
-			IF (WITH_LIBINTL)
-				SET(ADM_GETTEXT_LIB ${LIBINTL_LIB_DIR})
-				SET(HAVE_GETTEXT 1)
-				
-				MESSAGE(STATUS "Ok, libintl needed")
-			ELSE (WITH_LIBINTL)
-				MESSAGE(STATUS "Does not work, without ${outputWithoutLibintl}")
-				MESSAGE(STATUS "Does not work, with ${outputWithLibintl}")
-			ENDIF (WITH_LIBINTL)
-		ENDIF (WITHOUT_LIBINTL)
-	ENDIF (NOT LIBINTL_H_DIR STREQUAL "LIBINTL_H-NOTFOUND")
-
-	IF (HAVE_GETTEXT)
-		SET(ADM_GETTEXT_INCLUDE -I${LIBINTL_H_DIR})
-	ENDIF(HAVE_GETTEXT)
-ENDIF (NO_NLS)
-
-########################################
-# Locale
-########################################
-SET(ADM_LOCALE "${CMAKE_INSTALL_PREFIX}/share/locale")
-
-########################################
-# ALSA
-########################################
-IF (ADM_OS_UNIX)
-	MESSAGE(STATUS "<Checking for ALSA>")
-	MESSAGE(STATUS "<*****************>")
-	IF (NO_ALSA)
-		MESSAGE(STATUS "<disabled per request>")
-	ELSE (NO_ALSA)
-		INCLUDE(FindAlsa)
-		
-		IF (ALSA_FOUND)
-			ALSA_VERSION_STRING(alsaVersion)
-			
-			MESSAGE("Found alsa version :${alsaVersion}") 
-			MESSAGE("Found alsa lib     :${ASOUND_LIBRARY}")
-			
-			SET(ALSA_SUPPORT 1)
-			SET(ALSA_1_0_SUPPORT 1)
-		ENDIF (ALSA_FOUND)
-	ENDIF (NO_ALSA)
-ENDIF (ADM_OS_UNIX)
-
-########################################
-# SDL
-########################################
-MESSAGE(STATUS "<Checking for SDL>")
-MESSAGE(STATUS "<*****************>")
-
-INCLUDE(admSDL)
-
-IF (NO_SDL)
-	MESSAGE(STATUS "<disabled per request>")
-ELSE (NO_SDL)
-	include(FindSDL)
-
-	IF (SDL_FOUND)
-		SET(USE_SDL 1)
-		
-		MESSAGE(STATUS "Found")
-	ELSE (SDL_FOUND)
-		MESSAGE(STATUS "Not Found")
-	ENDIF (SDL_FOUND)
-	
-	MESSAGE(STATUS "Flags: -I${SDL_INCLUDE_DIR}")
-	MESSAGE(STATUS "Libraries: ${SDL_LIBRARY}")
-ENDIF (NO_SDL)
-
-########################################
-# FONTCONFIG
-########################################
-ADM_CHECK_HL(FontConfig fontconfig/fontconfig.h fontconfig FcStrSetCreate USE_FONTCONFIG)
-
-IF (USE_FONTCONFIG)
-	SET(HAVE_FONTCONFIG 1)
-ENDIF (USE_FONTCONFIG)
-
-########################################
-# Xvideo
-########################################
-IF (NOT NO_XV AND NOT ADM_OS_WINDOWS)
-	SET(CMAKE_REQUIRED_FLAGS "-include X11/Xlib.h")
-	SET(CMAKE_REQUIRED_LIBRARIES "${X11_LIBRARIES}")
-	SET(CMAKE_REQUIRED_INCLUDE "${X11_INCLUDE_DIR}")
-
-	ADM_CHECK_HL(Xvideo X11/extensions/Xvlib.h Xv XvShmPutImage USE_XV)
-	
-	MESSAGE(STATUS "Flags: ${CMAKE_REQUIRED_FLAGS} ${X11_INCLUDE_DIR}")
-	MESSAGE(STATUS "Libraries: ${X11_LIBRARIES}")
-
-	SET(CMAKE_REQUIRED_FLAGS)
-	SET(CMAKE_REQUIRED_LIBRARIES)
-	SET(CMAKE_REQUIRED_INCLUDE)
-ENDIF (NOT NO_XV AND NOT ADM_OS_WINDOWS)
-
-########################################
-# OSS
-########################################
-IF (NOT ADM_OS_WINDOWS)
-	MESSAGE(STATUS "<Checking for OSS>")
-	MESSAGE(STATUS "<*****************>")
-	
-	IF (NO_OSS)
-		MESSAGE(STATUS "<disabled per request>")
-	ELSE (NO_OSS)
-		CHECK_INCLUDE_FILES(sys/soundcard.h OSS_SUPPORT)
-		
-		IF (OSS_SUPPORT)
-			MESSAGE(STATUS "Found")
-		ELSE (OSS_SUPPORT)
-			MESSAGE(STATUS "Not found")
-		ENDIF (OSS_SUPPORT)
-	ENDIF (NO_OSS)
-ENDIF (NOT ADM_OS_WINDOWS)
-
-########################################
-# ARTS
-########################################
-IF (NOT ADM_OS_WINDOWS)
-	INCLUDE(FindArts)
-ENDIF (NOT ADM_OS_WINDOWS)
-
-########################################
-# ESD
-########################################
-IF (NOT ADM_OS_WINDOWS)
-	ADM_CHECK_HL(Esd esd.h  esd esd_close USE_ESD)
-ENDIF (NOT ADM_OS_WINDOWS)
-
-########################################
-# JACK
-########################################
-IF (NOT ADM_OS_WINDOWS)
-	ADM_CHECK_HL(Jack jack/jack.h  jack jack_client_close USE_JACK)
-ENDIF (NOT ADM_OS_WINDOWS)
-
-########################################
-# Aften
-########################################
-IF (ADM_OS_WINDOWS)
-	SET(CMAKE_REQUIRED_FLAGS "-lm -lpthreadGC2")
-ELSE (ADM_OS_WINDOWS)
-	SET(CMAKE_REQUIRED_FLAGS "-lm -lpthread")
-ENDIF (ADM_OS_WINDOWS)
-
-ADM_CHECK_HL(Aften aften/aften.h aften aften_encode_init USE_AFTEN)
-
-IF (USE_AFTEN)
-	FIND_LIBRARY(AFTEN_LIB_PATH NAMES aften $ENV{CXXFLAGS})
-
-	TRY_RUN(AFTEN_TEST_RUN_RESULT
-		AFTEN_TEST_COMPILE_RESULT
-		${CMAKE_BINARY_DIR}
-		"${CMAKE_SOURCE_DIR}/cmake_compile_check/aften_check.cpp"
-		CMAKE_FLAGS -DLINK_LIBRARIES=${AFTEN_LIB_PATH})
-
-	IF (AFTEN_TEST_RUN_RESULT EQUAL 8)
-		MESSAGE(STATUS "Aften Version: 0.0.8")
-		SET(USE_AFTEN_08 1)
-	ELSEIF (AFTEN_TEST_RUN_RESULT EQUAL 7)
-		MESSAGE(STATUS "Aften Version: 0.07")
-		SET(USE_AFTEN_07 1)
-	ELSE (AFTEN_TEST_RUN_RESULT EQUAL 8)
-		MESSAGE(STATUS "Warning: Unable to determine Aften version - support for Aften will be turned off")
-		SET(USE_AFTEN 0)
-	ENDIF (AFTEN_TEST_RUN_RESULT EQUAL 8)
-ENDIF (USE_AFTEN)
-
-SET(CMAKE_REQUIRED_FLAGS "")
-
-########################################
-# Secret Rabbit Code
-########################################
-ADM_CHECK_HL(libsamplerate samplerate.h samplerate src_get_version USE_SRC)
-
-########################################
-# ICONV
-########################################
-MESSAGE(STATUS "<Checking for iconv.h>")
-MESSAGE(STATUS "<******************************>")
-
-CHECK_INCLUDE_FILES(iconv.h HAVE_ICONV_H)
-
-IF (NOT HAVE_ICONV_H)
-	MESSAGE(FATAL "iconv.h not found")
-ENDIF (NOT HAVE_ICONV_H)
-
-# need libiconv ?
-CHECK_LIBRARY_EXISTS(iconv libiconv "" LINK_WITH_ICONV)
-
-IF (LINK_WITH_ICONV)
-	SET(NEED_LIBICONV 1)
-	MESSAGE(STATUS "libiconv found, probably needed")
-ELSE (LINK_WITH_ICONV)
-	MESSAGE(STATUS "libiconv not found, probably not needed")
-ENDIF (LINK_WITH_ICONV)
-
-MESSAGE(STATUS "<Checking if iconv needs const>")
-
-IF (NEED_LIBICONV)
-	ADM_COMPILE_WITH_WITHOUT(iconv_check.cpp "-DICONV_NEED_CONST" "-liconv" ICONV_WITH)
-ELSE (NEED_LIBICONV)
-	ADM_COMPILE_WITH_WITHOUT(iconv_check.cpp "-DICONV_NEED_CONST" "-lm" ICONV_WITH)
-ENDIF (NEED_LIBICONV)
-
-IF (ICONV_WITH)
-	MESSAGE(STATUS "Yes")
-	SET(ICONV_NEED_CONST 1)
-ELSE (ICONV_WITH)
-    MESSAGE(STATUS "No")
-ENDIF(ICONV_WITH)
-
-########################################
-# LAME
-########################################
-SET(CMAKE_REQUIRED_LIBRARIES "-lm")
-ADM_CHECK_HL(Lame lame/lame.h mp3lame lame_init HAVE_LIBMP3LAME)
-SET(CMAKE_REQUIRED_LIBRARIES)
-
-########################################
-# Xvid
-########################################
-ADM_CHECK_HL(Xvid xvid.h xvidcore xvid_plugin_single USE_XVID_4)
-
-########################################
-# AMR_NB
-########################################
-IF (USE_LATE_BINDING)
-	CHECK_INCLUDE_FILES(amrnb/interf_dec.h USE_AMR_NB)
-ELSE (USE_LATE_BINDING)
-	ADM_CHECK_HL(AMRNB amrnb/interf_dec.h amrnb GP3Decoder_Interface_Decode USE_AMR_NB)
-ENDIF (USE_LATE_BINDING)
-
-IF (USE_AMR_NB)
-	SET(CONFIG_AMR_NB 1)
-ENDIF (USE_AMR_NB)
-
-########################################
-# Libdca
-########################################
-SET(CMAKE_REQUIRED_FLAGS "-include stdint.h")
-SET(CMAKE_REQUIRED_LIBRARIES "-lm")
-
-IF (USE_LATE_BINDING)
-	CHECK_INCLUDE_FILES(dts.h USE_LIBDCA)
-ELSE (USE_LATE_BINDING)
-	ADM_CHECK_HL(libdca dts.h dts dts_init USE_LIBDCA)
-ENDIF (USE_LATE_BINDING)
-
-SET(CMAKE_REQUIRED_LIBRARIES)
-SET(CMAKE_REQUIRED_FLAGS)
-
-########################################
-# X264
-########################################
-SET(CMAKE_REQUIRED_FLAGS "-include stdint.h")
-
-IF (ADM_OS_WINDOWS)
-	SET(CMAKE_REQUIRED_LIBRARIES "-lm -lpthreadGC2")
-ELSE (ADM_OS_WINDOWS)
-	SET(CMAKE_REQUIRED_LIBRARIES "-lm -lpthread")
-ENDIF (ADM_OS_WINDOWS)
-
-ADM_CHECK_HL(x264 x264.h x264 x264_encoder_open USE_X264)
-SET(CMAKE_REQUIRED_FLAGS)
-SET(CMAKE_REQUIRED_LIBRARIES)
-
-########################################
-# PNG
-########################################
-ADM_CHECK_HL(libPNG png.h png png_malloc USE_PNG)
-
-########################################
-# FAAD
-########################################
-ADM_CHECK_HL(FAAD faad.h faad faacDecInit USE_FAAD_P)
-
-IF(NOT USE_FAAD_P)
-	MESSAGE(STATUS "Trying neaac variant")
-	ADM_CHECK_HL(NeAAC faad.h faad NeAACDecInit USE_FAAD_A)
-ENDIF(NOT USE_FAAD_P)
-
-IF(USE_FAAD_P OR USE_FAAD_A)
-	SET(USE_FAAD  1)
-ENDIF(USE_FAAD_P OR USE_FAAD_A)
-
-# See if we need old FAAD or new
-IF (USE_FAAD)
-	FIND_PATH(FAAD_H_DIR faad.h $ENV{CXXFLAGS})
-	FIND_LIBRARY(FAAD_LIB_DIR faad $ENV{CXXFLAGS})
-	
-	MESSAGE(STATUS "<Checking if faad needs old proto>")
-	ADM_COMPILE_WITH_WITHOUT(faad_check.cpp "-DOLD_FAAD_PROTO -I${FAAD_H_DIR}" "${FAAD_LIB_DIR}" FAAD_WITH)
-	
-	IF(FAAD_WITH)
-		MESSAGE(STATUS "Yes")
-		SET(OLD_FAAD_PROTO 1)
-	ELSE (FAAD_WITH)
-		MESSAGE(STATUS "No")
-	ENDIF (FAAD_WITH)
-ENDIF(USE_FAAD)
-
-########################################
-# FAAC
-########################################
-SET(CMAKE_REQUIRED_FLAGS "-include stdint.h")
-ADM_CHECK_HL(FAAC faac.h faac faacEncClose USE_FAAC)
-SET(CMAKE_REQUIRED_FLAGS)
-
-########################################
-# FreeType
-########################################
-IF (FT_FOUND)
-	SET(USE_FREETYPE 1)
-ENDIF (FT_FOUND)
-
-########################################
-# Vorbis
-########################################
-ADM_CHECK_HL(Vorbis vorbis/vorbisenc.h vorbis vorbis_info_init USE_VORBIS1)
-ADM_CHECK_HL(Vorbis vorbis/vorbisenc.h vorbisenc vorbis_encode_init USE_VORBIS2)
-
-IF (USE_VORBIS1 AND USE_VORBIS2)
-	SET(USE_VORBIS 1)
-ENDIF (USE_VORBIS1 AND USE_VORBIS2)
-
-########################################
-# End test
-########################################
-ADM_CHECK_HL(Invalid dummy_header.h dummy_libxyz dummy_func_tyu DUMMY_TEST)
-ADM_CHECK_HL(Invalid stdio.h dummy_libxyz dummy_func_tyu DUMMY_TEST2)
-
-IF (DUMMY_TEST OR DUMMY_TEST2)
-	MESSAGE(FATAL "This test should have failed!!!")
-	MESSAGE(FATAL "This test should have failed!!!")
-	MESSAGE(FATAL "This test should have failed!!!")
-ENDIF (DUMMY_TEST OR DUMMY_TEST2)
-
-INCLUDE(adm_log)
-
-IF (NOT CMAKE_BUILD_TYPE)
-	SET(CMAKE_BUILD_TYPE "Release")
-ENDIF (NOT CMAKE_BUILD_TYPE)
-
-IF (CMAKE_BUILD_TYPE STREQUAL "Debug")
-	MESSAGE(STATUS "** DEBUG BUILD (${CMAKE_BUILD_TYPE})**")
-	
-	SET(ADM_DEBUG 1)
-	ADD_DEFINITIONS(-DADM_DEBUG)
-ELSE (CMAKE_BUILD_TYPE STREQUAL "Debug")
-	MESSAGE(STATUS "** RELEASE BUILD (${CMAKE_BUILD_TYPE})**")
-ENDIF(CMAKE_BUILD_TYPE STREQUAL "Debug")
-
-MESSAGE("LINK_FLAGS ${CMAKE_LD_FLAGS}")
-# EOF
+########################################
+# CPU and Host
+########################################
+MESSAGE(STATUS "Checking CPU and OS")
+INCLUDE(CMakeDetermineSystem)
+
+IF (WIN32)
+	SET(ADM_OS_WINDOWS 1)
+
+	IF (${CMAKE_SYSTEM_PROCESSOR} STREQUAL "x86")
+		SET(ADM_CPU_X86 1)
+	ENDIF (${CMAKE_SYSTEM_PROCESSOR} STREQUAL "x86")
+ELSEIF (APPLE)
+	SET(ADM_OS_APPLE 1)
+
+	IF (${CMAKE_SYSTEM_PROCESSOR} STREQUAL "i386")
+		SET(ADM_CPU_X86 1)
+	ELSEIF (${CMAKE_SYSTEM_PROCESSOR} STREQUAL "powerpc" OR ${CMAKE_SYSTEM_PROCESSOR} STREQUAL "Power Macintosh")
+		SET(ADM_CPU_PPC 1)
+	ENDIF (${CMAKE_SYSTEM_PROCESSOR} STREQUAL "i386")
+ELSEIF (UNIX)
+	SET(ADM_OS_UNIX 1)
+
+	IF (${CMAKE_SYSTEM_PROCESSOR} STREQUAL "i586" OR ${CMAKE_SYSTEM_PROCESSOR} STREQUAL "i686")
+		SET(ADM_CPU_X86 1)
+	ELSEIF (${CMAKE_SYSTEM_PROCESSOR} STREQUAL "x86_64" OR ${CMAKE_SYSTEM_PROCESSOR} STREQUAL "amd64")
+		SET(ADM_CPU_X86_64 1)
+	ELSEIF (${CMAKE_SYSTEM_PROCESSOR} STREQUAL "ppc")
+		SET(ADM_CPU_PPC 1)
+	ENDIF (${CMAKE_SYSTEM_PROCESSOR} STREQUAL "i586" OR ${CMAKE_SYSTEM_PROCESSOR} STREQUAL "i686")
+ENDIF (WIN32)
+
+# Various defines (needs to be removed from Avidemux code one day and be library specific...)
+IF (ADM_CPU_X86)
+	SET(ARCH_X86 1)
+	SET(ARCH_X86_32 1)
+ELSEIF (ADM_CPU_X86_64)
+	SET(ARCH_X86 1)
+	SET(ARCH_X86_64 1)
+	SET(ARCH_64_BITS 1)
+ELSEIF (ADM_CPU_PPC)
+	OPTION(ALTIVEC "" ON)
+
+	SET(ADM_BIG_ENDIAN 1)
+	SET(WORDS_BIGENDIAN 1)
+
+	IF (ALTIVEC)
+		SET(BUILD_ALTIVEC 1)
+		SET(ARCH_PPC 1)
+		SET(ARCH_POWERPC 1)
+		SET(USE_ALTIVEC 1)
+		SET(HAVE_ALTIVEC 1)
+
+		SET(CMAKE_C_FLAGS "${CMAKE_C_FLAGS} -mabi=altivec -maltivec")
+		SET(CMAKE_CXX_FLAGS "${CMAKE_CXX_FLAGS} -mabi=altivec -maltivec")
+
+		IF (ADM_OS_APPLE)
+			SET(CMAKE_C_FLAGS "${CMAKE_C_FLAGS} -faltivec -force_cpusubtype_ALL")
+			SET(CMAKE_CXX_FLAGS "${CMAKE_CXX_FLAGS} -faltivec -force_cpusubtype_ALL")
+		ENDIF (ADM_OS_APPLE)
+	ENDIF (ALTIVEC)
+ENDIF (ADM_CPU_X86)
+
+########################################
+# Include CMake scripts
+########################################
+INCLUDE(CheckIncludeFiles)
+INCLUDE(CheckSymbolExists)
+INCLUDE(CheckFunctionExists)
+INCLUDE(CheckLibraryExists)
+INCLUDE(lavcodec)
+INCLUDE(adm_checkHeaderLib)
+INCLUDE(adm_compile)
+
+########################################
+# Mangling
+########################################
+IF (ADM_OS_WINDOWS OR ADM_OS_APPLE)
+	SET(CYG_MANGLING 1)
+ENDIF (ADM_OS_WINDOWS OR ADM_OS_APPLE)
+
+########################################
+# Avidemux OS specific tweaks
+########################################
+IF (ADM_OS_WINDOWS)
+	SET(ADM_WIN32 1)		# needs to be removed one day...
+	ADD_DEFINITIONS(-mms-bitfields -mno-cygwin)
+ELSE (ADM_OS_WINDOWS)
+	IF (ADM_OS_APPLE)
+		SET(CFLAGS_ORIG $ENV{CFLAGS})
+		SET(CXXFLAGS_ORIG $ENV{CXXFLAGS})
+		
+		SET(ENV{CFLAGS} "-I/opt/local/include -L/opt/local/lib $ENV{CFLAGS}")
+		SET(ENV{CXXFLAGS} "-I/opt/local/include -L/opt/local/lib $ENV{CXXFLAGS}")
+
+		LINK_DIRECTORIES(/opt/local/lib)
+
+		SET(ADM_BSD_FAMILY 1)
+	ENDIF (ADM_OS_APPLE)
+
+	CHECK_FUNCTION_EXISTS(chmod HAVE_CHMOD)         # __homedir/homedir.cpp, gpg/gpg.cpp
+ENDIF (ADM_OS_WINDOWS)
+
+# Jog shuttle is only available on linux due to its interface
+IF(CMAKE_SYSTEM_NAME STREQUAL "Linux")
+        SET(USE_JOG 1)
+ENDIF(CMAKE_SYSTEM_NAME STREQUAL "Linux")
+
+########################################
+# Standard Avidemux defines
+########################################
+SET(VERSION 2.4)
+SET(PACKAGE_VERSION 2.4)
+
+SET(HAVE_BUILTIN_VECTOR 1)
+SET(HAVE_AUDIO 1)
+
+SET(USE_MP3 1)
+SET(USE_AC3 1)
+SET(USE_FFMPEG 1)
+SET(USE_MJPEG 1)
+SET(USE_LIBXML2 1)
+
+########################################
+# Check functions, includes, symbols
+########################################
+CHECK_FUNCTION_EXISTS(gettimeofday HAVE_GETTIMEOFDAY)
+
+CHECK_INCLUDE_FILES(inttypes.h      HAVE_INTTYPES_H)                    # simapi.h
+CHECK_INCLUDE_FILES(stddef.h        HAVE_STDDEF_H)                      # simapi.h
+CHECK_INCLUDE_FILES(stdint.h        HAVE_STDINT_H)                      # simapi.h
+CHECK_INCLUDE_FILES(stdlib.h        HAVE_STDLIB_H)                      # simapi.h
+CHECK_INCLUDE_FILES(string.h        HAVE_STRING_H)                      # _core/libintl.cpp
+CHECK_INCLUDE_FILES(sys/stat.h      HAVE_SYS_STAT_H)                    # gpg/gpg.cpp
+CHECK_INCLUDE_FILES(sys/types.h     HAVE_SYS_TYPES_H)                   # simapi.h
+CHECK_INCLUDE_FILES(unistd.h        HAVE_UNISTD_H)                      # simapi.h
+CHECK_INCLUDE_FILES(malloc.h        HAVE_MALLOC_H)                      # simapi.h
+
+CHECK_SYMBOL_EXISTS(strcasecmp  "strings.h"         HAVE_STRCASECMP)    # simapi.h, various
+
+########################################
+# LibMad
+########################################
+IF (ADM_CPU_X86)
+	SET(FPM_INTEL 1)
+ELSEIF (ADM_CPU_X86_64)
+	SET(FPM_DEFAULT 1)
+ELSEIF (ADM_CPU_PPC)
+	SET(FPM_PPC 1)
+ENDIF (ADM_CPU_X86)
+
+########################################
+# Libavcodec
+########################################
+SET_LAVCODEC_FLAGS()
+
+########################################
+# LibMpeg2Dec
+########################################
+SET(ACCEL_DETECT 1)
+
+########################################
+# Gettext
+########################################
+MESSAGE(STATUS "<Checking gettext>")
+MESSAGE(STATUS "<****************>")
+
+IF (NO_NLS)
+	MESSAGE(STATUS "<disabled per request>")
+ELSE (NO_NLS)
+	FIND_PATH(LIBINTL_H_DIR libintl.h $ENV{CXXFLAGS})
+	MESSAGE(STATUS "libintl Header Path: ${LIBINTL_H_DIR}")
+
+	IF (NOT LIBINTL_H_DIR STREQUAL "LIBINTL_H-NOTFOUND")
+		FIND_LIBRARY(LIBINTL_LIB_DIR intl $ENV{CXXFLAGS})
+		MESSAGE(STATUS "libintl Library Path: ${LIBINTL_LIB_DIR}")
+
+		# Try linking without -lintl
+		ADM_COMPILE(gettext.cpp -I${LIBINTL_H_DIR} "" WITHOUT_LIBINTL outputWithoutLibintl)
+		
+		IF (WITHOUT_LIBINTL)
+			SET(HAVE_GETTEXT 1)
+			MESSAGE(STATUS "Ok, No lib needed (${ADM_GETTEXT_LIB})")
+		ELSE (WITHOUT_LIBINTL)
+			ADM_COMPILE(gettext.cpp -I${LIBINTL_H_DIR} ${LIBINTL_LIB_DIR} WITH_LIBINTL outputWithLibintl)
+			
+			IF (WITH_LIBINTL)
+				SET(ADM_GETTEXT_LIB ${LIBINTL_LIB_DIR})
+				SET(HAVE_GETTEXT 1)
+				
+				MESSAGE(STATUS "Ok, libintl needed")
+			ELSE (WITH_LIBINTL)
+				MESSAGE(STATUS "Does not work, without ${outputWithoutLibintl}")
+				MESSAGE(STATUS "Does not work, with ${outputWithLibintl}")
+			ENDIF (WITH_LIBINTL)
+		ENDIF (WITHOUT_LIBINTL)
+	ENDIF (NOT LIBINTL_H_DIR STREQUAL "LIBINTL_H-NOTFOUND")
+
+	IF (HAVE_GETTEXT)
+		SET(ADM_GETTEXT_INCLUDE -I${LIBINTL_H_DIR})
+	ENDIF(HAVE_GETTEXT)
+ENDIF (NO_NLS)
+
+########################################
+# Locale
+########################################
+SET(ADM_LOCALE "${CMAKE_INSTALL_PREFIX}/share/locale")
+
+########################################
+# ALSA
+########################################
+IF (ADM_OS_UNIX)
+	MESSAGE(STATUS "<Checking for ALSA>")
+	MESSAGE(STATUS "<*****************>")
+	IF (NO_ALSA)
+		MESSAGE(STATUS "<disabled per request>")
+	ELSE (NO_ALSA)
+		INCLUDE(FindAlsa)
+		
+		IF (ALSA_FOUND)
+			ALSA_VERSION_STRING(alsaVersion)
+			
+			MESSAGE("Found alsa version :${alsaVersion}") 
+			MESSAGE("Found alsa lib     :${ASOUND_LIBRARY}")
+			
+			SET(ALSA_SUPPORT 1)
+			SET(ALSA_1_0_SUPPORT 1)
+		ENDIF (ALSA_FOUND)
+	ENDIF (NO_ALSA)
+ENDIF (ADM_OS_UNIX)
+
+########################################
+# SDL
+########################################
+MESSAGE(STATUS "<Checking for SDL>")
+MESSAGE(STATUS "<*****************>")
+
+INCLUDE(admSDL)
+
+IF (NO_SDL)
+	MESSAGE(STATUS "<disabled per request>")
+ELSE (NO_SDL)
+	include(FindSDL)
+
+	IF (SDL_FOUND)
+		SET(USE_SDL 1)
+		
+		MESSAGE(STATUS "Found")
+	ELSE (SDL_FOUND)
+		MESSAGE(STATUS "Not Found")
+	ENDIF (SDL_FOUND)
+	
+	MESSAGE(STATUS "Flags: -I${SDL_INCLUDE_DIR}")
+	MESSAGE(STATUS "Libraries: ${SDL_LIBRARY}")
+ENDIF (NO_SDL)
+
+########################################
+# FONTCONFIG
+########################################
+ADM_CHECK_HL(FontConfig fontconfig/fontconfig.h fontconfig FcStrSetCreate USE_FONTCONFIG)
+
+IF (USE_FONTCONFIG)
+	SET(HAVE_FONTCONFIG 1)
+ENDIF (USE_FONTCONFIG)
+
+########################################
+# Xvideo
+########################################
+IF (NOT NO_XV AND NOT ADM_OS_WINDOWS)
+	SET(CMAKE_REQUIRED_FLAGS "-include X11/Xlib.h")
+	SET(CMAKE_REQUIRED_LIBRARIES "${X11_LIBRARIES}")
+	SET(CMAKE_REQUIRED_INCLUDE "${X11_INCLUDE_DIR}")
+
+	ADM_CHECK_HL(Xvideo X11/extensions/Xvlib.h Xv XvShmPutImage USE_XV)
+	
+	MESSAGE(STATUS "Flags: ${CMAKE_REQUIRED_FLAGS} ${X11_INCLUDE_DIR}")
+	MESSAGE(STATUS "Libraries: ${X11_LIBRARIES}")
+
+	SET(CMAKE_REQUIRED_FLAGS)
+	SET(CMAKE_REQUIRED_LIBRARIES)
+	SET(CMAKE_REQUIRED_INCLUDE)
+ENDIF (NOT NO_XV AND NOT ADM_OS_WINDOWS)
+
+########################################
+# OSS
+########################################
+IF (NOT ADM_OS_WINDOWS)
+	MESSAGE(STATUS "<Checking for OSS>")
+	MESSAGE(STATUS "<*****************>")
+	
+	IF (NO_OSS)
+		MESSAGE(STATUS "<disabled per request>")
+	ELSE (NO_OSS)
+		CHECK_INCLUDE_FILES(sys/soundcard.h OSS_SUPPORT)
+		
+		IF (OSS_SUPPORT)
+			MESSAGE(STATUS "Found")
+		ELSE (OSS_SUPPORT)
+			MESSAGE(STATUS "Not found")
+		ENDIF (OSS_SUPPORT)
+	ENDIF (NO_OSS)
+ENDIF (NOT ADM_OS_WINDOWS)
+
+########################################
+# ARTS
+########################################
+IF (NOT ADM_OS_WINDOWS)
+	INCLUDE(FindArts)
+ENDIF (NOT ADM_OS_WINDOWS)
+
+########################################
+# ESD
+########################################
+IF (NOT ADM_OS_WINDOWS)
+	ADM_CHECK_HL(Esd esd.h  esd esd_close USE_ESD)
+ENDIF (NOT ADM_OS_WINDOWS)
+
+########################################
+# JACK
+########################################
+IF (NOT ADM_OS_WINDOWS)
+	ADM_CHECK_HL(Jack jack/jack.h  jack jack_client_close USE_JACK)
+ENDIF (NOT ADM_OS_WINDOWS)
+
+########################################
+# Aften
+########################################
+IF (ADM_OS_WINDOWS)
+	SET(CMAKE_REQUIRED_FLAGS "-lm -lpthreadGC2")
+ELSE (ADM_OS_WINDOWS)
+	SET(CMAKE_REQUIRED_FLAGS "-lm -lpthread")
+ENDIF (ADM_OS_WINDOWS)
+
+ADM_CHECK_HL(Aften aften/aften.h aften aften_encode_init USE_AFTEN)
+
+IF (USE_AFTEN)
+	FIND_LIBRARY(AFTEN_LIB_PATH NAMES aften $ENV{CXXFLAGS})
+
+	TRY_RUN(AFTEN_TEST_RUN_RESULT
+		AFTEN_TEST_COMPILE_RESULT
+		${CMAKE_BINARY_DIR}
+		"${CMAKE_SOURCE_DIR}/cmake_compile_check/aften_check.cpp"
+		CMAKE_FLAGS -DLINK_LIBRARIES=${AFTEN_LIB_PATH})
+
+	IF (AFTEN_TEST_RUN_RESULT EQUAL 8)
+		MESSAGE(STATUS "Aften Version: 0.0.8")
+		SET(USE_AFTEN_08 1)
+	ELSEIF (AFTEN_TEST_RUN_RESULT EQUAL 7)
+		MESSAGE(STATUS "Aften Version: 0.07")
+		SET(USE_AFTEN_07 1)
+	ELSE (AFTEN_TEST_RUN_RESULT EQUAL 8)
+		MESSAGE(STATUS "Warning: Unable to determine Aften version - support for Aften will be turned off")
+		SET(USE_AFTEN 0)
+	ENDIF (AFTEN_TEST_RUN_RESULT EQUAL 8)
+ENDIF (USE_AFTEN)
+
+SET(CMAKE_REQUIRED_FLAGS "")
+
+########################################
+# Secret Rabbit Code
+########################################
+ADM_CHECK_HL(libsamplerate samplerate.h samplerate src_get_version USE_SRC)
+
+########################################
+# ICONV
+########################################
+MESSAGE(STATUS "<Checking for iconv.h>")
+MESSAGE(STATUS "<******************************>")
+
+CHECK_INCLUDE_FILES(iconv.h HAVE_ICONV_H)
+
+IF (NOT HAVE_ICONV_H)
+	MESSAGE(FATAL "iconv.h not found")
+ENDIF (NOT HAVE_ICONV_H)
+
+# need libiconv ?
+CHECK_LIBRARY_EXISTS(iconv libiconv "" LINK_WITH_ICONV)
+
+IF (LINK_WITH_ICONV)
+	SET(NEED_LIBICONV 1)
+	MESSAGE(STATUS "libiconv found, probably needed")
+ELSE (LINK_WITH_ICONV)
+	MESSAGE(STATUS "libiconv not found, probably not needed")
+ENDIF (LINK_WITH_ICONV)
+
+MESSAGE(STATUS "<Checking if iconv needs const>")
+
+IF (NEED_LIBICONV)
+	ADM_COMPILE_WITH_WITHOUT(iconv_check.cpp "-DICONV_NEED_CONST" "-liconv" ICONV_WITH)
+ELSE (NEED_LIBICONV)
+	ADM_COMPILE_WITH_WITHOUT(iconv_check.cpp "-DICONV_NEED_CONST" "-lm" ICONV_WITH)
+ENDIF (NEED_LIBICONV)
+
+IF (ICONV_WITH)
+	MESSAGE(STATUS "Yes")
+	SET(ICONV_NEED_CONST 1)
+ELSE (ICONV_WITH)
+    MESSAGE(STATUS "No")
+ENDIF(ICONV_WITH)
+
+########################################
+# LAME
+########################################
+SET(CMAKE_REQUIRED_LIBRARIES "-lm")
+ADM_CHECK_HL(Lame lame/lame.h mp3lame lame_init HAVE_LIBMP3LAME)
+SET(CMAKE_REQUIRED_LIBRARIES)
+
+########################################
+# Xvid
+########################################
+ADM_CHECK_HL(Xvid xvid.h xvidcore xvid_plugin_single USE_XVID_4)
+
+########################################
+# AMR_NB
+########################################
+IF (USE_LATE_BINDING)
+	CHECK_INCLUDE_FILES(amrnb/interf_dec.h USE_AMR_NB)
+ELSE (USE_LATE_BINDING)
+	ADM_CHECK_HL(AMRNB amrnb/interf_dec.h amrnb GP3Decoder_Interface_Decode USE_AMR_NB)
+ENDIF (USE_LATE_BINDING)
+
+IF (USE_AMR_NB)
+	SET(CONFIG_AMR_NB 1)
+ENDIF (USE_AMR_NB)
+
+########################################
+# Libdca
+########################################
+SET(CMAKE_REQUIRED_FLAGS "-include stdint.h")
+SET(CMAKE_REQUIRED_LIBRARIES "-lm")
+
+IF (USE_LATE_BINDING)
+	CHECK_INCLUDE_FILES(dts.h USE_LIBDCA)
+ELSE (USE_LATE_BINDING)
+	ADM_CHECK_HL(libdca dts.h dts dts_init USE_LIBDCA)
+ENDIF (USE_LATE_BINDING)
+
+SET(CMAKE_REQUIRED_LIBRARIES)
+SET(CMAKE_REQUIRED_FLAGS)
+
+########################################
+# X264
+########################################
+SET(CMAKE_REQUIRED_FLAGS "-include stdint.h")
+
+IF (ADM_OS_WINDOWS)
+	SET(CMAKE_REQUIRED_LIBRARIES "-lm -lpthreadGC2")
+ELSE (ADM_OS_WINDOWS)
+	SET(CMAKE_REQUIRED_LIBRARIES "-lm -lpthread")
+ENDIF (ADM_OS_WINDOWS)
+
+ADM_CHECK_HL(x264 x264.h x264 x264_encoder_open USE_X264)
+SET(CMAKE_REQUIRED_FLAGS)
+SET(CMAKE_REQUIRED_LIBRARIES)
+
+########################################
+# PNG
+########################################
+ADM_CHECK_HL(libPNG png.h png png_malloc USE_PNG)
+
+########################################
+# FAAD
+########################################
+ADM_CHECK_HL(FAAD faad.h faad faacDecInit USE_FAAD_P)
+
+IF(NOT USE_FAAD_P)
+	MESSAGE(STATUS "Trying neaac variant")
+	ADM_CHECK_HL(NeAAC faad.h faad NeAACDecInit USE_FAAD_A)
+ENDIF(NOT USE_FAAD_P)
+
+IF(USE_FAAD_P OR USE_FAAD_A)
+	SET(USE_FAAD  1)
+ENDIF(USE_FAAD_P OR USE_FAAD_A)
+
+# See if we need old FAAD or new
+IF (USE_FAAD)
+	FIND_PATH(FAAD_H_DIR faad.h $ENV{CXXFLAGS})
+	FIND_LIBRARY(FAAD_LIB_DIR faad $ENV{CXXFLAGS})
+	
+	MESSAGE(STATUS "<Checking if faad needs old proto>")
+	ADM_COMPILE_WITH_WITHOUT(faad_check.cpp "-DOLD_FAAD_PROTO -I${FAAD_H_DIR}" "${FAAD_LIB_DIR}" FAAD_WITH)
+	
+	IF(FAAD_WITH)
+		MESSAGE(STATUS "Yes")
+		SET(OLD_FAAD_PROTO 1)
+	ELSE (FAAD_WITH)
+		MESSAGE(STATUS "No")
+	ENDIF (FAAD_WITH)
+ENDIF(USE_FAAD)
+
+########################################
+# FAAC
+########################################
+SET(CMAKE_REQUIRED_FLAGS "-include stdint.h")
+ADM_CHECK_HL(FAAC faac.h faac faacEncClose USE_FAAC)
+SET(CMAKE_REQUIRED_FLAGS)
+
+########################################
+# FreeType
+########################################
+IF (FT_FOUND)
+	SET(USE_FREETYPE 1)
+ENDIF (FT_FOUND)
+
+########################################
+# Vorbis
+########################################
+ADM_CHECK_HL(Vorbis vorbis/vorbisenc.h vorbis vorbis_info_init USE_VORBIS1)
+ADM_CHECK_HL(Vorbis vorbis/vorbisenc.h vorbisenc vorbis_encode_init USE_VORBIS2)
+
+IF (USE_VORBIS1 AND USE_VORBIS2)
+	SET(USE_VORBIS 1)
+ENDIF (USE_VORBIS1 AND USE_VORBIS2)
+
+########################################
+# End test
+########################################
+ADM_CHECK_HL(Invalid dummy_header.h dummy_libxyz dummy_func_tyu DUMMY_TEST)
+ADM_CHECK_HL(Invalid stdio.h dummy_libxyz dummy_func_tyu DUMMY_TEST2)
+
+IF (DUMMY_TEST OR DUMMY_TEST2)
+	MESSAGE(FATAL "This test should have failed!!!")
+	MESSAGE(FATAL "This test should have failed!!!")
+	MESSAGE(FATAL "This test should have failed!!!")
+ENDIF (DUMMY_TEST OR DUMMY_TEST2)
+
+INCLUDE(adm_log)
+
+IF (NOT CMAKE_BUILD_TYPE)
+	SET(CMAKE_BUILD_TYPE "Release")
+ENDIF (NOT CMAKE_BUILD_TYPE)
+
+IF (CMAKE_BUILD_TYPE STREQUAL "Debug")
+	MESSAGE(STATUS "** DEBUG BUILD (${CMAKE_BUILD_TYPE})**")
+	
+	SET(ADM_DEBUG 1)
+	ADD_DEFINITIONS(-DADM_DEBUG)
+ELSE (CMAKE_BUILD_TYPE STREQUAL "Debug")
+	MESSAGE(STATUS "** RELEASE BUILD (${CMAKE_BUILD_TYPE})**")
+ENDIF(CMAKE_BUILD_TYPE STREQUAL "Debug")
+
+MESSAGE("LINK_FLAGS ${CMAKE_LD_FLAGS}")
+# EOF

Modified: branches/avidemux_2.4_branch/avidemux/ADM_libraries/ADM_lavcodec/CMakeLists.txt
===================================================================
--- branches/avidemux_2.4_branch/avidemux/ADM_libraries/ADM_lavcodec/CMakeLists.txt	2007-12-03 12:38:06 UTC (rev 3719)
+++ branches/avidemux_2.4_branch/avidemux/ADM_libraries/ADM_lavcodec/CMakeLists.txt	2007-12-04 00:19:48 UTC (rev 3720)
@@ -46,6 +46,15 @@
 			i386/snowdsp_mmx.c i386/fft_3dn.c 
 )
 ENDIF(ARCH_X86)
+
+IF (BUILD_ALTIVEC)
+	SET(${ADM_LIB}_SRCS  ${${ADM_LIB}_SRCS}
+		ppc/dsputil_altivec.c  ppc/dsputil_ppc.c  ppc/fdct_altivec.c  ppc/fft_altivec.c  ppc/float_altivec.c  ppc/gmc_altivec.c
+		ppc/h264_altivec.c  ppc/idct_altivec.c  ppc/int_altivec.c  ppc/mpegvideo_altivec.c  ppc/mpegvideo_ppc.c
+		ppc/snow_altivec.c  ppc/vc1dsp_altivec.c
+	)
+ENDIF (BUILD_ALTIVEC)
+
 ADD_LIBRARY(${ADM_LIB} STATIC ${${ADM_LIB}_SRCS})
 ADD_ADM_LIB(${ADM_LIB} ADM_libraries)
 

Modified: branches/avidemux_2.4_branch/avidemux/ADM_libraries/ADM_lavcodec/ppc/dsputil_altivec.c
===================================================================
--- branches/avidemux_2.4_branch/avidemux/ADM_libraries/ADM_lavcodec/ppc/dsputil_altivec.c	2007-12-03 12:38:06 UTC (rev 3719)
+++ branches/avidemux_2.4_branch/avidemux/ADM_libraries/ADM_lavcodec/ppc/dsputil_altivec.c	2007-12-04 00:19:48 UTC (rev 3720)
@@ -20,7 +20,7 @@
  * Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA
  */
 
-#include "../dsputil.h"
+#include "dsputil.h"
 
 #include "gcc_fixes.h"
 
@@ -56,7 +56,7 @@
 int sad16_x2_altivec(void *v, uint8_t *pix1, uint8_t *pix2, int line_size, int h)
 {
     int i;
-    int s __attribute__((aligned(16)));
+    DECLARE_ALIGNED_16(int, s);
     const_vector unsigned char zero = (const_vector unsigned char)vec_splat_u8(0);
     vector unsigned char *tv;
     vector unsigned char pix1v, pix2v, pix2iv, avgv, t5;
@@ -103,7 +103,7 @@
 int sad16_y2_altivec(void *v, uint8_t *pix1, uint8_t *pix2, int line_size, int h)
 {
     int i;
-    int s __attribute__((aligned(16)));
+    DECLARE_ALIGNED_16(int, s);
     const_vector unsigned char zero = (const_vector unsigned char)vec_splat_u8(0);
     vector unsigned char *tv;
     vector unsigned char pix1v, pix2v, pix3v, avgv, t5;
@@ -163,7 +163,7 @@
 int sad16_xy2_altivec(void *v, uint8_t *pix1, uint8_t *pix2, int line_size, int h)
 {
     int i;
-    int s __attribute__((aligned(16)));
+    DECLARE_ALIGNED_16(int, s);
     uint8_t *pix3 = pix2 + line_size;
     const_vector unsigned char zero = (const_vector unsigned char)vec_splat_u8(0);
     const_vector unsigned short two = (const_vector unsigned short)vec_splat_u16(2);
@@ -264,7 +264,7 @@
 int sad16_altivec(void *v, uint8_t *pix1, uint8_t *pix2, int line_size, int h)
 {
     int i;
-    int s __attribute__((aligned(16)));
+    DECLARE_ALIGNED_16(int, s);
     const_vector unsigned int zero = (const_vector unsigned int)vec_splat_u32(0);
     vector unsigned char perm1, perm2, *pix1v, *pix2v;
     vector unsigned char t1, t2, t3,t4, t5;
@@ -306,7 +306,7 @@
 int sad8_altivec(void *v, uint8_t *pix1, uint8_t *pix2, int line_size, int h)
 {
     int i;
-    int s __attribute__((aligned(16)));
+    DECLARE_ALIGNED_16(int, s);;
     const_vector unsigned int zero = (const_vector unsigned int)vec_splat_u32(0);
     vector unsigned char perm1, perm2, permclear, *pix1v, *pix2v;
     vector unsigned char t1, t2, t3,t4, t5;
@@ -351,7 +351,7 @@
 int pix_norm1_altivec(uint8_t *pix, int line_size)
 {
     int i;
-    int s __attribute__((aligned(16)));
+    DECLARE_ALIGNED_16(int, s);
     const_vector unsigned int zero = (const_vector unsigned int)vec_splat_u32(0);
     vector unsigned char *tv;
     vector unsigned char pixv;
@@ -387,7 +387,7 @@
 int sse8_altivec(void *v, uint8_t *pix1, uint8_t *pix2, int line_size, int h)
 {
     int i;
-    int s __attribute__((aligned(16)));
+    DECLARE_ALIGNED_16(int, s);
     const_vector unsigned int zero = (const_vector unsigned int)vec_splat_u32(0);
     vector unsigned char perm1, perm2, permclear, *pix1v, *pix2v;
     vector unsigned char t1, t2, t3,t4, t5;
@@ -443,7 +443,7 @@
 int sse16_altivec(void *v, uint8_t *pix1, uint8_t *pix2, int line_size, int h)
 {
     int i;
-    int s __attribute__((aligned(16)));
+    DECLARE_ALIGNED_16(int, s);
     const_vector unsigned int zero = (const_vector unsigned int)vec_splat_u32(0);
     vector unsigned char perm1, perm2, *pix1v, *pix2v;
     vector unsigned char t1, t2, t3,t4, t5;
@@ -495,7 +495,7 @@
     vector signed int sumdiffs;
 
     int i;
-    int s __attribute__((aligned(16)));
+    DECLARE_ALIGNED_16(int, s);
 
     sad = (vector unsigned int)vec_splat_u32(0);
 
@@ -1107,12 +1107,10 @@
       register vector signed short srcV, dstV;                          \
       register vector signed short but0, but1, but2, op1, op2, op3;     \
       src1 = vec_ld(stride * i, src);                                   \
-      if ((((stride * i) + (unsigned long)src) & 0x0000000F) > 8)       \
-        src2 = vec_ld((stride * i) + 16, src);                          \
+      src2 = vec_ld((stride * i) + 15, src);                            \
       srcO = vec_perm(src1, src2, vec_lvsl(stride * i, src));           \
       dst1 = vec_ld(stride * i, dst);                                   \
-      if ((((stride * i) + (unsigned long)dst) & 0x0000000F) > 8)       \
-        dst2 = vec_ld((stride * i) + 16, dst);                          \
+      dst2 = vec_ld((stride * i) + 15, dst);                            \
       dstO = vec_perm(dst1, dst2, vec_lvsl(stride * i, dst));           \
       /* promote the unsigned chars to signed shorts */                 \
       /* we're in the 8x8 function, we only care for the first 8 */     \

Modified: branches/avidemux_2.4_branch/avidemux/ADM_libraries/ADM_lavcodec/ppc/dsputil_altivec.h
===================================================================
--- branches/avidemux_2.4_branch/avidemux/ADM_libraries/ADM_lavcodec/ppc/dsputil_altivec.h	2007-12-03 12:38:06 UTC (rev 3719)
+++ branches/avidemux_2.4_branch/avidemux/ADM_libraries/ADM_lavcodec/ppc/dsputil_altivec.h	2007-12-04 00:19:48 UTC (rev 3720)
@@ -101,6 +101,17 @@
     h = vec_mergel (D2, H2); \
 } while (0)
 
+
+/** \brief loads unaligned vector \a *src with offset \a offset
+    and returns it */
+static inline vector unsigned char unaligned_load(int offset, uint8_t *src)
+{
+    register vector unsigned char first = vec_ld(offset, src);
+    register vector unsigned char second = vec_ld(offset+15, src);
+    register vector unsigned char mask = vec_lvsl(offset, src);
+    return vec_perm(first, second, mask);
+}
+
 #endif /* HAVE_ALTIVEC */
 
 #endif /* _DSPUTIL_ALTIVEC_ */

Deleted: branches/avidemux_2.4_branch/avidemux/ADM_libraries/ADM_lavcodec/ppc/dsputil_h264_altivec.c
===================================================================
--- branches/avidemux_2.4_branch/avidemux/ADM_libraries/ADM_lavcodec/ppc/dsputil_h264_altivec.c	2007-12-03 12:38:06 UTC (rev 3719)
+++ branches/avidemux_2.4_branch/avidemux/ADM_libraries/ADM_lavcodec/ppc/dsputil_h264_altivec.c	2007-12-04 00:19:48 UTC (rev 3720)
@@ -1,319 +0,0 @@
-/*
- * Copyright (c) 2004 Romain Dolbeau <romain at dolbeau.org>
- *
- * This library is free software; you can redistribute it and/or
- * modify it under the terms of the GNU Lesser General Public
- * License as published by the Free Software Foundation; either
- * version 2 of the License, or (at your option) any later version.
- *
- * This library is distributed in the hope that it will be useful,
- * but WITHOUT ANY WARRANTY; without even the implied warranty of
- * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
- * Lesser General Public License for more details.
- *
- * You should have received a copy of the GNU Lesser General Public
- * License along with this library; if not, write to the Free Software
- * Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA
- */
-
-#include "../dsputil.h"
-
-#include "gcc_fixes.h"
-
-#include "dsputil_altivec.h"
-
-#define PUT_OP_U8_ALTIVEC(d, s, dst) d = s
-#define AVG_OP_U8_ALTIVEC(d, s, dst) d = vec_avg(dst, s)
-
-#define OP_U8_ALTIVEC                          PUT_OP_U8_ALTIVEC
-#define PREFIX_h264_chroma_mc8_altivec         put_h264_chroma_mc8_altivec
-#define PREFIX_h264_chroma_mc8_num             altivec_put_h264_chroma_mc8_num
-#define PREFIX_h264_qpel16_h_lowpass_altivec   put_h264_qpel16_h_lowpass_altivec
-#define PREFIX_h264_qpel16_h_lowpass_num       altivec_put_h264_qpel16_h_lowpass_num
-#define PREFIX_h264_qpel16_v_lowpass_altivec   put_h264_qpel16_v_lowpass_altivec
-#define PREFIX_h264_qpel16_v_lowpass_num       altivec_put_h264_qpel16_v_lowpass_num
-#define PREFIX_h264_qpel16_hv_lowpass_altivec  put_h264_qpel16_hv_lowpass_altivec
-#define PREFIX_h264_qpel16_hv_lowpass_num      altivec_put_h264_qpel16_hv_lowpass_num
-#include "dsputil_h264_template_altivec.c"
-#undef OP_U8_ALTIVEC
-#undef PREFIX_h264_chroma_mc8_altivec
-#undef PREFIX_h264_chroma_mc8_num
-#undef PREFIX_h264_qpel16_h_lowpass_altivec
-#undef PREFIX_h264_qpel16_h_lowpass_num
-#undef PREFIX_h264_qpel16_v_lowpass_altivec
-#undef PREFIX_h264_qpel16_v_lowpass_num
-#undef PREFIX_h264_qpel16_hv_lowpass_altivec
-#undef PREFIX_h264_qpel16_hv_lowpass_num
-
-#define OP_U8_ALTIVEC                          AVG_OP_U8_ALTIVEC
-#define PREFIX_h264_chroma_mc8_altivec         avg_h264_chroma_mc8_altivec
-#define PREFIX_h264_chroma_mc8_num             altivec_avg_h264_chroma_mc8_num
-#define PREFIX_h264_qpel16_h_lowpass_altivec   avg_h264_qpel16_h_lowpass_altivec
-#define PREFIX_h264_qpel16_h_lowpass_num       altivec_avg_h264_qpel16_h_lowpass_num
-#define PREFIX_h264_qpel16_v_lowpass_altivec   avg_h264_qpel16_v_lowpass_altivec
-#define PREFIX_h264_qpel16_v_lowpass_num       altivec_avg_h264_qpel16_v_lowpass_num
-#define PREFIX_h264_qpel16_hv_lowpass_altivec  avg_h264_qpel16_hv_lowpass_altivec
-#define PREFIX_h264_qpel16_hv_lowpass_num      altivec_avg_h264_qpel16_hv_lowpass_num
-#include "dsputil_h264_template_altivec.c"
-#undef OP_U8_ALTIVEC
-#undef PREFIX_h264_chroma_mc8_altivec
-#undef PREFIX_h264_chroma_mc8_num
-#undef PREFIX_h264_qpel16_h_lowpass_altivec
-#undef PREFIX_h264_qpel16_h_lowpass_num
-#undef PREFIX_h264_qpel16_v_lowpass_altivec
-#undef PREFIX_h264_qpel16_v_lowpass_num
-#undef PREFIX_h264_qpel16_hv_lowpass_altivec
-#undef PREFIX_h264_qpel16_hv_lowpass_num
-
-#define H264_MC(OPNAME, SIZE, CODETYPE) \
-static void OPNAME ## h264_qpel ## SIZE ## _mc00_ ## CODETYPE (uint8_t *dst, uint8_t *src, int stride){\
-    OPNAME ## pixels ## SIZE ## _ ## CODETYPE(dst, src, stride, SIZE);\
-}\
-\
-static void OPNAME ## h264_qpel ## SIZE ## _mc10_ ## CODETYPE(uint8_t *dst, uint8_t *src, int stride){ \
-    DECLARE_ALIGNED_16(uint64_t, temp[SIZE*SIZE/8]);\
-    uint8_t * const half= (uint8_t*)temp;\
-    put_h264_qpel ## SIZE ## _h_lowpass_ ## CODETYPE(half, src, SIZE, stride);\
-    OPNAME ## pixels ## SIZE ## _l2_ ## CODETYPE(dst, src, half, stride, stride, SIZE);\
-}\
-\
-static void OPNAME ## h264_qpel ## SIZE ## _mc20_ ## CODETYPE(uint8_t *dst, uint8_t *src, int stride){\
-    OPNAME ## h264_qpel ## SIZE ## _h_lowpass_ ## CODETYPE(dst, src, stride, stride);\
-}\
-\
-static void OPNAME ## h264_qpel ## SIZE ## _mc30_ ## CODETYPE(uint8_t *dst, uint8_t *src, int stride){\
-    DECLARE_ALIGNED_16(uint64_t, temp[SIZE*SIZE/8]);\
-    uint8_t * const half= (uint8_t*)temp;\
-    put_h264_qpel ## SIZE ## _h_lowpass_ ## CODETYPE(half, src, SIZE, stride);\
-    OPNAME ## pixels ## SIZE ## _l2_ ## CODETYPE(dst, src+1, half, stride, stride, SIZE);\
-}\
-\
-static void OPNAME ## h264_qpel ## SIZE ## _mc01_ ## CODETYPE(uint8_t *dst, uint8_t *src, int stride){\
-    DECLARE_ALIGNED_16(uint64_t, temp[SIZE*SIZE/8]);\
-    uint8_t * const half= (uint8_t*)temp;\
-    put_h264_qpel ## SIZE ## _v_lowpass_ ## CODETYPE(half, src, SIZE, stride);\
-    OPNAME ## pixels ## SIZE ## _l2_ ## CODETYPE(dst, src, half, stride, stride, SIZE);\
-}\
-\
-static void OPNAME ## h264_qpel ## SIZE ## _mc02_ ## CODETYPE(uint8_t *dst, uint8_t *src, int stride){\
-    OPNAME ## h264_qpel ## SIZE ## _v_lowpass_ ## CODETYPE(dst, src, stride, stride);\
-}\
-\
-static void OPNAME ## h264_qpel ## SIZE ## _mc03_ ## CODETYPE(uint8_t *dst, uint8_t *src, int stride){\
-    DECLARE_ALIGNED_16(uint64_t, temp[SIZE*SIZE/8]);\
-    uint8_t * const half= (uint8_t*)temp;\
-    put_h264_qpel ## SIZE ## _v_lowpass_ ## CODETYPE(half, src, SIZE, stride);\
-    OPNAME ## pixels ## SIZE ## _l2_ ## CODETYPE(dst, src+stride, half, stride, stride, SIZE);\
-}\
-\
-static void OPNAME ## h264_qpel ## SIZE ## _mc11_ ## CODETYPE(uint8_t *dst, uint8_t *src, int stride){\
-    DECLARE_ALIGNED_16(uint64_t, temp[SIZE*SIZE/4]);\
-    uint8_t * const halfH= (uint8_t*)temp;\
-    uint8_t * const halfV= ((uint8_t*)temp) + SIZE*SIZE;\
-    put_h264_qpel ## SIZE ## _h_lowpass_ ## CODETYPE(halfH, src, SIZE, stride);\
-    put_h264_qpel ## SIZE ## _v_lowpass_ ## CODETYPE(halfV, src, SIZE, stride);\
-    OPNAME ## pixels ## SIZE ## _l2_ ## CODETYPE(dst, halfH, halfV, stride, SIZE, SIZE);\
-}\
-\
-static void OPNAME ## h264_qpel ## SIZE ## _mc31_ ## CODETYPE(uint8_t *dst, uint8_t *src, int stride){\
-    DECLARE_ALIGNED_16(uint64_t, temp[SIZE*SIZE/4]);\
-    uint8_t * const halfH= (uint8_t*)temp;\
-    uint8_t * const halfV= ((uint8_t*)temp) + SIZE*SIZE;\
-    put_h264_qpel ## SIZE ## _h_lowpass_ ## CODETYPE(halfH, src, SIZE, stride);\
-    put_h264_qpel ## SIZE ## _v_lowpass_ ## CODETYPE(halfV, src+1, SIZE, stride);\
-    OPNAME ## pixels ## SIZE ## _l2_ ## CODETYPE(dst, halfH, halfV, stride, SIZE, SIZE);\
-}\
-\
-static void OPNAME ## h264_qpel ## SIZE ## _mc13_ ## CODETYPE(uint8_t *dst, uint8_t *src, int stride){\
-    DECLARE_ALIGNED_16(uint64_t, temp[SIZE*SIZE/4]);\
-    uint8_t * const halfH= (uint8_t*)temp;\
-    uint8_t * const halfV= ((uint8_t*)temp) + SIZE*SIZE;\
-    put_h264_qpel ## SIZE ## _h_lowpass_ ## CODETYPE(halfH, src + stride, SIZE, stride);\
-    put_h264_qpel ## SIZE ## _v_lowpass_ ## CODETYPE(halfV, src, SIZE, stride);\
-    OPNAME ## pixels ## SIZE ## _l2_ ## CODETYPE(dst, halfH, halfV, stride, SIZE, SIZE);\
-}\
-\
-static void OPNAME ## h264_qpel ## SIZE ## _mc33_ ## CODETYPE(uint8_t *dst, uint8_t *src, int stride){\
-    DECLARE_ALIGNED_16(uint64_t, temp[SIZE*SIZE/4]);\
-    uint8_t * const halfH= (uint8_t*)temp;\
-    uint8_t * const halfV= ((uint8_t*)temp) + SIZE*SIZE;\
-    put_h264_qpel ## SIZE ## _h_lowpass_ ## CODETYPE(halfH, src + stride, SIZE, stride);\
-    put_h264_qpel ## SIZE ## _v_lowpass_ ## CODETYPE(halfV, src+1, SIZE, stride);\
-    OPNAME ## pixels ## SIZE ## _l2_ ## CODETYPE(dst, halfH, halfV, stride, SIZE, SIZE);\
-}\
-\
-static void OPNAME ## h264_qpel ## SIZE ## _mc22_ ## CODETYPE(uint8_t *dst, uint8_t *src, int stride){\
-    DECLARE_ALIGNED_16(uint64_t, temp[SIZE*(SIZE+8)/4]);\
-    int16_t * const tmp= (int16_t*)temp;\
-    OPNAME ## h264_qpel ## SIZE ## _hv_lowpass_ ## CODETYPE(dst, tmp, src, stride, SIZE, stride);\
-}\
-\
-static void OPNAME ## h264_qpel ## SIZE ## _mc21_ ## CODETYPE(uint8_t *dst, uint8_t *src, int stride){\
-    DECLARE_ALIGNED_16(uint64_t, temp[SIZE*(SIZE+8)/4 + SIZE*SIZE/4]);\
-    uint8_t * const halfH= (uint8_t*)temp;\
-    uint8_t * const halfHV= ((uint8_t*)temp) + SIZE*SIZE;\
-    int16_t * const tmp= ((int16_t*)temp) + SIZE*SIZE;\
-    put_h264_qpel ## SIZE ## _h_lowpass_ ## CODETYPE(halfH, src, SIZE, stride);\
-    put_h264_qpel ## SIZE ## _hv_lowpass_ ## CODETYPE(halfHV, tmp, src, SIZE, SIZE, stride);\
-    OPNAME ## pixels ## SIZE ## _l2_ ## CODETYPE(dst, halfH, halfHV, stride, SIZE, SIZE);\
-}\
-\
-static void OPNAME ## h264_qpel ## SIZE ## _mc23_ ## CODETYPE(uint8_t *dst, uint8_t *src, int stride){\
-    DECLARE_ALIGNED_16(uint64_t, temp[SIZE*(SIZE+8)/4 + SIZE*SIZE/4]);\
-    uint8_t * const halfH= (uint8_t*)temp;\
-    uint8_t * const halfHV= ((uint8_t*)temp) + SIZE*SIZE;\
-    int16_t * const tmp= ((int16_t*)temp) + SIZE*SIZE;\
-    put_h264_qpel ## SIZE ## _h_lowpass_ ## CODETYPE(halfH, src + stride, SIZE, stride);\
-    put_h264_qpel ## SIZE ## _hv_lowpass_ ## CODETYPE(halfHV, tmp, src, SIZE, SIZE, stride);\
-    OPNAME ## pixels ## SIZE ## _l2_ ## CODETYPE(dst, halfH, halfHV, stride, SIZE, SIZE);\
-}\
-\
-static void OPNAME ## h264_qpel ## SIZE ## _mc12_ ## CODETYPE(uint8_t *dst, uint8_t *src, int stride){\
-    DECLARE_ALIGNED_16(uint64_t, temp[SIZE*(SIZE+8)/4 + SIZE*SIZE/4]);\
-    uint8_t * const halfV= (uint8_t*)temp;\
-    uint8_t * const halfHV= ((uint8_t*)temp) + SIZE*SIZE;\
-    int16_t * const tmp= ((int16_t*)temp) + SIZE*SIZE;\
-    put_h264_qpel ## SIZE ## _v_lowpass_ ## CODETYPE(halfV, src, SIZE, stride);\
-    put_h264_qpel ## SIZE ## _hv_lowpass_ ## CODETYPE(halfHV, tmp, src, SIZE, SIZE, stride);\
-    OPNAME ## pixels ## SIZE ## _l2_ ## CODETYPE(dst, halfV, halfHV, stride, SIZE, SIZE);\
-}\
-\
-static void OPNAME ## h264_qpel ## SIZE ## _mc32_ ## CODETYPE(uint8_t *dst, uint8_t *src, int stride){\
-    DECLARE_ALIGNED_16(uint64_t, temp[SIZE*(SIZE+8)/4 + SIZE*SIZE/4]);\
-    uint8_t * const halfV= (uint8_t*)temp;\
-    uint8_t * const halfHV= ((uint8_t*)temp) + SIZE*SIZE;\
-    int16_t * const tmp= ((int16_t*)temp) + SIZE*SIZE;\
-    put_h264_qpel ## SIZE ## _v_lowpass_ ## CODETYPE(halfV, src+1, SIZE, stride);\
-    put_h264_qpel ## SIZE ## _hv_lowpass_ ## CODETYPE(halfHV, tmp, src, SIZE, SIZE, stride);\
-    OPNAME ## pixels ## SIZE ## _l2_ ## CODETYPE(dst, halfV, halfHV, stride, SIZE, SIZE);\
-}\
-
-static inline void put_pixels16_l2_altivec( uint8_t * dst, const uint8_t * src1,
-                                    const uint8_t * src2, int dst_stride,
-                                    int src_stride1, int h)
-{
-    int i;
-    vector unsigned char a, b, d, tmp1, tmp2, mask, mask_, edges, align;
-
-    mask_ = vec_lvsl(0, src2);
-
-    for (i = 0; i < h; i++) {
-
-        tmp1 = vec_ld(i * src_stride1, src1);
-        mask = vec_lvsl(i * src_stride1, src1);
-        tmp2 = vec_ld(i * src_stride1 + 15, src1);
-
-        a = vec_perm(tmp1, tmp2, mask);
-
-        tmp1 = vec_ld(i * 16, src2);
-        tmp2 = vec_ld(i * 16 + 15, src2);
-
-        b = vec_perm(tmp1, tmp2, mask_);
-
-        tmp1 = vec_ld(0, dst);
-        mask = vec_lvsl(0, dst);
-        tmp2 = vec_ld(15, dst);
-
-        d = vec_avg(a, b);
-
-        edges = vec_perm(tmp2, tmp1, mask);
-
-        align = vec_lvsr(0, dst);
-
-        tmp1 = vec_perm(edges, d, align);
-        tmp2 = vec_perm(d, edges, align);
-
-        vec_st(tmp2, 15, dst);
-        vec_st(tmp1, 0 , dst);
-
-        dst += dst_stride;
-    }
-}
-
-static inline void avg_pixels16_l2_altivec( uint8_t * dst, const uint8_t * src1,
-                                    const uint8_t * src2, int dst_stride,
-                                    int src_stride1, int h)
-{
-    int i;
-    vector unsigned char a, b, d, tmp1, tmp2, mask, mask_, edges, align;
-
-    mask_ = vec_lvsl(0, src2);
-
-    for (i = 0; i < h; i++) {
-
-        tmp1 = vec_ld(i * src_stride1, src1);
-        mask = vec_lvsl(i * src_stride1, src1);
-        tmp2 = vec_ld(i * src_stride1 + 15, src1);
-
-        a = vec_perm(tmp1, tmp2, mask);
-
-        tmp1 = vec_ld(i * 16, src2);
-        tmp2 = vec_ld(i * 16 + 15, src2);
-
-        b = vec_perm(tmp1, tmp2, mask_);
-
-        tmp1 = vec_ld(0, dst);
-        mask = vec_lvsl(0, dst);
-        tmp2 = vec_ld(15, dst);
-
-        d = vec_avg(vec_perm(tmp1, tmp2, mask), vec_avg(a, b));
-
-        edges = vec_perm(tmp2, tmp1, mask);
-
-        align = vec_lvsr(0, dst);
-
-        tmp1 = vec_perm(edges, d, align);
-        tmp2 = vec_perm(d, edges, align);
-
-        vec_st(tmp2, 15, dst);
-        vec_st(tmp1, 0 , dst);
-
-        dst += dst_stride;
-    }
-}
-
-/* Implemented but could be faster
-#define put_pixels16_l2_altivec(d,s1,s2,ds,s1s,h) put_pixels16_l2(d,s1,s2,ds,s1s,16,h)
-#define avg_pixels16_l2_altivec(d,s1,s2,ds,s1s,h) avg_pixels16_l2(d,s1,s2,ds,s1s,16,h)
- */
-
-  H264_MC(put_, 16, altivec)
-  H264_MC(avg_, 16, altivec)
-
-void dsputil_h264_init_ppc(DSPContext* c, AVCodecContext *avctx) {
-
-#ifdef HAVE_ALTIVEC
-  if (has_altivec()) {
-    c->put_h264_chroma_pixels_tab[0] = put_h264_chroma_mc8_altivec;
-    c->avg_h264_chroma_pixels_tab[0] = avg_h264_chroma_mc8_altivec;
-
-#define dspfunc(PFX, IDX, NUM) \
-    c->PFX ## _pixels_tab[IDX][ 0] = PFX ## NUM ## _mc00_altivec; \
-    c->PFX ## _pixels_tab[IDX][ 1] = PFX ## NUM ## _mc10_altivec; \
-    c->PFX ## _pixels_tab[IDX][ 2] = PFX ## NUM ## _mc20_altivec; \
-    c->PFX ## _pixels_tab[IDX][ 3] = PFX ## NUM ## _mc30_altivec; \
-    c->PFX ## _pixels_tab[IDX][ 4] = PFX ## NUM ## _mc01_altivec; \
-    c->PFX ## _pixels_tab[IDX][ 5] = PFX ## NUM ## _mc11_altivec; \
-    c->PFX ## _pixels_tab[IDX][ 6] = PFX ## NUM ## _mc21_altivec; \
-    c->PFX ## _pixels_tab[IDX][ 7] = PFX ## NUM ## _mc31_altivec; \
-    c->PFX ## _pixels_tab[IDX][ 8] = PFX ## NUM ## _mc02_altivec; \
-    c->PFX ## _pixels_tab[IDX][ 9] = PFX ## NUM ## _mc12_altivec; \
-    c->PFX ## _pixels_tab[IDX][10] = PFX ## NUM ## _mc22_altivec; \
-    c->PFX ## _pixels_tab[IDX][11] = PFX ## NUM ## _mc32_altivec; \
-    c->PFX ## _pixels_tab[IDX][12] = PFX ## NUM ## _mc03_altivec; \
-    c->PFX ## _pixels_tab[IDX][13] = PFX ## NUM ## _mc13_altivec; \
-    c->PFX ## _pixels_tab[IDX][14] = PFX ## NUM ## _mc23_altivec; \
-    c->PFX ## _pixels_tab[IDX][15] = PFX ## NUM ## _mc33_altivec
-
-    dspfunc(put_h264_qpel, 0, 16);
-    dspfunc(avg_h264_qpel, 0, 16);
-#undef dspfunc
-
-  } else
-#endif /* HAVE_ALTIVEC */
-  {
-    // Non-AltiVec PPC optimisations
-
-    // ... pending ...
-  }
-}

Deleted: branches/avidemux_2.4_branch/avidemux/ADM_libraries/ADM_lavcodec/ppc/dsputil_h264_template_altivec.c
===================================================================
--- branches/avidemux_2.4_branch/avidemux/ADM_libraries/ADM_lavcodec/ppc/dsputil_h264_template_altivec.c	2007-12-03 12:38:06 UTC (rev 3719)
+++ branches/avidemux_2.4_branch/avidemux/ADM_libraries/ADM_lavcodec/ppc/dsputil_h264_template_altivec.c	2007-12-04 00:19:48 UTC (rev 3720)
@@ -1,717 +0,0 @@
-/*
- * Copyright (c) 2004 Romain Dolbeau <romain at dolbeau.org>
- *
- * This library is free software; you can redistribute it and/or
- * modify it under the terms of the GNU Lesser General Public
- * License as published by the Free Software Foundation; either
- * version 2 of the License, or (at your option) any later version.
- *
- * This library is distributed in the hope that it will be useful,
- * but WITHOUT ANY WARRANTY; without even the implied warranty of
- * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
- * Lesser General Public License for more details.
- *
- * You should have received a copy of the GNU Lesser General Public
- * License along with this library; if not, write to the Free Software
- * Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA
- */
-
-/* this code assume that stride % 16 == 0 */
-void PREFIX_h264_chroma_mc8_altivec(uint8_t * dst, uint8_t * src, int stride, int h, int x, int y) {
-  POWERPC_PERF_DECLARE(PREFIX_h264_chroma_mc8_num, 1);
-    signed int ABCD[4] __attribute__((aligned(16))) =
-                        {((8 - x) * (8 - y)),
-                          ((x) * (8 - y)),
-                          ((8 - x) * (y)),
-                          ((x) * (y))};
-    register int i;
-    vector unsigned char fperm;
-    const vector signed int vABCD = vec_ld(0, ABCD);
-    const vector signed short vA = vec_splat((vector signed short)vABCD, 1);
-    const vector signed short vB = vec_splat((vector signed short)vABCD, 3);
-    const vector signed short vC = vec_splat((vector signed short)vABCD, 5);
-    const vector signed short vD = vec_splat((vector signed short)vABCD, 7);
-    const vector signed int vzero = vec_splat_s32(0);
-    const vector signed short v32ss = vec_sl(vec_splat_s16(1),vec_splat_u16(5));
-    const vector unsigned short v6us = vec_splat_u16(6);
-    register int loadSecond = (((unsigned long)src) % 16) <= 7 ? 0 : 1;
-    register int reallyBadAlign = (((unsigned long)src) % 16) == 15 ? 1 : 0;
-
-    vector unsigned char vsrcAuc, vsrcBuc, vsrcperm0, vsrcperm1;
-    vector unsigned char vsrc0uc, vsrc1uc;
-    vector signed short vsrc0ssH, vsrc1ssH;
-    vector unsigned char vsrcCuc, vsrc2uc, vsrc3uc;
-    vector signed short vsrc2ssH, vsrc3ssH, psum;
-    vector unsigned char vdst, ppsum, vfdst, fsum;
-
-  POWERPC_PERF_START_COUNT(PREFIX_h264_chroma_mc8_num, 1);
-
-    if (((unsigned long)dst) % 16 == 0) {
-      fperm = (vector unsigned char)AVV(0x10, 0x11, 0x12, 0x13,
-                                        0x14, 0x15, 0x16, 0x17,
-                                        0x08, 0x09, 0x0A, 0x0B,
-                                        0x0C, 0x0D, 0x0E, 0x0F);
-    } else {
-      fperm = (vector unsigned char)AVV(0x00, 0x01, 0x02, 0x03,
-                                        0x04, 0x05, 0x06, 0x07,
-                                        0x18, 0x19, 0x1A, 0x1B,
-                                        0x1C, 0x1D, 0x1E, 0x1F);
-    }
-
-    vsrcAuc = vec_ld(0, src);
-
-    if (loadSecond)
-      vsrcBuc = vec_ld(16, src);
-    vsrcperm0 = vec_lvsl(0, src);
-    vsrcperm1 = vec_lvsl(1, src);
-
-    vsrc0uc = vec_perm(vsrcAuc, vsrcBuc, vsrcperm0);
-    if (reallyBadAlign)
-      vsrc1uc = vsrcBuc;
-    else
-      vsrc1uc = vec_perm(vsrcAuc, vsrcBuc, vsrcperm1);
-
-    vsrc0ssH = (vector signed short)vec_mergeh((vector unsigned char)vzero,
-                                               (vector unsigned char)vsrc0uc);
-    vsrc1ssH = (vector signed short)vec_mergeh((vector unsigned char)vzero,
-                                               (vector unsigned char)vsrc1uc);
-
-    if (!loadSecond) {// -> !reallyBadAlign
-      for (i = 0 ; i < h ; i++) {
-
-
-        vsrcCuc = vec_ld(stride + 0, src);
-
-        vsrc2uc = vec_perm(vsrcCuc, vsrcCuc, vsrcperm0);
-        vsrc3uc = vec_perm(vsrcCuc, vsrcCuc, vsrcperm1);
-
-        vsrc2ssH = (vector signed short)vec_mergeh((vector unsigned char)vzero,
-                                                (vector unsigned char)vsrc2uc);
-        vsrc3ssH = (vector signed short)vec_mergeh((vector unsigned char)vzero,
-                                                (vector unsigned char)vsrc3uc);
-
-        psum = vec_mladd(vA, vsrc0ssH, vec_splat_s16(0));
-        psum = vec_mladd(vB, vsrc1ssH, psum);
-        psum = vec_mladd(vC, vsrc2ssH, psum);
-        psum = vec_mladd(vD, vsrc3ssH, psum);
-        psum = vec_add(v32ss, psum);
-        psum = vec_sra(psum, v6us);
-
-        vdst = vec_ld(0, dst);
-        ppsum = (vector unsigned char)vec_packsu(psum, psum);
-        vfdst = vec_perm(vdst, ppsum, fperm);
-
-        OP_U8_ALTIVEC(fsum, vfdst, vdst);
-
-        vec_st(fsum, 0, dst);
-
-        vsrc0ssH = vsrc2ssH;
-        vsrc1ssH = vsrc3ssH;
-
-        dst += stride;
-        src += stride;
-      }
-    } else {
-        vector unsigned char vsrcDuc;
-      for (i = 0 ; i < h ; i++) {
-        vsrcCuc = vec_ld(stride + 0, src);
-        vsrcDuc = vec_ld(stride + 16, src);
-
-        vsrc2uc = vec_perm(vsrcCuc, vsrcDuc, vsrcperm0);
-        if (reallyBadAlign)
-          vsrc3uc = vsrcDuc;
-        else
-          vsrc3uc = vec_perm(vsrcCuc, vsrcDuc, vsrcperm1);
-
-        vsrc2ssH = (vector signed short)vec_mergeh((vector unsigned char)vzero,
-                                                (vector unsigned char)vsrc2uc);
-        vsrc3ssH = (vector signed short)vec_mergeh((vector unsigned char)vzero,
-                                                (vector unsigned char)vsrc3uc);
-
-        psum = vec_mladd(vA, vsrc0ssH, vec_splat_s16(0));
-        psum = vec_mladd(vB, vsrc1ssH, psum);
-        psum = vec_mladd(vC, vsrc2ssH, psum);
-        psum = vec_mladd(vD, vsrc3ssH, psum);
-        psum = vec_add(v32ss, psum);
-        psum = vec_sr(psum, v6us);
-
-        vdst = vec_ld(0, dst);
-        ppsum = (vector unsigned char)vec_pack(psum, psum);
-        vfdst = vec_perm(vdst, ppsum, fperm);
-
-        OP_U8_ALTIVEC(fsum, vfdst, vdst);
-
-        vec_st(fsum, 0, dst);
-
-        vsrc0ssH = vsrc2ssH;
-        vsrc1ssH = vsrc3ssH;
-
-        dst += stride;
-        src += stride;
-      }
-    }
-    POWERPC_PERF_STOP_COUNT(PREFIX_h264_chroma_mc8_num, 1);
-}
-
-/* this code assume stride % 16 == 0 */
-static void PREFIX_h264_qpel16_h_lowpass_altivec(uint8_t * dst, uint8_t * src, int dstStride, int srcStride) {
-  POWERPC_PERF_DECLARE(PREFIX_h264_qpel16_h_lowpass_num, 1);
-  register int i;
-
-  const vector signed int vzero = vec_splat_s32(0);
-  const vector unsigned char permM2 = vec_lvsl(-2, src);
-  const vector unsigned char permM1 = vec_lvsl(-1, src);
-  const vector unsigned char permP0 = vec_lvsl(+0, src);
-  const vector unsigned char permP1 = vec_lvsl(+1, src);
-  const vector unsigned char permP2 = vec_lvsl(+2, src);
-  const vector unsigned char permP3 = vec_lvsl(+3, src);
-  const vector signed short v5ss = vec_splat_s16(5);
-  const vector unsigned short v5us = vec_splat_u16(5);
-  const vector signed short v20ss = vec_sl(vec_splat_s16(5),vec_splat_u16(2));
-  const vector signed short v16ss = vec_sl(vec_splat_s16(1),vec_splat_u16(4));
-  const vector unsigned char dstperm = vec_lvsr(0, dst);
-  const vector unsigned char neg1 =
-                                (const vector unsigned char) vec_splat_s8(-1);
-
-  const vector unsigned char dstmask =
-                                vec_perm((const vector unsigned char)vzero,
-                                                               neg1, dstperm);
-
-  vector unsigned char srcM2, srcM1, srcP0, srcP1, srcP2, srcP3;
-
-  register int align = ((((unsigned long)src) - 2) % 16);
-
-  vector signed short srcP0A, srcP0B, srcP1A, srcP1B,
-                      srcP2A, srcP2B, srcP3A, srcP3B,
-                      srcM1A, srcM1B, srcM2A, srcM2B,
-                      sum1A, sum1B, sum2A, sum2B, sum3A, sum3B,
-                      pp1A, pp1B, pp2A, pp2B, pp3A, pp3B,
-                      psumA, psumB, sumA, sumB;
-
-  vector unsigned char sum, dst1, dst2, vdst, fsum,
-                       rsum, fdst1, fdst2;
-
-  POWERPC_PERF_START_COUNT(PREFIX_h264_qpel16_h_lowpass_num, 1);
-
-  for (i = 0 ; i < 16 ; i ++) {
-    vector unsigned char srcR1 = vec_ld(-2, src);
-    vector unsigned char srcR2 = vec_ld(14, src);
-
-    switch (align) {
-    default: {
-      srcM2 = vec_perm(srcR1, srcR2, permM2);
-      srcM1 = vec_perm(srcR1, srcR2, permM1);
-      srcP0 = vec_perm(srcR1, srcR2, permP0);
-      srcP1 = vec_perm(srcR1, srcR2, permP1);
-      srcP2 = vec_perm(srcR1, srcR2, permP2);
-      srcP3 = vec_perm(srcR1, srcR2, permP3);
-    } break;
-    case 11: {
-      srcM2 = vec_perm(srcR1, srcR2, permM2);
-      srcM1 = vec_perm(srcR1, srcR2, permM1);
-      srcP0 = vec_perm(srcR1, srcR2, permP0);
-      srcP1 = vec_perm(srcR1, srcR2, permP1);
-      srcP2 = vec_perm(srcR1, srcR2, permP2);
-      srcP3 = srcR2;
-    } break;
-    case 12: {
-      vector unsigned char srcR3 = vec_ld(30, src);
-      srcM2 = vec_perm(srcR1, srcR2, permM2);
-      srcM1 = vec_perm(srcR1, srcR2, permM1);
-      srcP0 = vec_perm(srcR1, srcR2, permP0);
-      srcP1 = vec_perm(srcR1, srcR2, permP1);
-      srcP2 = srcR2;
-      srcP3 = vec_perm(srcR2, srcR3, permP3);
-    } break;
-    case 13: {
-      vector unsigned char srcR3 = vec_ld(30, src);
-      srcM2 = vec_perm(srcR1, srcR2, permM2);
-      srcM1 = vec_perm(srcR1, srcR2, permM1);
-      srcP0 = vec_perm(srcR1, srcR2, permP0);
-      srcP1 = srcR2;
-      srcP2 = vec_perm(srcR2, srcR3, permP2);
-      srcP3 = vec_perm(srcR2, srcR3, permP3);
-    } break;
-    case 14: {
-      vector unsigned char srcR3 = vec_ld(30, src);
-      srcM2 = vec_perm(srcR1, srcR2, permM2);
-      srcM1 = vec_perm(srcR1, srcR2, permM1);
-      srcP0 = srcR2;
-      srcP1 = vec_perm(srcR2, srcR3, permP1);
-      srcP2 = vec_perm(srcR2, srcR3, permP2);
-      srcP3 = vec_perm(srcR2, srcR3, permP3);
-    } break;
-    case 15: {
-      vector unsigned char srcR3 = vec_ld(30, src);
-      srcM2 = vec_perm(srcR1, srcR2, permM2);
-      srcM1 = srcR2;
-      srcP0 = vec_perm(srcR2, srcR3, permP0);
-      srcP1 = vec_perm(srcR2, srcR3, permP1);
-      srcP2 = vec_perm(srcR2, srcR3, permP2);
-      srcP3 = vec_perm(srcR2, srcR3, permP3);
-    } break;
-    }
-
-    srcP0A = (vector signed short)
-                vec_mergeh((vector unsigned char)vzero, srcP0);
-    srcP0B = (vector signed short)
-                vec_mergel((vector unsigned char)vzero, srcP0);
-    srcP1A = (vector signed short)
-                vec_mergeh((vector unsigned char)vzero, srcP1);
-    srcP1B = (vector signed short)
-                vec_mergel((vector unsigned char)vzero, srcP1);
-
-    srcP2A = (vector signed short)
-                vec_mergeh((vector unsigned char)vzero, srcP2);
-    srcP2B = (vector signed short)
-                vec_mergel((vector unsigned char)vzero, srcP2);
-    srcP3A = (vector signed short)
-                vec_mergeh((vector unsigned char)vzero, srcP3);
-    srcP3B = (vector signed short)
-                vec_mergel((vector unsigned char)vzero, srcP3);
-
-    srcM1A = (vector signed short)
-                vec_mergeh((vector unsigned char)vzero, srcM1);
-    srcM1B = (vector signed short)
-                vec_mergel((vector unsigned char)vzero, srcM1);
-    srcM2A = (vector signed short)
-                vec_mergeh((vector unsigned char)vzero, srcM2);
-    srcM2B = (vector signed short)
-                vec_mergel((vector unsigned char)vzero, srcM2);
-
-    sum1A = vec_adds(srcP0A, srcP1A);
-    sum1B = vec_adds(srcP0B, srcP1B);
-    sum2A = vec_adds(srcM1A, srcP2A);
-    sum2B = vec_adds(srcM1B, srcP2B);
-    sum3A = vec_adds(srcM2A, srcP3A);
-    sum3B = vec_adds(srcM2B, srcP3B);
-
-    pp1A = vec_mladd(sum1A, v20ss, v16ss);
-    pp1B = vec_mladd(sum1B, v20ss, v16ss);
-
-    pp2A = vec_mladd(sum2A, v5ss, (vector signed short)vzero);
-    pp2B = vec_mladd(sum2B, v5ss, (vector signed short)vzero);
-
-    pp3A = vec_add(sum3A, pp1A);
-    pp3B = vec_add(sum3B, pp1B);
-
-    psumA = vec_sub(pp3A, pp2A);
-    psumB = vec_sub(pp3B, pp2B);
-
-    sumA = vec_sra(psumA, v5us);
-    sumB = vec_sra(psumB, v5us);
-
-    sum = vec_packsu(sumA, sumB);
-
-    dst1 = vec_ld(0, dst);
-    dst2 = vec_ld(16, dst);
-    vdst = vec_perm(dst1, dst2, vec_lvsl(0, dst));
-
-    OP_U8_ALTIVEC(fsum, sum, vdst);
-
-    rsum = vec_perm(fsum, fsum, dstperm);
-    fdst1 = vec_sel(dst1, rsum, dstmask);
-    fdst2 = vec_sel(rsum, dst2, dstmask);
-
-    vec_st(fdst1, 0, dst);
-    vec_st(fdst2, 16, dst);
-
-    src += srcStride;
-    dst += dstStride;
-  }
-POWERPC_PERF_STOP_COUNT(PREFIX_h264_qpel16_h_lowpass_num, 1);
-}
-
-/* this code assume stride % 16 == 0 */
-static void PREFIX_h264_qpel16_v_lowpass_altivec(uint8_t * dst, uint8_t * src, int dstStride, int srcStride) {
-  POWERPC_PERF_DECLARE(PREFIX_h264_qpel16_v_lowpass_num, 1);
-
-  register int i;
-
-  const vector signed int vzero = vec_splat_s32(0);
-  const vector unsigned char perm = vec_lvsl(0, src);
-  const vector signed short v20ss = vec_sl(vec_splat_s16(5),vec_splat_u16(2));
-  const vector unsigned short v5us = vec_splat_u16(5);
-  const vector signed short v5ss = vec_splat_s16(5);
-  const vector signed short v16ss = vec_sl(vec_splat_s16(1),vec_splat_u16(4));
-  const vector unsigned char dstperm = vec_lvsr(0, dst);
-  const vector unsigned char neg1 = (const vector unsigned char)vec_splat_s8(-1);
-  const vector unsigned char dstmask = vec_perm((const vector unsigned char)vzero, neg1, dstperm);
-
-  uint8_t *srcbis = src - (srcStride * 2);
-
-  const vector unsigned char srcM2a = vec_ld(0, srcbis);
-  const vector unsigned char srcM2b = vec_ld(16, srcbis);
-  const vector unsigned char srcM2 = vec_perm(srcM2a, srcM2b, perm);
-//  srcbis += srcStride;
-  const vector unsigned char srcM1a = vec_ld(0, srcbis += srcStride);
-  const vector unsigned char srcM1b = vec_ld(16, srcbis);
-  const vector unsigned char srcM1 = vec_perm(srcM1a, srcM1b, perm);
-//  srcbis += srcStride;
-  const vector unsigned char srcP0a = vec_ld(0, srcbis += srcStride);
-  const vector unsigned char srcP0b = vec_ld(16, srcbis);
-  const vector unsigned char srcP0 = vec_perm(srcP0a, srcP0b, perm);
-//  srcbis += srcStride;
-  const vector unsigned char srcP1a = vec_ld(0, srcbis += srcStride);
-  const vector unsigned char srcP1b = vec_ld(16, srcbis);
-  const vector unsigned char srcP1 = vec_perm(srcP1a, srcP1b, perm);
-//  srcbis += srcStride;
-  const vector unsigned char srcP2a = vec_ld(0, srcbis += srcStride);
-  const vector unsigned char srcP2b = vec_ld(16, srcbis);
-  const vector unsigned char srcP2 = vec_perm(srcP2a, srcP2b, perm);
-//  srcbis += srcStride;
-
-  vector signed short srcM2ssA = (vector signed short)
-                                vec_mergeh((vector unsigned char)vzero, srcM2);
-  vector signed short srcM2ssB = (vector signed short)
-                                vec_mergel((vector unsigned char)vzero, srcM2);
-  vector signed short srcM1ssA = (vector signed short)
-                                vec_mergeh((vector unsigned char)vzero, srcM1);
-  vector signed short srcM1ssB = (vector signed short)
-                                vec_mergel((vector unsigned char)vzero, srcM1);
-  vector signed short srcP0ssA = (vector signed short)
-                                vec_mergeh((vector unsigned char)vzero, srcP0);
-  vector signed short srcP0ssB = (vector signed short)
-                                vec_mergel((vector unsigned char)vzero, srcP0);
-  vector signed short srcP1ssA = (vector signed short)
-                                vec_mergeh((vector unsigned char)vzero, srcP1);
-  vector signed short srcP1ssB = (vector signed short)
-                                vec_mergel((vector unsigned char)vzero, srcP1);
-  vector signed short srcP2ssA = (vector signed short)
-                                vec_mergeh((vector unsigned char)vzero, srcP2);
-  vector signed short srcP2ssB = (vector signed short)
-                                vec_mergel((vector unsigned char)vzero, srcP2);
-
-  vector signed short pp1A, pp1B, pp2A, pp2B, pp3A, pp3B,
-                      psumA, psumB, sumA, sumB,
-                      srcP3ssA, srcP3ssB,
-                      sum1A, sum1B, sum2A, sum2B, sum3A, sum3B;
-
-  vector unsigned char sum, dst1, dst2, vdst, fsum, rsum, fdst1, fdst2,
-                       srcP3a, srcP3b, srcP3;
-
-  POWERPC_PERF_START_COUNT(PREFIX_h264_qpel16_v_lowpass_num, 1);
-
-  for (i = 0 ; i < 16 ; i++) {
-    srcP3a = vec_ld(0, srcbis += srcStride);
-    srcP3b = vec_ld(16, srcbis);
-    srcP3 = vec_perm(srcP3a, srcP3b, perm);
-    srcP3ssA = (vector signed short)
-                                vec_mergeh((vector unsigned char)vzero, srcP3);
-    srcP3ssB = (vector signed short)
-                                vec_mergel((vector unsigned char)vzero, srcP3);
-//    srcbis += srcStride;
-
-    sum1A = vec_adds(srcP0ssA, srcP1ssA);
-    sum1B = vec_adds(srcP0ssB, srcP1ssB);
-    sum2A = vec_adds(srcM1ssA, srcP2ssA);
-    sum2B = vec_adds(srcM1ssB, srcP2ssB);
-    sum3A = vec_adds(srcM2ssA, srcP3ssA);
-    sum3B = vec_adds(srcM2ssB, srcP3ssB);
-
-    srcM2ssA = srcM1ssA;
-    srcM2ssB = srcM1ssB;
-    srcM1ssA = srcP0ssA;
-    srcM1ssB = srcP0ssB;
-    srcP0ssA = srcP1ssA;
-    srcP0ssB = srcP1ssB;
-    srcP1ssA = srcP2ssA;
-    srcP1ssB = srcP2ssB;
-    srcP2ssA = srcP3ssA;
-    srcP2ssB = srcP3ssB;
-
-    pp1A = vec_mladd(sum1A, v20ss, v16ss);
-    pp1B = vec_mladd(sum1B, v20ss, v16ss);
-
-    pp2A = vec_mladd(sum2A, v5ss, (vector signed short)vzero);
-    pp2B = vec_mladd(sum2B, v5ss, (vector signed short)vzero);
-
-    pp3A = vec_add(sum3A, pp1A);
-    pp3B = vec_add(sum3B, pp1B);
-
-    psumA = vec_sub(pp3A, pp2A);
-    psumB = vec_sub(pp3B, pp2B);
-
-    sumA = vec_sra(psumA, v5us);
-    sumB = vec_sra(psumB, v5us);
-
-    sum = vec_packsu(sumA, sumB);
-
-    dst1 = vec_ld(0, dst);
-    dst2 = vec_ld(16, dst);
-    vdst = vec_perm(dst1, dst2, vec_lvsl(0, dst));
-
-    OP_U8_ALTIVEC(fsum, sum, vdst);
-
-    rsum = vec_perm(fsum, fsum, dstperm);
-    fdst1 = vec_sel(dst1, rsum, dstmask);
-    fdst2 = vec_sel(rsum, dst2, dstmask);
-
-    vec_st(fdst1, 0, dst);
-    vec_st(fdst2, 16, dst);
-
-    dst += dstStride;
-  }
-  POWERPC_PERF_STOP_COUNT(PREFIX_h264_qpel16_v_lowpass_num, 1);
-}
-
-/* this code assume stride % 16 == 0 *and* tmp is properly aligned */
-static void PREFIX_h264_qpel16_hv_lowpass_altivec(uint8_t * dst, int16_t * tmp, uint8_t * src, int dstStride, int tmpStride, int srcStride) {
-  POWERPC_PERF_DECLARE(PREFIX_h264_qpel16_hv_lowpass_num, 1);
-  register int i;
-  const vector signed int vzero = vec_splat_s32(0);
-  const vector unsigned char permM2 = vec_lvsl(-2, src);
-  const vector unsigned char permM1 = vec_lvsl(-1, src);
-  const vector unsigned char permP0 = vec_lvsl(+0, src);
-  const vector unsigned char permP1 = vec_lvsl(+1, src);
-  const vector unsigned char permP2 = vec_lvsl(+2, src);
-  const vector unsigned char permP3 = vec_lvsl(+3, src);
-  const vector signed short v20ss = vec_sl(vec_splat_s16(5),vec_splat_u16(2));
-  const vector unsigned int v10ui = vec_splat_u32(10);
-  const vector signed short v5ss = vec_splat_s16(5);
-  const vector signed short v1ss = vec_splat_s16(1);
-  const vector signed int v512si = vec_sl(vec_splat_s32(1),vec_splat_u32(9));
-  const vector unsigned int v16ui = vec_sl(vec_splat_u32(1),vec_splat_u32(4));
-
-  register int align = ((((unsigned long)src) - 2) % 16);
-
-  const vector unsigned char neg1 = (const vector unsigned char)
-                                                        vec_splat_s8(-1);
-
-  vector signed short srcP0A, srcP0B, srcP1A, srcP1B,
-                      srcP2A, srcP2B, srcP3A, srcP3B,
-                      srcM1A, srcM1B, srcM2A, srcM2B,
-                      sum1A, sum1B, sum2A, sum2B, sum3A, sum3B,
-                      pp1A, pp1B, pp2A, pp2B, psumA, psumB;
-
-  const vector unsigned char dstperm = vec_lvsr(0, dst);
-
-  const vector unsigned char dstmask = vec_perm((const vector unsigned char)vzero, neg1, dstperm);
-
-  const vector unsigned char mperm = (const vector unsigned char)
-    AVV(0x00, 0x08, 0x01, 0x09, 0x02, 0x0A, 0x03, 0x0B,
-        0x04, 0x0C, 0x05, 0x0D, 0x06, 0x0E, 0x07, 0x0F);
-  int16_t *tmpbis = tmp;
-
-  vector signed short tmpM1ssA, tmpM1ssB, tmpM2ssA, tmpM2ssB,
-                      tmpP0ssA, tmpP0ssB, tmpP1ssA, tmpP1ssB,
-                      tmpP2ssA, tmpP2ssB;
-
-  vector signed int pp1Ae, pp1Ao, pp1Be, pp1Bo, pp2Ae, pp2Ao, pp2Be, pp2Bo,
-                    pp3Ae, pp3Ao, pp3Be, pp3Bo, pp1cAe, pp1cAo, pp1cBe, pp1cBo,
-                    pp32Ae, pp32Ao, pp32Be, pp32Bo, sumAe, sumAo, sumBe, sumBo,
-                    ssumAe, ssumAo, ssumBe, ssumBo;
-  vector unsigned char fsum, sumv, sum, dst1, dst2, vdst,
-                       rsum, fdst1, fdst2;
-  vector signed short ssume, ssumo;
-
-  POWERPC_PERF_START_COUNT(PREFIX_h264_qpel16_hv_lowpass_num, 1);
-  src -= (2 * srcStride);
-  for (i = 0 ; i < 21 ; i ++) {
-    vector unsigned char srcM2, srcM1, srcP0, srcP1, srcP2, srcP3;
-    vector unsigned char srcR1 = vec_ld(-2, src);
-    vector unsigned char srcR2 = vec_ld(14, src);
-
-    switch (align) {
-    default: {
-      srcM2 = vec_perm(srcR1, srcR2, permM2);
-      srcM1 = vec_perm(srcR1, srcR2, permM1);
-      srcP0 = vec_perm(srcR1, srcR2, permP0);
-      srcP1 = vec_perm(srcR1, srcR2, permP1);
-      srcP2 = vec_perm(srcR1, srcR2, permP2);
-      srcP3 = vec_perm(srcR1, srcR2, permP3);
-    } break;
-    case 11: {
-      srcM2 = vec_perm(srcR1, srcR2, permM2);
-      srcM1 = vec_perm(srcR1, srcR2, permM1);
-      srcP0 = vec_perm(srcR1, srcR2, permP0);
-      srcP1 = vec_perm(srcR1, srcR2, permP1);
-      srcP2 = vec_perm(srcR1, srcR2, permP2);
-      srcP3 = srcR2;
-    } break;
-    case 12: {
-      vector unsigned char srcR3 = vec_ld(30, src);
-      srcM2 = vec_perm(srcR1, srcR2, permM2);
-      srcM1 = vec_perm(srcR1, srcR2, permM1);
-      srcP0 = vec_perm(srcR1, srcR2, permP0);
-      srcP1 = vec_perm(srcR1, srcR2, permP1);
-      srcP2 = srcR2;
-      srcP3 = vec_perm(srcR2, srcR3, permP3);
-    } break;
-    case 13: {
-      vector unsigned char srcR3 = vec_ld(30, src);
-      srcM2 = vec_perm(srcR1, srcR2, permM2);
-      srcM1 = vec_perm(srcR1, srcR2, permM1);
-      srcP0 = vec_perm(srcR1, srcR2, permP0);
-      srcP1 = srcR2;
-      srcP2 = vec_perm(srcR2, srcR3, permP2);
-      srcP3 = vec_perm(srcR2, srcR3, permP3);
-    } break;
-    case 14: {
-      vector unsigned char srcR3 = vec_ld(30, src);
-      srcM2 = vec_perm(srcR1, srcR2, permM2);
-      srcM1 = vec_perm(srcR1, srcR2, permM1);
-      srcP0 = srcR2;
-      srcP1 = vec_perm(srcR2, srcR3, permP1);
-      srcP2 = vec_perm(srcR2, srcR3, permP2);
-      srcP3 = vec_perm(srcR2, srcR3, permP3);
-    } break;
-    case 15: {
-      vector unsigned char srcR3 = vec_ld(30, src);
-      srcM2 = vec_perm(srcR1, srcR2, permM2);
-      srcM1 = srcR2;
-      srcP0 = vec_perm(srcR2, srcR3, permP0);
-      srcP1 = vec_perm(srcR2, srcR3, permP1);
-      srcP2 = vec_perm(srcR2, srcR3, permP2);
-      srcP3 = vec_perm(srcR2, srcR3, permP3);
-    } break;
-    }
-
-    srcP0A = (vector signed short)
-                            vec_mergeh((vector unsigned char)vzero, srcP0);
-    srcP0B = (vector signed short)
-                            vec_mergel((vector unsigned char)vzero, srcP0);
-    srcP1A = (vector signed short)
-                            vec_mergeh((vector unsigned char)vzero, srcP1);
-    srcP1B = (vector signed short)
-                            vec_mergel((vector unsigned char)vzero, srcP1);
-
-    srcP2A = (vector signed short)
-                            vec_mergeh((vector unsigned char)vzero, srcP2);
-    srcP2B = (vector signed short)
-                            vec_mergel((vector unsigned char)vzero, srcP2);
-    srcP3A = (vector signed short)
-                            vec_mergeh((vector unsigned char)vzero, srcP3);
-    srcP3B = (vector signed short)
-                            vec_mergel((vector unsigned char)vzero, srcP3);
-
-    srcM1A = (vector signed short)
-                            vec_mergeh((vector unsigned char)vzero, srcM1);
-    srcM1B = (vector signed short)
-                            vec_mergel((vector unsigned char)vzero, srcM1);
-    srcM2A = (vector signed short)
-                            vec_mergeh((vector unsigned char)vzero, srcM2);
-    srcM2B = (vector signed short)
-                            vec_mergel((vector unsigned char)vzero, srcM2);
-
-    sum1A = vec_adds(srcP0A, srcP1A);
-    sum1B = vec_adds(srcP0B, srcP1B);
-    sum2A = vec_adds(srcM1A, srcP2A);
-    sum2B = vec_adds(srcM1B, srcP2B);
-    sum3A = vec_adds(srcM2A, srcP3A);
-    sum3B = vec_adds(srcM2B, srcP3B);
-
-    pp1A = vec_mladd(sum1A, v20ss, sum3A);
-    pp1B = vec_mladd(sum1B, v20ss, sum3B);
-
-    pp2A = vec_mladd(sum2A, v5ss, (vector signed short)vzero);
-    pp2B = vec_mladd(sum2B, v5ss, (vector signed short)vzero);
-
-    psumA = vec_sub(pp1A, pp2A);
-    psumB = vec_sub(pp1B, pp2B);
-
-    vec_st(psumA, 0, tmp);
-    vec_st(psumB, 16, tmp);
-
-    src += srcStride;
-    tmp += tmpStride; /* int16_t*, and stride is 16, so it's OK here */
-  }
-
-  tmpM2ssA = vec_ld(0, tmpbis);
-  tmpM2ssB = vec_ld(16, tmpbis);
-  tmpbis += tmpStride;
-  tmpM1ssA = vec_ld(0, tmpbis);
-  tmpM1ssB = vec_ld(16, tmpbis);
-  tmpbis += tmpStride;
-  tmpP0ssA = vec_ld(0, tmpbis);
-  tmpP0ssB = vec_ld(16, tmpbis);
-  tmpbis += tmpStride;
-  tmpP1ssA = vec_ld(0, tmpbis);
-  tmpP1ssB = vec_ld(16, tmpbis);
-  tmpbis += tmpStride;
-  tmpP2ssA = vec_ld(0, tmpbis);
-  tmpP2ssB = vec_ld(16, tmpbis);
-  tmpbis += tmpStride;
-
-  for (i = 0 ; i < 16 ; i++) {
-    const vector signed short tmpP3ssA = vec_ld(0, tmpbis);
-    const vector signed short tmpP3ssB = vec_ld(16, tmpbis);
-
-    const vector signed short sum1A = vec_adds(tmpP0ssA, tmpP1ssA);
-    const vector signed short sum1B = vec_adds(tmpP0ssB, tmpP1ssB);
-    const vector signed short sum2A = vec_adds(tmpM1ssA, tmpP2ssA);
-    const vector signed short sum2B = vec_adds(tmpM1ssB, tmpP2ssB);
-    const vector signed short sum3A = vec_adds(tmpM2ssA, tmpP3ssA);
-    const vector signed short sum3B = vec_adds(tmpM2ssB, tmpP3ssB);
-
-    tmpbis += tmpStride;
-
-    tmpM2ssA = tmpM1ssA;
-    tmpM2ssB = tmpM1ssB;
-    tmpM1ssA = tmpP0ssA;
-    tmpM1ssB = tmpP0ssB;
-    tmpP0ssA = tmpP1ssA;
-    tmpP0ssB = tmpP1ssB;
-    tmpP1ssA = tmpP2ssA;
-    tmpP1ssB = tmpP2ssB;
-    tmpP2ssA = tmpP3ssA;
-    tmpP2ssB = tmpP3ssB;
-
-    pp1Ae = vec_mule(sum1A, v20ss);
-    pp1Ao = vec_mulo(sum1A, v20ss);
-    pp1Be = vec_mule(sum1B, v20ss);
-    pp1Bo = vec_mulo(sum1B, v20ss);
-
-    pp2Ae = vec_mule(sum2A, v5ss);
-    pp2Ao = vec_mulo(sum2A, v5ss);
-    pp2Be = vec_mule(sum2B, v5ss);
-    pp2Bo = vec_mulo(sum2B, v5ss);
-
-    pp3Ae = vec_sra((vector signed int)sum3A, v16ui);
-    pp3Ao = vec_mulo(sum3A, v1ss);
-    pp3Be = vec_sra((vector signed int)sum3B, v16ui);
-    pp3Bo = vec_mulo(sum3B, v1ss);
-
-    pp1cAe = vec_add(pp1Ae, v512si);
-    pp1cAo = vec_add(pp1Ao, v512si);
-    pp1cBe = vec_add(pp1Be, v512si);
-    pp1cBo = vec_add(pp1Bo, v512si);
-
-    pp32Ae = vec_sub(pp3Ae, pp2Ae);
-    pp32Ao = vec_sub(pp3Ao, pp2Ao);
-    pp32Be = vec_sub(pp3Be, pp2Be);
-    pp32Bo = vec_sub(pp3Bo, pp2Bo);
-
-    sumAe = vec_add(pp1cAe, pp32Ae);
-    sumAo = vec_add(pp1cAo, pp32Ao);
-    sumBe = vec_add(pp1cBe, pp32Be);
-    sumBo = vec_add(pp1cBo, pp32Bo);
-
-    ssumAe = vec_sra(sumAe, v10ui);
-    ssumAo = vec_sra(sumAo, v10ui);
-    ssumBe = vec_sra(sumBe, v10ui);
-    ssumBo = vec_sra(sumBo, v10ui);
-
-    ssume = vec_packs(ssumAe, ssumBe);
-    ssumo = vec_packs(ssumAo, ssumBo);
-
-    sumv = vec_packsu(ssume, ssumo);
-    sum = vec_perm(sumv, sumv, mperm);
-
-    dst1 = vec_ld(0, dst);
-    dst2 = vec_ld(16, dst);
-    vdst = vec_perm(dst1, dst2, vec_lvsl(0, dst));
-
-    OP_U8_ALTIVEC(fsum, sum, vdst);
-
-    rsum = vec_perm(fsum, fsum, dstperm);
-    fdst1 = vec_sel(dst1, rsum, dstmask);
-    fdst2 = vec_sel(rsum, dst2, dstmask);
-
-    vec_st(fdst1, 0, dst);
-    vec_st(fdst2, 16, dst);
-
-    dst += dstStride;
-  }
-  POWERPC_PERF_STOP_COUNT(PREFIX_h264_qpel16_hv_lowpass_num, 1);
-}

Modified: branches/avidemux_2.4_branch/avidemux/ADM_libraries/ADM_lavcodec/ppc/dsputil_ppc.c
===================================================================
--- branches/avidemux_2.4_branch/avidemux/ADM_libraries/ADM_lavcodec/ppc/dsputil_ppc.c	2007-12-03 12:38:06 UTC (rev 3719)
+++ branches/avidemux_2.4_branch/avidemux/ADM_libraries/ADM_lavcodec/ppc/dsputil_ppc.c	2007-12-04 00:19:48 UTC (rev 3720)
@@ -20,7 +20,7 @@
  * Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA
  */
 
-#include "../dsputil.h"
+#include "dsputil.h"
 
 #include "dsputil_ppc.h"
 
@@ -39,6 +39,7 @@
 void vc1dsp_init_altivec(DSPContext* c, AVCodecContext *avctx);
 void snow_init_altivec(DSPContext* c, AVCodecContext *avctx);
 void float_init_altivec(DSPContext* c, AVCodecContext *avctx);
+void int_init_altivec(DSPContext* c, AVCodecContext *avctx);
 
 #endif
 
@@ -55,7 +56,7 @@
     return result;
 }
 
-#ifdef POWERPC_PERFORMANCE_REPORT
+#ifdef CONFIG_POWERPC_PERF
 unsigned long long perfdata[POWERPC_NUM_PMC_ENABLED][powerpc_perf_total][powerpc_data_total];
 /* list below must match enum in dsputil_ppc.h */
 static unsigned char* perfname[] = {
@@ -90,7 +91,7 @@
 #include <stdio.h>
 #endif
 
-#ifdef POWERPC_PERFORMANCE_REPORT
+#ifdef CONFIG_POWERPC_PERF
 void powerpc_display_perf_report(void)
 {
   int i, j;
@@ -112,7 +113,7 @@
       }
   }
 }
-#endif /* POWERPC_PERFORMANCE_REPORT */
+#endif /* CONFIG_POWERPC_PERF */
 
 /* ***** WARNING ***** WARNING ***** WARNING ***** */
 /*
@@ -284,6 +285,7 @@
         if(ENABLE_VC1_DECODER || ENABLE_WMV3_DECODER)
             vc1dsp_init_altivec(c, avctx);
         float_init_altivec(c, avctx);
+        int_init_altivec(c, avctx);
         c->gmc1 = gmc1_altivec;
 
 #ifdef CONFIG_ENCODERS
@@ -305,7 +307,7 @@
         }
         }
 
-#ifdef POWERPC_PERFORMANCE_REPORT
+#ifdef CONFIG_POWERPC_PERF
         {
           int i, j;
           for (i = 0 ; i < powerpc_perf_total ; i++)
@@ -319,7 +321,7 @@
               }
           }
         }
-#endif /* POWERPC_PERFORMANCE_REPORT */
+#endif /* CONFIG_POWERPC_PERF */
     }
 #endif /* HAVE_ALTIVEC */
 }

Modified: branches/avidemux_2.4_branch/avidemux/ADM_libraries/ADM_lavcodec/ppc/dsputil_ppc.h
===================================================================
--- branches/avidemux_2.4_branch/avidemux/ADM_libraries/ADM_lavcodec/ppc/dsputil_ppc.h	2007-12-03 12:38:06 UTC (rev 3719)
+++ branches/avidemux_2.4_branch/avidemux/ADM_libraries/ADM_lavcodec/ppc/dsputil_ppc.h	2007-12-04 00:19:48 UTC (rev 3720)
@@ -21,11 +21,11 @@
 #ifndef _DSPUTIL_PPC_
 #define _DSPUTIL_PPC_
 
-#ifdef POWERPC_PERFORMANCE_REPORT
+#ifdef CONFIG_POWERPC_PERF
 void powerpc_display_perf_report(void);
 /* the 604* have 2, the G3* have 4, the G4s have 6,
    and the G5 are completely different (they MUST use
-   POWERPC_MODE_64BITS, and let's hope all future 64 bis PPC
+   HAVE_PPC64, and let's hope all future 64 bis PPC
    will use the same PMCs... */
 #define POWERPC_NUM_PMC_ENABLED 6
 /* if you add to the enum below, also add to the perfname array
@@ -68,7 +68,7 @@
 };
 extern unsigned long long perfdata[POWERPC_NUM_PMC_ENABLED][powerpc_perf_total][powerpc_data_total];
 
-#ifndef POWERPC_MODE_64BITS
+#ifndef HAVE_PPC64
 #define POWERP_PMC_DATATYPE unsigned long
 #define POWERPC_GET_PMC1(a) asm volatile("mfspr %0, 937" : "=r" (a))
 #define POWERPC_GET_PMC2(a) asm volatile("mfspr %0, 938" : "=r" (a))
@@ -86,7 +86,7 @@
 #define POWERPC_GET_PMC5(a) do {} while (0)
 #define POWERPC_GET_PMC6(a) do {} while (0)
 #endif
-#else /* POWERPC_MODE_64BITS */
+#else /* HAVE_PPC64 */
 #define POWERP_PMC_DATATYPE unsigned long long
 #define POWERPC_GET_PMC1(a) asm volatile("mfspr %0, 771" : "=r" (a))
 #define POWERPC_GET_PMC2(a) asm volatile("mfspr %0, 772" : "=r" (a))
@@ -104,7 +104,7 @@
 #define POWERPC_GET_PMC5(a) do {} while (0)
 #define POWERPC_GET_PMC6(a) do {} while (0)
 #endif
-#endif /* POWERPC_MODE_64BITS */
+#endif /* HAVE_PPC64 */
 #define POWERPC_PERF_DECLARE(a, cond)   \
   POWERP_PMC_DATATYPE                   \
     pmc_start[POWERPC_NUM_PMC_ENABLED], \
@@ -145,11 +145,11 @@
     }                             \
   }                               \
 } while (0)
-#else /* POWERPC_PERFORMANCE_REPORT */
+#else /* CONFIG_POWERPC_PERF */
 // those are needed to avoid empty statements.
 #define POWERPC_PERF_DECLARE(a, cond)        int altivec_placeholder __attribute__ ((unused))
 #define POWERPC_PERF_START_COUNT(a, cond)    do {} while (0)
 #define POWERPC_PERF_STOP_COUNT(a, cond)     do {} while (0)
-#endif /* POWERPC_PERFORMANCE_REPORT */
+#endif /* CONFIG_POWERPC_PERF */
 
 #endif /*  _DSPUTIL_PPC_ */

Modified: branches/avidemux_2.4_branch/avidemux/ADM_libraries/ADM_lavcodec/ppc/fdct_altivec.c
===================================================================
--- branches/avidemux_2.4_branch/avidemux/ADM_libraries/ADM_lavcodec/ppc/fdct_altivec.c	2007-12-03 12:38:06 UTC (rev 3719)
+++ branches/avidemux_2.4_branch/avidemux/ADM_libraries/ADM_lavcodec/ppc/fdct_altivec.c	2007-12-04 00:19:48 UTC (rev 3720)
@@ -21,7 +21,7 @@
 
 
 #include "common.h"
-#include "../dsputil.h"
+#include "dsputil.h"
 #include "dsputil_altivec.h"
 #include "gcc_fixes.h"
 

Modified: branches/avidemux_2.4_branch/avidemux/ADM_libraries/ADM_lavcodec/ppc/fft_altivec.c
===================================================================
--- branches/avidemux_2.4_branch/avidemux/ADM_libraries/ADM_lavcodec/ppc/fft_altivec.c	2007-12-03 12:38:06 UTC (rev 3719)
+++ branches/avidemux_2.4_branch/avidemux/ADM_libraries/ADM_lavcodec/ppc/fft_altivec.c	2007-12-04 00:19:48 UTC (rev 3720)
@@ -20,7 +20,7 @@
  * License along with FFmpeg; if not, write to the Free Software
  * Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA
  */
-#include "../dsputil.h"
+#include "dsputil.h"
 
 #include "gcc_fixes.h"
 

Added: branches/avidemux_2.4_branch/avidemux/ADM_libraries/ADM_lavcodec/ppc/float_altivec.c
===================================================================
--- branches/avidemux_2.4_branch/avidemux/ADM_libraries/ADM_lavcodec/ppc/float_altivec.c	2007-12-03 12:38:06 UTC (rev 3719)
+++ branches/avidemux_2.4_branch/avidemux/ADM_libraries/ADM_lavcodec/ppc/float_altivec.c	2007-12-04 00:19:48 UTC (rev 3720)
@@ -0,0 +1,193 @@
+/*
+ * Copyright (c) 2006 Luca Barbato <lu_zero at gentoo.org>
+ *
+ * This file is part of FFmpeg.
+ *
+ * FFmpeg is free software; you can redistribute it and/or
+ * modify it under the terms of the GNU Lesser General Public
+ * License as published by the Free Software Foundation; either
+ * version 2.1 of the License, or (at your option) any later version.
+ *
+ * FFmpeg is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+ * Lesser General Public License for more details.
+ *
+ * You should have received a copy of the GNU Lesser General Public
+ * License along with FFmpeg; if not, write to the Free Software
+ * Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA
+ */
+
+#include "dsputil.h"
+
+#include "gcc_fixes.h"
+
+#include "dsputil_altivec.h"
+
+static void vector_fmul_altivec(float *dst, const float *src, int len)
+{
+    int i;
+    vector float d0, d1, s, zero = (vector float)vec_splat_u32(0);
+    for(i=0; i<len-7; i+=8) {
+        d0 = vec_ld(0, dst+i);
+        s = vec_ld(0, src+i);
+        d1 = vec_ld(16, dst+i);
+        d0 = vec_madd(d0, s, zero);
+        d1 = vec_madd(d1, vec_ld(16,src+i), zero);
+        vec_st(d0, 0, dst+i);
+        vec_st(d1, 16, dst+i);
+    }
+}
+
+static void vector_fmul_reverse_altivec(float *dst, const float *src0,
+                                        const float *src1, int len)
+{
+    int i;
+    vector float d, s0, s1, h0, l0,
+                 s2, s3, zero = (vector float)vec_splat_u32(0);
+    src1 += len-4;
+    for(i=0; i<len-7; i+=8) {
+        s1 = vec_ld(0, src1-i);              // [a,b,c,d]
+        s0 = vec_ld(0, src0+i);
+        l0 = vec_mergel(s1, s1);             // [c,c,d,d]
+        s3 = vec_ld(-16, src1-i);
+        h0 = vec_mergeh(s1, s1);             // [a,a,b,b]
+        s2 = vec_ld(16, src0+i);
+        s1 = vec_mergeh(vec_mergel(l0,h0),   // [d,b,d,b]
+                        vec_mergeh(l0,h0));  // [c,a,c,a]
+                                             // [d,c,b,a]
+        l0 = vec_mergel(s3, s3);
+        d = vec_madd(s0, s1, zero);
+        h0 = vec_mergeh(s3, s3);
+        vec_st(d, 0, dst+i);
+        s3 = vec_mergeh(vec_mergel(l0,h0),
+                        vec_mergeh(l0,h0));
+        d = vec_madd(s2, s3, zero);
+        vec_st(d, 16, dst+i);
+    }
+}
+
+static void vector_fmul_add_add_altivec(float *dst, const float *src0,
+                                        const float *src1, const float *src2,
+                                        int src3, int len, int step)
+{
+    int i;
+    vector float d, s0, s1, s2, t0, t1, edges;
+    vector unsigned char align = vec_lvsr(0,dst),
+                         mask = vec_lvsl(0, dst);
+
+#if 0 //FIXME: there is still something wrong
+    if (step == 2) {
+        int y;
+        vector float d0, d1, s3, t2;
+        vector unsigned int sel =
+                vec_mergeh(vec_splat_u32(-1), vec_splat_u32(0));
+        t1 = vec_ld(16, dst);
+        for (i=0,y=0; i<len-3; i+=4,y+=8) {
+
+            s0 = vec_ld(0,src0+i);
+            s1 = vec_ld(0,src1+i);
+            s2 = vec_ld(0,src2+i);
+
+//          t0 = vec_ld(0, dst+y);  //[x x x|a]
+//          t1 = vec_ld(16, dst+y); //[b c d|e]
+            t2 = vec_ld(31, dst+y); //[f g h|x]
+
+            d = vec_madd(s0,s1,s2); // [A B C D]
+
+                                                 // [A A B B]
+
+                                                 // [C C D D]
+
+            d0 = vec_perm(t0, t1, mask); // [a b c d]
+
+            d0 = vec_sel(vec_mergeh(d, d), d0, sel);   // [A b B d]
+
+            edges = vec_perm(t1, t0, mask);
+
+            t0 = vec_perm(edges, d0, align); // [x x x|A]
+
+            t1 = vec_perm(d0, edges, align); // [b B d|e]
+
+            vec_stl(t0, 0, dst+y);
+
+            d1 = vec_perm(t1, t2, mask); // [e f g h]
+
+            d1 = vec_sel(vec_mergel(d, d), d1, sel); // [C f D h]
+
+            edges = vec_perm(t2, t1, mask);
+
+            t1 = vec_perm(edges, d1, align); // [b B d|C]
+
+            t2 = vec_perm(d1, edges, align); // [f D h|x]
+
+            vec_stl(t1, 16, dst+y);
+
+            t0 = t1;
+
+            vec_stl(t2, 31, dst+y);
+
+            t1 = t2;
+        }
+    } else
+    #endif
+    if (step == 1 && src3 == 0)
+        for (i=0; i<len-3; i+=4) {
+            t0 = vec_ld(0, dst+i);
+            t1 = vec_ld(15, dst+i);
+            s0 = vec_ld(0, src0+i);
+            s1 = vec_ld(0, src1+i);
+            s2 = vec_ld(0, src2+i);
+            edges = vec_perm(t1 ,t0, mask);
+            d = vec_madd(s0,s1,s2);
+            t1 = vec_perm(d, edges, align);
+            t0 = vec_perm(edges, d, align);
+            vec_st(t1, 15, dst+i);
+            vec_st(t0, 0, dst+i);
+        }
+    else
+        ff_vector_fmul_add_add_c(dst, src0, src1, src2, src3, len, step);
+}
+
+void float_to_int16_altivec(int16_t *dst, const float *src, int len)
+{
+    int i;
+    vector float s0, s1;
+    vector signed int t0, t1;
+    vector signed short d0, d1, d;
+    vector unsigned char align;
+    if(((long)dst)&15) //FIXME
+    for(i=0; i<len-7; i+=8) {
+        s0 = vec_ld(0, src+i);
+        s1 = vec_ld(16, src+i);
+        t0 = vec_cts(s0, 0);
+        d0 = vec_ld(0, dst+i);
+        t1 = vec_cts(s1, 0);
+        d1 = vec_ld(15, dst+i);
+        d = vec_packs(t0,t1);
+        d1 = vec_perm(d1, d0, vec_lvsl(0,dst+i));
+        align = vec_lvsr(0, dst+i);
+        d0 = vec_perm(d1, d, align);
+        d1 = vec_perm(d, d1, align);
+        vec_st(d0, 0, dst+i);
+        vec_st(d1,15, dst+i);
+    }
+    else
+    for(i=0; i<len-7; i+=8) {
+        s0 = vec_ld(0, src+i);
+        s1 = vec_ld(16, src+i);
+        t0 = vec_cts(s0, 0);
+        t1 = vec_cts(s1, 0);
+        d = vec_packs(t0,t1);
+        vec_st(d, 0, dst+i);
+    }
+}
+
+void float_init_altivec(DSPContext* c, AVCodecContext *avctx)
+{
+    c->vector_fmul = vector_fmul_altivec;
+    c->vector_fmul_reverse = vector_fmul_reverse_altivec;
+    c->vector_fmul_add_add = vector_fmul_add_add_altivec;
+    if(!(avctx->flags & CODEC_FLAG_BITEXACT))
+        c->float_to_int16 = float_to_int16_altivec;
+}

Modified: branches/avidemux_2.4_branch/avidemux/ADM_libraries/ADM_lavcodec/ppc/gmc_altivec.c
===================================================================
--- branches/avidemux_2.4_branch/avidemux/ADM_libraries/ADM_lavcodec/ppc/gmc_altivec.c	2007-12-03 12:38:06 UTC (rev 3719)
+++ branches/avidemux_2.4_branch/avidemux/ADM_libraries/ADM_lavcodec/ppc/gmc_altivec.c	2007-12-04 00:19:48 UTC (rev 3720)
@@ -20,7 +20,7 @@
  * Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA
  */
 
-#include "../dsputil.h"
+#include "dsputil.h"
 
 #include "gcc_fixes.h"
 
@@ -34,10 +34,10 @@
 void gmc1_altivec(uint8_t *dst /* align 8 */, uint8_t *src /* align1 */, int stride, int h, int x16, int y16, int rounder)
 {
 POWERPC_PERF_DECLARE(altivec_gmc1_num, GMC1_PERF_COND);
-    const unsigned short __attribute__ ((aligned(16))) rounder_a[8] =
+    const DECLARE_ALIGNED_16(unsigned short, rounder_a[8]) =
       {rounder, rounder, rounder, rounder,
        rounder, rounder, rounder, rounder};
-    const unsigned short __attribute__ ((aligned(16))) ABCD[8] =
+    const DECLARE_ALIGNED_16(unsigned short, ABCD[8]) =
       {
         (16-x16)*(16-y16), /* A */
         (   x16)*(16-y16), /* B */

Copied: branches/avidemux_2.4_branch/avidemux/ADM_libraries/ADM_lavcodec/ppc/h264_altivec.c (from rev 3719, branches/avidemux_2.4_branch/avidemux/ADM_libraries/ADM_lavcodec/ppc/dsputil_h264_altivec.c)
===================================================================
--- branches/avidemux_2.4_branch/avidemux/ADM_libraries/ADM_lavcodec/ppc/dsputil_h264_altivec.c	2007-12-03 12:38:06 UTC (rev 3719)
+++ branches/avidemux_2.4_branch/avidemux/ADM_libraries/ADM_lavcodec/ppc/h264_altivec.c	2007-12-04 00:19:48 UTC (rev 3720)
@@ -0,0 +1,917 @@
+/*
+ * Copyright (c) 2004 Romain Dolbeau <romain at dolbeau.org>
+ *
+ * This file is part of FFmpeg.
+ *
+ * FFmpeg is free software; you can redistribute it and/or
+ * modify it under the terms of the GNU Lesser General Public
+ * License as published by the Free Software Foundation; either
+ * version 2.1 of the License, or (at your option) any later version.
+ *
+ * FFmpeg is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+ * Lesser General Public License for more details.
+ *
+ * You should have received a copy of the GNU Lesser General Public
+ * License along with FFmpeg; if not, write to the Free Software
+ * Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA
+ */
+
+#include "dsputil.h"
+
+#include "gcc_fixes.h"
+
+#include "dsputil_altivec.h"
+#include "types_altivec.h"
+
+#define PUT_OP_U8_ALTIVEC(d, s, dst) d = s
+#define AVG_OP_U8_ALTIVEC(d, s, dst) d = vec_avg(dst, s)
+
+#define OP_U8_ALTIVEC                          PUT_OP_U8_ALTIVEC
+#define PREFIX_h264_chroma_mc8_altivec         put_h264_chroma_mc8_altivec
+#define PREFIX_h264_chroma_mc8_num             altivec_put_h264_chroma_mc8_num
+#define PREFIX_h264_qpel16_h_lowpass_altivec   put_h264_qpel16_h_lowpass_altivec
+#define PREFIX_h264_qpel16_h_lowpass_num       altivec_put_h264_qpel16_h_lowpass_num
+#define PREFIX_h264_qpel16_v_lowpass_altivec   put_h264_qpel16_v_lowpass_altivec
+#define PREFIX_h264_qpel16_v_lowpass_num       altivec_put_h264_qpel16_v_lowpass_num
+#define PREFIX_h264_qpel16_hv_lowpass_altivec  put_h264_qpel16_hv_lowpass_altivec
+#define PREFIX_h264_qpel16_hv_lowpass_num      altivec_put_h264_qpel16_hv_lowpass_num
+#include "h264_template_altivec.c"
+#undef OP_U8_ALTIVEC
+#undef PREFIX_h264_chroma_mc8_altivec
+#undef PREFIX_h264_chroma_mc8_num
+#undef PREFIX_h264_qpel16_h_lowpass_altivec
+#undef PREFIX_h264_qpel16_h_lowpass_num
+#undef PREFIX_h264_qpel16_v_lowpass_altivec
+#undef PREFIX_h264_qpel16_v_lowpass_num
+#undef PREFIX_h264_qpel16_hv_lowpass_altivec
+#undef PREFIX_h264_qpel16_hv_lowpass_num
+
+#define OP_U8_ALTIVEC                          AVG_OP_U8_ALTIVEC
+#define PREFIX_h264_chroma_mc8_altivec         avg_h264_chroma_mc8_altivec
+#define PREFIX_h264_chroma_mc8_num             altivec_avg_h264_chroma_mc8_num
+#define PREFIX_h264_qpel16_h_lowpass_altivec   avg_h264_qpel16_h_lowpass_altivec
+#define PREFIX_h264_qpel16_h_lowpass_num       altivec_avg_h264_qpel16_h_lowpass_num
+#define PREFIX_h264_qpel16_v_lowpass_altivec   avg_h264_qpel16_v_lowpass_altivec
+#define PREFIX_h264_qpel16_v_lowpass_num       altivec_avg_h264_qpel16_v_lowpass_num
+#define PREFIX_h264_qpel16_hv_lowpass_altivec  avg_h264_qpel16_hv_lowpass_altivec
+#define PREFIX_h264_qpel16_hv_lowpass_num      altivec_avg_h264_qpel16_hv_lowpass_num
+#include "h264_template_altivec.c"
+#undef OP_U8_ALTIVEC
+#undef PREFIX_h264_chroma_mc8_altivec
+#undef PREFIX_h264_chroma_mc8_num
+#undef PREFIX_h264_qpel16_h_lowpass_altivec
+#undef PREFIX_h264_qpel16_h_lowpass_num
+#undef PREFIX_h264_qpel16_v_lowpass_altivec
+#undef PREFIX_h264_qpel16_v_lowpass_num
+#undef PREFIX_h264_qpel16_hv_lowpass_altivec
+#undef PREFIX_h264_qpel16_hv_lowpass_num
+
+#define H264_MC(OPNAME, SIZE, CODETYPE) \
+static void OPNAME ## h264_qpel ## SIZE ## _mc00_ ## CODETYPE (uint8_t *dst, uint8_t *src, int stride){\
+    OPNAME ## pixels ## SIZE ## _ ## CODETYPE(dst, src, stride, SIZE);\
+}\
+\
+static void OPNAME ## h264_qpel ## SIZE ## _mc10_ ## CODETYPE(uint8_t *dst, uint8_t *src, int stride){ \
+    DECLARE_ALIGNED_16(uint8_t, half[SIZE*SIZE]);\
+    put_h264_qpel ## SIZE ## _h_lowpass_ ## CODETYPE(half, src, SIZE, stride);\
+    OPNAME ## pixels ## SIZE ## _l2_ ## CODETYPE(dst, src, half, stride, stride, SIZE);\
+}\
+\
+static void OPNAME ## h264_qpel ## SIZE ## _mc20_ ## CODETYPE(uint8_t *dst, uint8_t *src, int stride){\
+    OPNAME ## h264_qpel ## SIZE ## _h_lowpass_ ## CODETYPE(dst, src, stride, stride);\
+}\
+\
+static void OPNAME ## h264_qpel ## SIZE ## _mc30_ ## CODETYPE(uint8_t *dst, uint8_t *src, int stride){\
+    DECLARE_ALIGNED_16(uint8_t, half[SIZE*SIZE]);\
+    put_h264_qpel ## SIZE ## _h_lowpass_ ## CODETYPE(half, src, SIZE, stride);\
+    OPNAME ## pixels ## SIZE ## _l2_ ## CODETYPE(dst, src+1, half, stride, stride, SIZE);\
+}\
+\
+static void OPNAME ## h264_qpel ## SIZE ## _mc01_ ## CODETYPE(uint8_t *dst, uint8_t *src, int stride){\
+    DECLARE_ALIGNED_16(uint8_t, half[SIZE*SIZE]);\
+    put_h264_qpel ## SIZE ## _v_lowpass_ ## CODETYPE(half, src, SIZE, stride);\
+    OPNAME ## pixels ## SIZE ## _l2_ ## CODETYPE(dst, src, half, stride, stride, SIZE);\
+}\
+\
+static void OPNAME ## h264_qpel ## SIZE ## _mc02_ ## CODETYPE(uint8_t *dst, uint8_t *src, int stride){\
+    OPNAME ## h264_qpel ## SIZE ## _v_lowpass_ ## CODETYPE(dst, src, stride, stride);\
+}\
+\
+static void OPNAME ## h264_qpel ## SIZE ## _mc03_ ## CODETYPE(uint8_t *dst, uint8_t *src, int stride){\
+    DECLARE_ALIGNED_16(uint8_t, half[SIZE*SIZE]);\
+    put_h264_qpel ## SIZE ## _v_lowpass_ ## CODETYPE(half, src, SIZE, stride);\
+    OPNAME ## pixels ## SIZE ## _l2_ ## CODETYPE(dst, src+stride, half, stride, stride, SIZE);\
+}\
+\
+static void OPNAME ## h264_qpel ## SIZE ## _mc11_ ## CODETYPE(uint8_t *dst, uint8_t *src, int stride){\
+    DECLARE_ALIGNED_16(uint8_t, halfH[SIZE*SIZE]);\
+    DECLARE_ALIGNED_16(uint8_t, halfV[SIZE*SIZE]);\
+    put_h264_qpel ## SIZE ## _h_lowpass_ ## CODETYPE(halfH, src, SIZE, stride);\
+    put_h264_qpel ## SIZE ## _v_lowpass_ ## CODETYPE(halfV, src, SIZE, stride);\
+    OPNAME ## pixels ## SIZE ## _l2_ ## CODETYPE(dst, halfH, halfV, stride, SIZE, SIZE);\
+}\
+\
+static void OPNAME ## h264_qpel ## SIZE ## _mc31_ ## CODETYPE(uint8_t *dst, uint8_t *src, int stride){\
+    DECLARE_ALIGNED_16(uint8_t, halfH[SIZE*SIZE]);\
+    DECLARE_ALIGNED_16(uint8_t, halfV[SIZE*SIZE]);\
+    put_h264_qpel ## SIZE ## _h_lowpass_ ## CODETYPE(halfH, src, SIZE, stride);\
+    put_h264_qpel ## SIZE ## _v_lowpass_ ## CODETYPE(halfV, src+1, SIZE, stride);\
+    OPNAME ## pixels ## SIZE ## _l2_ ## CODETYPE(dst, halfH, halfV, stride, SIZE, SIZE);\
+}\
+\
+static void OPNAME ## h264_qpel ## SIZE ## _mc13_ ## CODETYPE(uint8_t *dst, uint8_t *src, int stride){\
+    DECLARE_ALIGNED_16(uint8_t, halfH[SIZE*SIZE]);\
+    DECLARE_ALIGNED_16(uint8_t, halfV[SIZE*SIZE]);\
+    put_h264_qpel ## SIZE ## _h_lowpass_ ## CODETYPE(halfH, src + stride, SIZE, stride);\
+    put_h264_qpel ## SIZE ## _v_lowpass_ ## CODETYPE(halfV, src, SIZE, stride);\
+    OPNAME ## pixels ## SIZE ## _l2_ ## CODETYPE(dst, halfH, halfV, stride, SIZE, SIZE);\
+}\
+\
+static void OPNAME ## h264_qpel ## SIZE ## _mc33_ ## CODETYPE(uint8_t *dst, uint8_t *src, int stride){\
+    DECLARE_ALIGNED_16(uint8_t, halfH[SIZE*SIZE]);\
+    DECLARE_ALIGNED_16(uint8_t, halfV[SIZE*SIZE]);\
+    put_h264_qpel ## SIZE ## _h_lowpass_ ## CODETYPE(halfH, src + stride, SIZE, stride);\
+    put_h264_qpel ## SIZE ## _v_lowpass_ ## CODETYPE(halfV, src+1, SIZE, stride);\
+    OPNAME ## pixels ## SIZE ## _l2_ ## CODETYPE(dst, halfH, halfV, stride, SIZE, SIZE);\
+}\
+\
+static void OPNAME ## h264_qpel ## SIZE ## _mc22_ ## CODETYPE(uint8_t *dst, uint8_t *src, int stride){\
+    DECLARE_ALIGNED_16(int16_t, tmp[SIZE*(SIZE+8)]);\
+    OPNAME ## h264_qpel ## SIZE ## _hv_lowpass_ ## CODETYPE(dst, tmp, src, stride, SIZE, stride);\
+}\
+\
+static void OPNAME ## h264_qpel ## SIZE ## _mc21_ ## CODETYPE(uint8_t *dst, uint8_t *src, int stride){\
+    DECLARE_ALIGNED_16(uint8_t, halfH[SIZE*SIZE]);\
+    DECLARE_ALIGNED_16(uint8_t, halfHV[SIZE*SIZE]);\
+    DECLARE_ALIGNED_16(int16_t, tmp[SIZE*(SIZE+8)]);\
+    put_h264_qpel ## SIZE ## _h_lowpass_ ## CODETYPE(halfH, src, SIZE, stride);\
+    put_h264_qpel ## SIZE ## _hv_lowpass_ ## CODETYPE(halfHV, tmp, src, SIZE, SIZE, stride);\
+    OPNAME ## pixels ## SIZE ## _l2_ ## CODETYPE(dst, halfH, halfHV, stride, SIZE, SIZE);\
+}\
+\
+static void OPNAME ## h264_qpel ## SIZE ## _mc23_ ## CODETYPE(uint8_t *dst, uint8_t *src, int stride){\
+    DECLARE_ALIGNED_16(uint8_t, halfH[SIZE*SIZE]);\
+    DECLARE_ALIGNED_16(uint8_t, halfHV[SIZE*SIZE]);\
+    DECLARE_ALIGNED_16(int16_t, tmp[SIZE*(SIZE+8)]);\
+    put_h264_qpel ## SIZE ## _h_lowpass_ ## CODETYPE(halfH, src + stride, SIZE, stride);\
+    put_h264_qpel ## SIZE ## _hv_lowpass_ ## CODETYPE(halfHV, tmp, src, SIZE, SIZE, stride);\
+    OPNAME ## pixels ## SIZE ## _l2_ ## CODETYPE(dst, halfH, halfHV, stride, SIZE, SIZE);\
+}\
+\
+static void OPNAME ## h264_qpel ## SIZE ## _mc12_ ## CODETYPE(uint8_t *dst, uint8_t *src, int stride){\
+    DECLARE_ALIGNED_16(uint8_t, halfV[SIZE*SIZE]);\
+    DECLARE_ALIGNED_16(uint8_t, halfHV[SIZE*SIZE]);\
+    DECLARE_ALIGNED_16(int16_t, tmp[SIZE*(SIZE+8)]);\
+    put_h264_qpel ## SIZE ## _v_lowpass_ ## CODETYPE(halfV, src, SIZE, stride);\
+    put_h264_qpel ## SIZE ## _hv_lowpass_ ## CODETYPE(halfHV, tmp, src, SIZE, SIZE, stride);\
+    OPNAME ## pixels ## SIZE ## _l2_ ## CODETYPE(dst, halfV, halfHV, stride, SIZE, SIZE);\
+}\
+\
+static void OPNAME ## h264_qpel ## SIZE ## _mc32_ ## CODETYPE(uint8_t *dst, uint8_t *src, int stride){\
+    DECLARE_ALIGNED_16(uint8_t, halfV[SIZE*SIZE]);\
+    DECLARE_ALIGNED_16(uint8_t, halfHV[SIZE*SIZE]);\
+    DECLARE_ALIGNED_16(int16_t, tmp[SIZE*(SIZE+8)]);\
+    put_h264_qpel ## SIZE ## _v_lowpass_ ## CODETYPE(halfV, src+1, SIZE, stride);\
+    put_h264_qpel ## SIZE ## _hv_lowpass_ ## CODETYPE(halfHV, tmp, src, SIZE, SIZE, stride);\
+    OPNAME ## pixels ## SIZE ## _l2_ ## CODETYPE(dst, halfV, halfHV, stride, SIZE, SIZE);\
+}\
+
+/* this code assume that stride % 16 == 0 */
+void put_no_rnd_h264_chroma_mc8_altivec(uint8_t * dst, uint8_t * src, int stride, int h, int x, int y) {
+   DECLARE_ALIGNED_16(signed int, ABCD[4]) =
+                        {((8 - x) * (8 - y)),
+                          ((x) * (8 - y)),
+                          ((8 - x) * (y)),
+                          ((x) * (y))};
+    register int i;
+    vector unsigned char fperm;
+    const vector signed int vABCD = vec_ld(0, ABCD);
+    const vector signed short vA = vec_splat((vector signed short)vABCD, 1);
+    const vector signed short vB = vec_splat((vector signed short)vABCD, 3);
+    const vector signed short vC = vec_splat((vector signed short)vABCD, 5);
+    const vector signed short vD = vec_splat((vector signed short)vABCD, 7);
+    const vector signed int vzero = vec_splat_s32(0);
+    const vector signed short v28ss = vec_sub(vec_sl(vec_splat_s16(1),vec_splat_u16(5)),vec_splat_s16(4));
+    const vector unsigned short v6us = vec_splat_u16(6);
+    register int loadSecond = (((unsigned long)src) % 16) <= 7 ? 0 : 1;
+    register int reallyBadAlign = (((unsigned long)src) % 16) == 15 ? 1 : 0;
+
+    vector unsigned char vsrcAuc, vsrcBuc, vsrcperm0, vsrcperm1;
+    vector unsigned char vsrc0uc, vsrc1uc;
+    vector signed short vsrc0ssH, vsrc1ssH;
+    vector unsigned char vsrcCuc, vsrc2uc, vsrc3uc;
+    vector signed short vsrc2ssH, vsrc3ssH, psum;
+    vector unsigned char vdst, ppsum, fsum;
+
+    if (((unsigned long)dst) % 16 == 0) {
+      fperm = (vector unsigned char)AVV(0x10, 0x11, 0x12, 0x13,
+                                        0x14, 0x15, 0x16, 0x17,
+                                        0x08, 0x09, 0x0A, 0x0B,
+                                        0x0C, 0x0D, 0x0E, 0x0F);
+    } else {
+      fperm = (vector unsigned char)AVV(0x00, 0x01, 0x02, 0x03,
+                                        0x04, 0x05, 0x06, 0x07,
+                                        0x18, 0x19, 0x1A, 0x1B,
+                                        0x1C, 0x1D, 0x1E, 0x1F);
+    }
+
+    vsrcAuc = vec_ld(0, src);
+
+    if (loadSecond)
+      vsrcBuc = vec_ld(16, src);
+    vsrcperm0 = vec_lvsl(0, src);
+    vsrcperm1 = vec_lvsl(1, src);
+
+    vsrc0uc = vec_perm(vsrcAuc, vsrcBuc, vsrcperm0);
+    if (reallyBadAlign)
+      vsrc1uc = vsrcBuc;
+    else
+      vsrc1uc = vec_perm(vsrcAuc, vsrcBuc, vsrcperm1);
+
+    vsrc0ssH = (vector signed short)vec_mergeh((vector unsigned char)vzero,
+                                               (vector unsigned char)vsrc0uc);
+    vsrc1ssH = (vector signed short)vec_mergeh((vector unsigned char)vzero,
+                                               (vector unsigned char)vsrc1uc);
+
+    if (!loadSecond) {// -> !reallyBadAlign
+      for (i = 0 ; i < h ; i++) {
+
+
+        vsrcCuc = vec_ld(stride + 0, src);
+
+        vsrc2uc = vec_perm(vsrcCuc, vsrcCuc, vsrcperm0);
+        vsrc3uc = vec_perm(vsrcCuc, vsrcCuc, vsrcperm1);
+
+        vsrc2ssH = (vector signed short)vec_mergeh((vector unsigned char)vzero,
+                                                (vector unsigned char)vsrc2uc);
+        vsrc3ssH = (vector signed short)vec_mergeh((vector unsigned char)vzero,
+                                                (vector unsigned char)vsrc3uc);
+
+        psum = vec_mladd(vA, vsrc0ssH, vec_splat_s16(0));
+        psum = vec_mladd(vB, vsrc1ssH, psum);
+        psum = vec_mladd(vC, vsrc2ssH, psum);
+        psum = vec_mladd(vD, vsrc3ssH, psum);
+        psum = vec_add(v28ss, psum);
+        psum = vec_sra(psum, v6us);
+
+        vdst = vec_ld(0, dst);
+        ppsum = (vector unsigned char)vec_packsu(psum, psum);
+        fsum = vec_perm(vdst, ppsum, fperm);
+
+        vec_st(fsum, 0, dst);
+
+        vsrc0ssH = vsrc2ssH;
+        vsrc1ssH = vsrc3ssH;
+
+        dst += stride;
+        src += stride;
+      }
+    } else {
+        vector unsigned char vsrcDuc;
+      for (i = 0 ; i < h ; i++) {
+        vsrcCuc = vec_ld(stride + 0, src);
+        vsrcDuc = vec_ld(stride + 16, src);
+
+        vsrc2uc = vec_perm(vsrcCuc, vsrcDuc, vsrcperm0);
+        if (reallyBadAlign)
+          vsrc3uc = vsrcDuc;
+        else
+          vsrc3uc = vec_perm(vsrcCuc, vsrcDuc, vsrcperm1);
+
+        vsrc2ssH = (vector signed short)vec_mergeh((vector unsigned char)vzero,
+                                                (vector unsigned char)vsrc2uc);
+        vsrc3ssH = (vector signed short)vec_mergeh((vector unsigned char)vzero,
+                                                (vector unsigned char)vsrc3uc);
+
+        psum = vec_mladd(vA, vsrc0ssH, vec_splat_s16(0));
+        psum = vec_mladd(vB, vsrc1ssH, psum);
+        psum = vec_mladd(vC, vsrc2ssH, psum);
+        psum = vec_mladd(vD, vsrc3ssH, psum);
+        psum = vec_add(v28ss, psum);
+        psum = vec_sr(psum, v6us);
+
+        vdst = vec_ld(0, dst);
+        ppsum = (vector unsigned char)vec_pack(psum, psum);
+        fsum = vec_perm(vdst, ppsum, fperm);
+
+        vec_st(fsum, 0, dst);
+
+        vsrc0ssH = vsrc2ssH;
+        vsrc1ssH = vsrc3ssH;
+
+        dst += stride;
+        src += stride;
+      }
+    }
+}
+
+static inline void put_pixels16_l2_altivec( uint8_t * dst, const uint8_t * src1,
+                                    const uint8_t * src2, int dst_stride,
+                                    int src_stride1, int h)
+{
+    int i;
+    vector unsigned char a, b, d, tmp1, tmp2, mask, mask_, edges, align;
+
+    mask_ = vec_lvsl(0, src2);
+
+    for (i = 0; i < h; i++) {
+
+        tmp1 = vec_ld(i * src_stride1, src1);
+        mask = vec_lvsl(i * src_stride1, src1);
+        tmp2 = vec_ld(i * src_stride1 + 15, src1);
+
+        a = vec_perm(tmp1, tmp2, mask);
+
+        tmp1 = vec_ld(i * 16, src2);
+        tmp2 = vec_ld(i * 16 + 15, src2);
+
+        b = vec_perm(tmp1, tmp2, mask_);
+
+        tmp1 = vec_ld(0, dst);
+        mask = vec_lvsl(0, dst);
+        tmp2 = vec_ld(15, dst);
+
+        d = vec_avg(a, b);
+
+        edges = vec_perm(tmp2, tmp1, mask);
+
+        align = vec_lvsr(0, dst);
+
+        tmp2 = vec_perm(d, edges, align);
+        tmp1 = vec_perm(edges, d, align);
+
+        vec_st(tmp2, 15, dst);
+        vec_st(tmp1, 0 , dst);
+
+        dst += dst_stride;
+    }
+}
+
+static inline void avg_pixels16_l2_altivec( uint8_t * dst, const uint8_t * src1,
+                                    const uint8_t * src2, int dst_stride,
+                                    int src_stride1, int h)
+{
+    int i;
+    vector unsigned char a, b, d, tmp1, tmp2, mask, mask_, edges, align;
+
+    mask_ = vec_lvsl(0, src2);
+
+    for (i = 0; i < h; i++) {
+
+        tmp1 = vec_ld(i * src_stride1, src1);
+        mask = vec_lvsl(i * src_stride1, src1);
+        tmp2 = vec_ld(i * src_stride1 + 15, src1);
+
+        a = vec_perm(tmp1, tmp2, mask);
+
+        tmp1 = vec_ld(i * 16, src2);
+        tmp2 = vec_ld(i * 16 + 15, src2);
+
+        b = vec_perm(tmp1, tmp2, mask_);
+
+        tmp1 = vec_ld(0, dst);
+        mask = vec_lvsl(0, dst);
+        tmp2 = vec_ld(15, dst);
+
+        d = vec_avg(vec_perm(tmp1, tmp2, mask), vec_avg(a, b));
+
+        edges = vec_perm(tmp2, tmp1, mask);
+
+        align = vec_lvsr(0, dst);
+
+        tmp2 = vec_perm(d, edges, align);
+        tmp1 = vec_perm(edges, d, align);
+
+        vec_st(tmp2, 15, dst);
+        vec_st(tmp1, 0 , dst);
+
+        dst += dst_stride;
+    }
+}
+
+/* Implemented but could be faster
+#define put_pixels16_l2_altivec(d,s1,s2,ds,s1s,h) put_pixels16_l2(d,s1,s2,ds,s1s,16,h)
+#define avg_pixels16_l2_altivec(d,s1,s2,ds,s1s,h) avg_pixels16_l2(d,s1,s2,ds,s1s,16,h)
+ */
+
+  H264_MC(put_, 16, altivec)
+  H264_MC(avg_, 16, altivec)
+
+
+/****************************************************************************
+ * IDCT transform:
+ ****************************************************************************/
+
+#define VEC_1D_DCT(vb0,vb1,vb2,vb3,va0,va1,va2,va3)              \
+   /* 1st stage */                                               \
+   vz0 = vec_add(vb0,vb2);       /* temp[0] = Y[0] + Y[2] */     \
+   vz1 = vec_sub(vb0,vb2);       /* temp[1] = Y[0] - Y[2] */     \
+   vz2 = vec_sra(vb1,vec_splat_u16(1));                          \
+   vz2 = vec_sub(vz2,vb3);       /* temp[2] = Y[1].1/2 - Y[3] */ \
+   vz3 = vec_sra(vb3,vec_splat_u16(1));                          \
+   vz3 = vec_add(vb1,vz3);       /* temp[3] = Y[1] + Y[3].1/2 */ \
+   /* 2nd stage: output */                                       \
+   va0 = vec_add(vz0,vz3);       /* x[0] = temp[0] + temp[3] */  \
+   va1 = vec_add(vz1,vz2);       /* x[1] = temp[1] + temp[2] */  \
+   va2 = vec_sub(vz1,vz2);       /* x[2] = temp[1] - temp[2] */  \
+   va3 = vec_sub(vz0,vz3)        /* x[3] = temp[0] - temp[3] */
+
+#define VEC_TRANSPOSE_4(a0,a1,a2,a3,b0,b1,b2,b3) \
+    b0 = vec_mergeh( a0, a0 ); \
+    b1 = vec_mergeh( a1, a0 ); \
+    b2 = vec_mergeh( a2, a0 ); \
+    b3 = vec_mergeh( a3, a0 ); \
+    a0 = vec_mergeh( b0, b2 ); \
+    a1 = vec_mergel( b0, b2 ); \
+    a2 = vec_mergeh( b1, b3 ); \
+    a3 = vec_mergel( b1, b3 ); \
+    b0 = vec_mergeh( a0, a2 ); \
+    b1 = vec_mergel( a0, a2 ); \
+    b2 = vec_mergeh( a1, a3 ); \
+    b3 = vec_mergel( a1, a3 )
+
+#define VEC_LOAD_U8_ADD_S16_STORE_U8(va)                      \
+    vdst_orig = vec_ld(0, dst);                               \
+    vdst = vec_perm(vdst_orig, zero_u8v, vdst_mask);          \
+    vdst_ss = (vec_s16_t) vec_mergeh(zero_u8v, vdst);         \
+    va = vec_add(va, vdst_ss);                                \
+    va_u8 = vec_packsu(va, zero_s16v);                        \
+    va_u32 = vec_splat((vec_u32_t)va_u8, 0);                  \
+    vec_ste(va_u32, element, (uint32_t*)dst);
+
+static void ff_h264_idct_add_altivec(uint8_t *dst, DCTELEM *block, int stride)
+{
+    vec_s16_t va0, va1, va2, va3;
+    vec_s16_t vz0, vz1, vz2, vz3;
+    vec_s16_t vtmp0, vtmp1, vtmp2, vtmp3;
+    vec_u8_t va_u8;
+    vec_u32_t va_u32;
+    vec_s16_t vdst_ss;
+    const vec_u16_t v6us = vec_splat_u16(6);
+    vec_u8_t vdst, vdst_orig;
+    vec_u8_t vdst_mask = vec_lvsl(0, dst);
+    int element = ((unsigned long)dst & 0xf) >> 2;
+    LOAD_ZERO;
+
+    block[0] += 32;  /* add 32 as a DC-level for rounding */
+
+    vtmp0 = vec_ld(0,block);
+    vtmp1 = vec_sld(vtmp0, vtmp0, 8);
+    vtmp2 = vec_ld(16,block);
+    vtmp3 = vec_sld(vtmp2, vtmp2, 8);
+
+    VEC_1D_DCT(vtmp0,vtmp1,vtmp2,vtmp3,va0,va1,va2,va3);
+    VEC_TRANSPOSE_4(va0,va1,va2,va3,vtmp0,vtmp1,vtmp2,vtmp3);
+    VEC_1D_DCT(vtmp0,vtmp1,vtmp2,vtmp3,va0,va1,va2,va3);
+
+    va0 = vec_sra(va0,v6us);
+    va1 = vec_sra(va1,v6us);
+    va2 = vec_sra(va2,v6us);
+    va3 = vec_sra(va3,v6us);
+
+    VEC_LOAD_U8_ADD_S16_STORE_U8(va0);
+    dst += stride;
+    VEC_LOAD_U8_ADD_S16_STORE_U8(va1);
+    dst += stride;
+    VEC_LOAD_U8_ADD_S16_STORE_U8(va2);
+    dst += stride;
+    VEC_LOAD_U8_ADD_S16_STORE_U8(va3);
+}
+
+#define IDCT8_1D_ALTIVEC(s0, s1, s2, s3, s4, s5, s6, s7,  d0, d1, d2, d3, d4, d5, d6, d7) {\
+    /*        a0  = SRC(0) + SRC(4); */ \
+    vec_s16_t a0v = vec_add(s0, s4);    \
+    /*        a2  = SRC(0) - SRC(4); */ \
+    vec_s16_t a2v = vec_sub(s0, s4);    \
+    /*        a4  =           (SRC(2)>>1) - SRC(6); */ \
+    vec_s16_t a4v = vec_sub(vec_sra(s2, onev), s6);    \
+    /*        a6  =           (SRC(6)>>1) + SRC(2); */ \
+    vec_s16_t a6v = vec_add(vec_sra(s6, onev), s2);    \
+    /*        b0  =         a0 + a6; */ \
+    vec_s16_t b0v = vec_add(a0v, a6v);  \
+    /*        b2  =         a2 + a4; */ \
+    vec_s16_t b2v = vec_add(a2v, a4v);  \
+    /*        b4  =         a2 - a4; */ \
+    vec_s16_t b4v = vec_sub(a2v, a4v);  \
+    /*        b6  =         a0 - a6; */ \
+    vec_s16_t b6v = vec_sub(a0v, a6v);  \
+    /* a1 =  SRC(5) - SRC(3) - SRC(7) - (SRC(7)>>1); */ \
+    /*        a1 =             (SRC(5)-SRC(3)) -  (SRC(7)  +  (SRC(7)>>1)); */ \
+    vec_s16_t a1v = vec_sub( vec_sub(s5, s3), vec_add(s7, vec_sra(s7, onev)) ); \
+    /* a3 =  SRC(7) + SRC(1) - SRC(3) - (SRC(3)>>1); */ \
+    /*        a3 =             (SRC(7)+SRC(1)) -  (SRC(3)  +  (SRC(3)>>1)); */ \
+    vec_s16_t a3v = vec_sub( vec_add(s7, s1), vec_add(s3, vec_sra(s3, onev)) );\
+    /* a5 =  SRC(7) - SRC(1) + SRC(5) + (SRC(5)>>1); */ \
+    /*        a5 =             (SRC(7)-SRC(1)) +   SRC(5) +   (SRC(5)>>1); */ \
+    vec_s16_t a5v = vec_add( vec_sub(s7, s1), vec_add(s5, vec_sra(s5, onev)) );\
+    /*        a7 =                SRC(5)+SRC(3) +  SRC(1) +   (SRC(1)>>1); */ \
+    vec_s16_t a7v = vec_add( vec_add(s5, s3), vec_add(s1, vec_sra(s1, onev)) );\
+    /*        b1 =                  (a7>>2)  +  a1; */ \
+    vec_s16_t b1v = vec_add( vec_sra(a7v, twov), a1v); \
+    /*        b3 =          a3 +        (a5>>2); */ \
+    vec_s16_t b3v = vec_add(a3v, vec_sra(a5v, twov)); \
+    /*        b5 =                  (a3>>2)  -   a5; */ \
+    vec_s16_t b5v = vec_sub( vec_sra(a3v, twov), a5v); \
+    /*        b7 =           a7 -        (a1>>2); */ \
+    vec_s16_t b7v = vec_sub( a7v, vec_sra(a1v, twov)); \
+    /* DST(0,    b0 + b7); */ \
+    d0 = vec_add(b0v, b7v); \
+    /* DST(1,    b2 + b5); */ \
+    d1 = vec_add(b2v, b5v); \
+    /* DST(2,    b4 + b3); */ \
+    d2 = vec_add(b4v, b3v); \
+    /* DST(3,    b6 + b1); */ \
+    d3 = vec_add(b6v, b1v); \
+    /* DST(4,    b6 - b1); */ \
+    d4 = vec_sub(b6v, b1v); \
+    /* DST(5,    b4 - b3); */ \
+    d5 = vec_sub(b4v, b3v); \
+    /* DST(6,    b2 - b5); */ \
+    d6 = vec_sub(b2v, b5v); \
+    /* DST(7,    b0 - b7); */ \
+    d7 = vec_sub(b0v, b7v); \
+}
+
+#define ALTIVEC_STORE_SUM_CLIP(dest, idctv, perm_ldv, perm_stv, sel) { \
+    /* unaligned load */                                       \
+    vec_u8_t hv = vec_ld( 0, dest );                           \
+    vec_u8_t lv = vec_ld( 7, dest );                           \
+    vec_u8_t dstv   = vec_perm( hv, lv, (vec_u8_t)perm_ldv );  \
+    vec_s16_t idct_sh6 = vec_sra(idctv, sixv);                 \
+    vec_u16_t dst16 = (vec_u16_t)vec_mergeh(zero_u8v, dstv);   \
+    vec_s16_t idstsum = vec_adds(idct_sh6, (vec_s16_t)dst16);  \
+    vec_u8_t idstsum8 = vec_packsu(zero_s16v, idstsum);        \
+    vec_u8_t edgehv;                                           \
+    /* unaligned store */                                      \
+    vec_u8_t bodyv  = vec_perm( idstsum8, idstsum8, perm_stv );\
+    vec_u8_t edgelv = vec_perm( sel, zero_u8v, perm_stv );     \
+    lv    = vec_sel( lv, bodyv, edgelv );                      \
+    vec_st( lv, 7, dest );                                     \
+    hv    = vec_ld( 0, dest );                                 \
+    edgehv = vec_perm( zero_u8v, sel, perm_stv );              \
+    hv    = vec_sel( hv, bodyv, edgehv );                      \
+    vec_st( hv, 0, dest );                                     \
+ }
+
+void ff_h264_idct8_add_altivec( uint8_t *dst, DCTELEM *dct, int stride ) {
+    vec_s16_t s0, s1, s2, s3, s4, s5, s6, s7;
+    vec_s16_t d0, d1, d2, d3, d4, d5, d6, d7;
+    vec_s16_t idct0, idct1, idct2, idct3, idct4, idct5, idct6, idct7;
+
+    vec_u8_t perm_ldv = vec_lvsl(0, dst);
+    vec_u8_t perm_stv = vec_lvsr(8, dst);
+
+    const vec_u16_t onev = vec_splat_u16(1);
+    const vec_u16_t twov = vec_splat_u16(2);
+    const vec_u16_t sixv = vec_splat_u16(6);
+
+    const vec_u8_t sel = (vec_u8_t) AVV(0,0,0,0,0,0,0,0,
+                                        -1,-1,-1,-1,-1,-1,-1,-1);
+    LOAD_ZERO;
+
+    dct[0] += 32; // rounding for the >>6 at the end
+
+    s0 = vec_ld(0x00, (int16_t*)dct);
+    s1 = vec_ld(0x10, (int16_t*)dct);
+    s2 = vec_ld(0x20, (int16_t*)dct);
+    s3 = vec_ld(0x30, (int16_t*)dct);
+    s4 = vec_ld(0x40, (int16_t*)dct);
+    s5 = vec_ld(0x50, (int16_t*)dct);
+    s6 = vec_ld(0x60, (int16_t*)dct);
+    s7 = vec_ld(0x70, (int16_t*)dct);
+
+    IDCT8_1D_ALTIVEC(s0, s1, s2, s3, s4, s5, s6, s7,
+                     d0, d1, d2, d3, d4, d5, d6, d7);
+
+    TRANSPOSE8( d0,  d1,  d2,  d3,  d4,  d5,  d6, d7 );
+
+    IDCT8_1D_ALTIVEC(d0,  d1,  d2,  d3,  d4,  d5,  d6, d7,
+                     idct0, idct1, idct2, idct3, idct4, idct5, idct6, idct7);
+
+    ALTIVEC_STORE_SUM_CLIP(&dst[0*stride], idct0, perm_ldv, perm_stv, sel);
+    ALTIVEC_STORE_SUM_CLIP(&dst[1*stride], idct1, perm_ldv, perm_stv, sel);
+    ALTIVEC_STORE_SUM_CLIP(&dst[2*stride], idct2, perm_ldv, perm_stv, sel);
+    ALTIVEC_STORE_SUM_CLIP(&dst[3*stride], idct3, perm_ldv, perm_stv, sel);
+    ALTIVEC_STORE_SUM_CLIP(&dst[4*stride], idct4, perm_ldv, perm_stv, sel);
+    ALTIVEC_STORE_SUM_CLIP(&dst[5*stride], idct5, perm_ldv, perm_stv, sel);
+    ALTIVEC_STORE_SUM_CLIP(&dst[6*stride], idct6, perm_ldv, perm_stv, sel);
+    ALTIVEC_STORE_SUM_CLIP(&dst[7*stride], idct7, perm_ldv, perm_stv, sel);
+}
+
+#define transpose4x16(r0, r1, r2, r3) {      \
+    register vector unsigned char r4;        \
+    register vector unsigned char r5;        \
+    register vector unsigned char r6;        \
+    register vector unsigned char r7;        \
+                                             \
+    r4 = vec_mergeh(r0, r2);  /*0, 2 set 0*/ \
+    r5 = vec_mergel(r0, r2);  /*0, 2 set 1*/ \
+    r6 = vec_mergeh(r1, r3);  /*1, 3 set 0*/ \
+    r7 = vec_mergel(r1, r3);  /*1, 3 set 1*/ \
+                                             \
+    r0 = vec_mergeh(r4, r6);  /*all set 0*/  \
+    r1 = vec_mergel(r4, r6);  /*all set 1*/  \
+    r2 = vec_mergeh(r5, r7);  /*all set 2*/  \
+    r3 = vec_mergel(r5, r7);  /*all set 3*/  \
+}
+
+static inline void write16x4(uint8_t *dst, int dst_stride,
+                             register vector unsigned char r0, register vector unsigned char r1,
+                             register vector unsigned char r2, register vector unsigned char r3) {
+    DECLARE_ALIGNED_16(unsigned char, result[64]);
+    uint32_t *src_int = (uint32_t *)result, *dst_int = (uint32_t *)dst;
+    int int_dst_stride = dst_stride/4;
+
+    vec_st(r0, 0, result);
+    vec_st(r1, 16, result);
+    vec_st(r2, 32, result);
+    vec_st(r3, 48, result);
+    /* FIXME: there has to be a better way!!!! */
+    *dst_int = *src_int;
+    *(dst_int+   int_dst_stride) = *(src_int + 1);
+    *(dst_int+ 2*int_dst_stride) = *(src_int + 2);
+    *(dst_int+ 3*int_dst_stride) = *(src_int + 3);
+    *(dst_int+ 4*int_dst_stride) = *(src_int + 4);
+    *(dst_int+ 5*int_dst_stride) = *(src_int + 5);
+    *(dst_int+ 6*int_dst_stride) = *(src_int + 6);
+    *(dst_int+ 7*int_dst_stride) = *(src_int + 7);
+    *(dst_int+ 8*int_dst_stride) = *(src_int + 8);
+    *(dst_int+ 9*int_dst_stride) = *(src_int + 9);
+    *(dst_int+10*int_dst_stride) = *(src_int + 10);
+    *(dst_int+11*int_dst_stride) = *(src_int + 11);
+    *(dst_int+12*int_dst_stride) = *(src_int + 12);
+    *(dst_int+13*int_dst_stride) = *(src_int + 13);
+    *(dst_int+14*int_dst_stride) = *(src_int + 14);
+    *(dst_int+15*int_dst_stride) = *(src_int + 15);
+}
+
+/** \brief performs a 6x16 transpose of data in src, and stores it to dst
+    \todo FIXME: see if we can't spare some vec_lvsl() by them factorizing
+    out of unaligned_load() */
+#define readAndTranspose16x6(src, src_stride, r8, r9, r10, r11, r12, r13) {\
+    register vector unsigned char r0  = unaligned_load(0,             src);\
+    register vector unsigned char r1  = unaligned_load(   src_stride, src);\
+    register vector unsigned char r2  = unaligned_load(2* src_stride, src);\
+    register vector unsigned char r3  = unaligned_load(3* src_stride, src);\
+    register vector unsigned char r4  = unaligned_load(4* src_stride, src);\
+    register vector unsigned char r5  = unaligned_load(5* src_stride, src);\
+    register vector unsigned char r6  = unaligned_load(6* src_stride, src);\
+    register vector unsigned char r7  = unaligned_load(7* src_stride, src);\
+    register vector unsigned char r14 = unaligned_load(14*src_stride, src);\
+    register vector unsigned char r15 = unaligned_load(15*src_stride, src);\
+                                                                           \
+    r8  = unaligned_load( 8*src_stride, src);                              \
+    r9  = unaligned_load( 9*src_stride, src);                              \
+    r10 = unaligned_load(10*src_stride, src);                              \
+    r11 = unaligned_load(11*src_stride, src);                              \
+    r12 = unaligned_load(12*src_stride, src);                              \
+    r13 = unaligned_load(13*src_stride, src);                              \
+                                                                           \
+    /*Merge first pairs*/                                                  \
+    r0 = vec_mergeh(r0, r8);    /*0, 8*/                                   \
+    r1 = vec_mergeh(r1, r9);    /*1, 9*/                                   \
+    r2 = vec_mergeh(r2, r10);   /*2,10*/                                   \
+    r3 = vec_mergeh(r3, r11);   /*3,11*/                                   \
+    r4 = vec_mergeh(r4, r12);   /*4,12*/                                   \
+    r5 = vec_mergeh(r5, r13);   /*5,13*/                                   \
+    r6 = vec_mergeh(r6, r14);   /*6,14*/                                   \
+    r7 = vec_mergeh(r7, r15);   /*7,15*/                                   \
+                                                                           \
+    /*Merge second pairs*/                                                 \
+    r8  = vec_mergeh(r0, r4);   /*0,4, 8,12 set 0*/                        \
+    r9  = vec_mergel(r0, r4);   /*0,4, 8,12 set 1*/                        \
+    r10 = vec_mergeh(r1, r5);   /*1,5, 9,13 set 0*/                        \
+    r11 = vec_mergel(r1, r5);   /*1,5, 9,13 set 1*/                        \
+    r12 = vec_mergeh(r2, r6);   /*2,6,10,14 set 0*/                        \
+    r13 = vec_mergel(r2, r6);   /*2,6,10,14 set 1*/                        \
+    r14 = vec_mergeh(r3, r7);   /*3,7,11,15 set 0*/                        \
+    r15 = vec_mergel(r3, r7);   /*3,7,11,15 set 1*/                        \
+                                                                           \
+    /*Third merge*/                                                        \
+    r0 = vec_mergeh(r8, r12);   /*0,2,4,6,8,10,12,14 set 0*/               \
+    r1 = vec_mergel(r8, r12);   /*0,2,4,6,8,10,12,14 set 1*/               \
+    r2 = vec_mergeh(r9, r13);   /*0,2,4,6,8,10,12,14 set 2*/               \
+    r4 = vec_mergeh(r10, r14);  /*1,3,5,7,9,11,13,15 set 0*/               \
+    r5 = vec_mergel(r10, r14);  /*1,3,5,7,9,11,13,15 set 1*/               \
+    r6 = vec_mergeh(r11, r15);  /*1,3,5,7,9,11,13,15 set 2*/               \
+    /* Don't need to compute 3 and 7*/                                     \
+                                                                           \
+    /*Final merge*/                                                        \
+    r8  = vec_mergeh(r0, r4);   /*all set 0*/                              \
+    r9  = vec_mergel(r0, r4);   /*all set 1*/                              \
+    r10 = vec_mergeh(r1, r5);   /*all set 2*/                              \
+    r11 = vec_mergel(r1, r5);   /*all set 3*/                              \
+    r12 = vec_mergeh(r2, r6);   /*all set 4*/                              \
+    r13 = vec_mergel(r2, r6);   /*all set 5*/                              \
+    /* Don't need to compute 14 and 15*/                                   \
+                                                                           \
+}
+
+// out: o = |x-y| < a
+static inline vector unsigned char diff_lt_altivec ( register vector unsigned char x,
+                                                     register vector unsigned char y,
+                                                     register vector unsigned char a) {
+
+    register vector unsigned char diff = vec_subs(x, y);
+    register vector unsigned char diffneg = vec_subs(y, x);
+    register vector unsigned char o = vec_or(diff, diffneg); /* |x-y| */
+    o = (vector unsigned char)vec_cmplt(o, a);
+    return o;
+}
+
+static inline vector unsigned char h264_deblock_mask ( register vector unsigned char p0,
+                                                       register vector unsigned char p1,
+                                                       register vector unsigned char q0,
+                                                       register vector unsigned char q1,
+                                                       register vector unsigned char alpha,
+                                                       register vector unsigned char beta) {
+
+    register vector unsigned char mask;
+    register vector unsigned char tempmask;
+
+    mask = diff_lt_altivec(p0, q0, alpha);
+    tempmask = diff_lt_altivec(p1, p0, beta);
+    mask = vec_and(mask, tempmask);
+    tempmask = diff_lt_altivec(q1, q0, beta);
+    mask = vec_and(mask, tempmask);
+
+    return mask;
+}
+
+// out: newp1 = clip((p2 + ((p0 + q0 + 1) >> 1)) >> 1, p1-tc0, p1+tc0)
+static inline vector unsigned char h264_deblock_q1(register vector unsigned char p0,
+                                                   register vector unsigned char p1,
+                                                   register vector unsigned char p2,
+                                                   register vector unsigned char q0,
+                                                   register vector unsigned char tc0) {
+
+    register vector unsigned char average = vec_avg(p0, q0);
+    register vector unsigned char temp;
+    register vector unsigned char uncliped;
+    register vector unsigned char ones;
+    register vector unsigned char max;
+    register vector unsigned char min;
+    register vector unsigned char newp1;
+
+    temp = vec_xor(average, p2);
+    average = vec_avg(average, p2);     /*avg(p2, avg(p0, q0)) */
+    ones = vec_splat_u8(1);
+    temp = vec_and(temp, ones);         /*(p2^avg(p0, q0)) & 1 */
+    uncliped = vec_subs(average, temp); /*(p2+((p0+q0+1)>>1))>>1 */
+    max = vec_adds(p1, tc0);
+    min = vec_subs(p1, tc0);
+    newp1 = vec_max(min, uncliped);
+    newp1 = vec_min(max, newp1);
+    return newp1;
+}
+
+#define h264_deblock_p0_q0(p0, p1, q0, q1, tc0masked) {                                           \
+                                                                                                  \
+    const vector unsigned char A0v = vec_sl(vec_splat_u8(10), vec_splat_u8(4));                   \
+                                                                                                  \
+    register vector unsigned char pq0bit = vec_xor(p0,q0);                                        \
+    register vector unsigned char q1minus;                                                        \
+    register vector unsigned char p0minus;                                                        \
+    register vector unsigned char stage1;                                                         \
+    register vector unsigned char stage2;                                                         \
+    register vector unsigned char vec160;                                                         \
+    register vector unsigned char delta;                                                          \
+    register vector unsigned char deltaneg;                                                       \
+                                                                                                  \
+    q1minus = vec_nor(q1, q1);                 /* 255 - q1 */                                     \
+    stage1 = vec_avg(p1, q1minus);             /* (p1 - q1 + 256)>>1 */                           \
+    stage2 = vec_sr(stage1, vec_splat_u8(1));  /* (p1 - q1 + 256)>>2 = 64 + (p1 - q1) >> 2 */     \
+    p0minus = vec_nor(p0, p0);                 /* 255 - p0 */                                     \
+    stage1 = vec_avg(q0, p0minus);             /* (q0 - p0 + 256)>>1 */                           \
+    pq0bit = vec_and(pq0bit, vec_splat_u8(1));                                                    \
+    stage2 = vec_avg(stage2, pq0bit);          /* 32 + ((q0 - p0)&1 + (p1 - q1) >> 2 + 1) >> 1 */ \
+    stage2 = vec_adds(stage2, stage1);         /* 160 + ((p0 - q0) + (p1 - q1) >> 2 + 1) >> 1 */  \
+    vec160 = vec_ld(0, &A0v);                                                                     \
+    deltaneg = vec_subs(vec160, stage2);       /* -d */                                           \
+    delta = vec_subs(stage2, vec160);          /* d */                                            \
+    deltaneg = vec_min(tc0masked, deltaneg);                                                      \
+    delta = vec_min(tc0masked, delta);                                                            \
+    p0 = vec_subs(p0, deltaneg);                                                                  \
+    q0 = vec_subs(q0, delta);                                                                     \
+    p0 = vec_adds(p0, delta);                                                                     \
+    q0 = vec_adds(q0, deltaneg);                                                                  \
+}
+
+#define h264_loop_filter_luma_altivec(p2, p1, p0, q0, q1, q2, alpha, beta, tc0) {            \
+    DECLARE_ALIGNED_16(unsigned char, temp[16]);                                             \
+    register vector unsigned char alphavec;                                                  \
+    register vector unsigned char betavec;                                                   \
+    register vector unsigned char mask;                                                      \
+    register vector unsigned char p1mask;                                                    \
+    register vector unsigned char q1mask;                                                    \
+    register vector signed   char tc0vec;                                                    \
+    register vector unsigned char finaltc0;                                                  \
+    register vector unsigned char tc0masked;                                                 \
+    register vector unsigned char newp1;                                                     \
+    register vector unsigned char newq1;                                                     \
+                                                                                             \
+    temp[0] = alpha;                                                                         \
+    temp[1] = beta;                                                                          \
+    alphavec = vec_ld(0, temp);                                                              \
+    betavec = vec_splat(alphavec, 0x1);                                                      \
+    alphavec = vec_splat(alphavec, 0x0);                                                     \
+    mask = h264_deblock_mask(p0, p1, q0, q1, alphavec, betavec); /*if in block */            \
+                                                                                             \
+    *((int *)temp) = *((int *)tc0);                                                          \
+    tc0vec = vec_ld(0, (signed char*)temp);                                                  \
+    tc0vec = vec_mergeh(tc0vec, tc0vec);                                                     \
+    tc0vec = vec_mergeh(tc0vec, tc0vec);                                                     \
+    mask = vec_and(mask, vec_cmpgt(tc0vec, vec_splat_s8(-1)));  /* if tc0[i] >= 0 */         \
+    finaltc0 = vec_and((vector unsigned char)tc0vec, mask);     /* tc = tc0 */               \
+                                                                                             \
+    p1mask = diff_lt_altivec(p2, p0, betavec);                                               \
+    p1mask = vec_and(p1mask, mask);                             /* if( |p2 - p0| < beta) */  \
+    tc0masked = vec_and(p1mask, (vector unsigned char)tc0vec);                               \
+    finaltc0 = vec_sub(finaltc0, p1mask);                       /* tc++ */                   \
+    newp1 = h264_deblock_q1(p0, p1, p2, q0, tc0masked);                                      \
+    /*end if*/                                                                               \
+                                                                                             \
+    q1mask = diff_lt_altivec(q2, q0, betavec);                                               \
+    q1mask = vec_and(q1mask, mask);                             /* if ( |q2 - q0| < beta ) */\
+    tc0masked = vec_and(q1mask, (vector unsigned char)tc0vec);                               \
+    finaltc0 = vec_sub(finaltc0, q1mask);                       /* tc++ */                   \
+    newq1 = h264_deblock_q1(p0, q1, q2, q0, tc0masked);                                      \
+    /*end if*/                                                                               \
+                                                                                             \
+    h264_deblock_p0_q0(p0, p1, q0, q1, finaltc0);                                            \
+    p1 = newp1;                                                                              \
+    q1 = newq1;                                                                              \
+}
+
+static void h264_v_loop_filter_luma_altivec(uint8_t *pix, int stride, int alpha, int beta, int8_t *tc0) {
+
+    if((tc0[0] & tc0[1] & tc0[2] & tc0[3]) >= 0) {
+        register vector unsigned char p2 = vec_ld(-3*stride, pix);
+        register vector unsigned char p1 = vec_ld(-2*stride, pix);
+        register vector unsigned char p0 = vec_ld(-1*stride, pix);
+        register vector unsigned char q0 = vec_ld(0, pix);
+        register vector unsigned char q1 = vec_ld(stride, pix);
+        register vector unsigned char q2 = vec_ld(2*stride, pix);
+        h264_loop_filter_luma_altivec(p2, p1, p0, q0, q1, q2, alpha, beta, tc0);
+        vec_st(p1, -2*stride, pix);
+        vec_st(p0, -1*stride, pix);
+        vec_st(q0, 0, pix);
+        vec_st(q1, stride, pix);
+    }
+}
+
+static void h264_h_loop_filter_luma_altivec(uint8_t *pix, int stride, int alpha, int beta, int8_t *tc0) {
+
+    register vector unsigned char line0, line1, line2, line3, line4, line5;
+    if((tc0[0] & tc0[1] & tc0[2] & tc0[3]) < 0)
+        return;
+    readAndTranspose16x6(pix-3, stride, line0, line1, line2, line3, line4, line5);
+    h264_loop_filter_luma_altivec(line0, line1, line2, line3, line4, line5, alpha, beta, tc0);
+    transpose4x16(line1, line2, line3, line4);
+    write16x4(pix-2, stride, line1, line2, line3, line4);
+}
+
+void dsputil_h264_init_ppc(DSPContext* c, AVCodecContext *avctx) {
+
+#ifdef HAVE_ALTIVEC
+  if (has_altivec()) {
+    c->put_h264_chroma_pixels_tab[0] = put_h264_chroma_mc8_altivec;
+    c->put_no_rnd_h264_chroma_pixels_tab[0] = put_no_rnd_h264_chroma_mc8_altivec;
+    c->avg_h264_chroma_pixels_tab[0] = avg_h264_chroma_mc8_altivec;
+    c->h264_idct_add = ff_h264_idct_add_altivec;
+    c->h264_idct8_add = ff_h264_idct8_add_altivec;
+    c->h264_v_loop_filter_luma= h264_v_loop_filter_luma_altivec;
+    c->h264_h_loop_filter_luma= h264_h_loop_filter_luma_altivec;
+
+#define dspfunc(PFX, IDX, NUM) \
+    c->PFX ## _pixels_tab[IDX][ 0] = PFX ## NUM ## _mc00_altivec; \
+    c->PFX ## _pixels_tab[IDX][ 1] = PFX ## NUM ## _mc10_altivec; \
+    c->PFX ## _pixels_tab[IDX][ 2] = PFX ## NUM ## _mc20_altivec; \
+    c->PFX ## _pixels_tab[IDX][ 3] = PFX ## NUM ## _mc30_altivec; \
+    c->PFX ## _pixels_tab[IDX][ 4] = PFX ## NUM ## _mc01_altivec; \
+    c->PFX ## _pixels_tab[IDX][ 5] = PFX ## NUM ## _mc11_altivec; \
+    c->PFX ## _pixels_tab[IDX][ 6] = PFX ## NUM ## _mc21_altivec; \
+    c->PFX ## _pixels_tab[IDX][ 7] = PFX ## NUM ## _mc31_altivec; \
+    c->PFX ## _pixels_tab[IDX][ 8] = PFX ## NUM ## _mc02_altivec; \
+    c->PFX ## _pixels_tab[IDX][ 9] = PFX ## NUM ## _mc12_altivec; \
+    c->PFX ## _pixels_tab[IDX][10] = PFX ## NUM ## _mc22_altivec; \
+    c->PFX ## _pixels_tab[IDX][11] = PFX ## NUM ## _mc32_altivec; \
+    c->PFX ## _pixels_tab[IDX][12] = PFX ## NUM ## _mc03_altivec; \
+    c->PFX ## _pixels_tab[IDX][13] = PFX ## NUM ## _mc13_altivec; \
+    c->PFX ## _pixels_tab[IDX][14] = PFX ## NUM ## _mc23_altivec; \
+    c->PFX ## _pixels_tab[IDX][15] = PFX ## NUM ## _mc33_altivec
+
+    dspfunc(put_h264_qpel, 0, 16);
+    dspfunc(avg_h264_qpel, 0, 16);
+#undef dspfunc
+
+  } else
+#endif /* HAVE_ALTIVEC */
+  {
+    // Non-AltiVec PPC optimisations
+
+    // ... pending ...
+  }
+}

Copied: branches/avidemux_2.4_branch/avidemux/ADM_libraries/ADM_lavcodec/ppc/h264_template_altivec.c (from rev 3719, branches/avidemux_2.4_branch/avidemux/ADM_libraries/ADM_lavcodec/ppc/dsputil_h264_template_altivec.c)
===================================================================
--- branches/avidemux_2.4_branch/avidemux/ADM_libraries/ADM_lavcodec/ppc/dsputil_h264_template_altivec.c	2007-12-03 12:38:06 UTC (rev 3719)
+++ branches/avidemux_2.4_branch/avidemux/ADM_libraries/ADM_lavcodec/ppc/h264_template_altivec.c	2007-12-04 00:19:48 UTC (rev 3720)
@@ -0,0 +1,719 @@
+/*
+ * Copyright (c) 2004 Romain Dolbeau <romain at dolbeau.org>
+ *
+ * This file is part of FFmpeg.
+ *
+ * FFmpeg is free software; you can redistribute it and/or
+ * modify it under the terms of the GNU Lesser General Public
+ * License as published by the Free Software Foundation; either
+ * version 2.1 of the License, or (at your option) any later version.
+ *
+ * FFmpeg is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+ * Lesser General Public License for more details.
+ *
+ * You should have received a copy of the GNU Lesser General Public
+ * License along with FFmpeg; if not, write to the Free Software
+ * Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA
+ */
+
+/* this code assume that stride % 16 == 0 */
+void PREFIX_h264_chroma_mc8_altivec(uint8_t * dst, uint8_t * src, int stride, int h, int x, int y) {
+  POWERPC_PERF_DECLARE(PREFIX_h264_chroma_mc8_num, 1);
+    DECLARE_ALIGNED_16(signed int, ABCD[4]) =
+                        {((8 - x) * (8 - y)),
+                          ((x) * (8 - y)),
+                          ((8 - x) * (y)),
+                          ((x) * (y))};
+    register int i;
+    vector unsigned char fperm;
+    const vector signed int vABCD = vec_ld(0, ABCD);
+    const vector signed short vA = vec_splat((vector signed short)vABCD, 1);
+    const vector signed short vB = vec_splat((vector signed short)vABCD, 3);
+    const vector signed short vC = vec_splat((vector signed short)vABCD, 5);
+    const vector signed short vD = vec_splat((vector signed short)vABCD, 7);
+    const vector signed int vzero = vec_splat_s32(0);
+    const vector signed short v32ss = vec_sl(vec_splat_s16(1),vec_splat_u16(5));
+    const vector unsigned short v6us = vec_splat_u16(6);
+    register int loadSecond = (((unsigned long)src) % 16) <= 7 ? 0 : 1;
+    register int reallyBadAlign = (((unsigned long)src) % 16) == 15 ? 1 : 0;
+
+    vector unsigned char vsrcAuc, vsrcBuc, vsrcperm0, vsrcperm1;
+    vector unsigned char vsrc0uc, vsrc1uc;
+    vector signed short vsrc0ssH, vsrc1ssH;
+    vector unsigned char vsrcCuc, vsrc2uc, vsrc3uc;
+    vector signed short vsrc2ssH, vsrc3ssH, psum;
+    vector unsigned char vdst, ppsum, vfdst, fsum;
+
+  POWERPC_PERF_START_COUNT(PREFIX_h264_chroma_mc8_num, 1);
+
+    if (((unsigned long)dst) % 16 == 0) {
+      fperm = (vector unsigned char)AVV(0x10, 0x11, 0x12, 0x13,
+                                        0x14, 0x15, 0x16, 0x17,
+                                        0x08, 0x09, 0x0A, 0x0B,
+                                        0x0C, 0x0D, 0x0E, 0x0F);
+    } else {
+      fperm = (vector unsigned char)AVV(0x00, 0x01, 0x02, 0x03,
+                                        0x04, 0x05, 0x06, 0x07,
+                                        0x18, 0x19, 0x1A, 0x1B,
+                                        0x1C, 0x1D, 0x1E, 0x1F);
+    }
+
+    vsrcAuc = vec_ld(0, src);
+
+    if (loadSecond)
+      vsrcBuc = vec_ld(16, src);
+    vsrcperm0 = vec_lvsl(0, src);
+    vsrcperm1 = vec_lvsl(1, src);
+
+    vsrc0uc = vec_perm(vsrcAuc, vsrcBuc, vsrcperm0);
+    if (reallyBadAlign)
+      vsrc1uc = vsrcBuc;
+    else
+      vsrc1uc = vec_perm(vsrcAuc, vsrcBuc, vsrcperm1);
+
+    vsrc0ssH = (vector signed short)vec_mergeh((vector unsigned char)vzero,
+                                               (vector unsigned char)vsrc0uc);
+    vsrc1ssH = (vector signed short)vec_mergeh((vector unsigned char)vzero,
+                                               (vector unsigned char)vsrc1uc);
+
+    if (!loadSecond) {// -> !reallyBadAlign
+      for (i = 0 ; i < h ; i++) {
+
+
+        vsrcCuc = vec_ld(stride + 0, src);
+
+        vsrc2uc = vec_perm(vsrcCuc, vsrcCuc, vsrcperm0);
+        vsrc3uc = vec_perm(vsrcCuc, vsrcCuc, vsrcperm1);
+
+        vsrc2ssH = (vector signed short)vec_mergeh((vector unsigned char)vzero,
+                                                (vector unsigned char)vsrc2uc);
+        vsrc3ssH = (vector signed short)vec_mergeh((vector unsigned char)vzero,
+                                                (vector unsigned char)vsrc3uc);
+
+        psum = vec_mladd(vA, vsrc0ssH, vec_splat_s16(0));
+        psum = vec_mladd(vB, vsrc1ssH, psum);
+        psum = vec_mladd(vC, vsrc2ssH, psum);
+        psum = vec_mladd(vD, vsrc3ssH, psum);
+        psum = vec_add(v32ss, psum);
+        psum = vec_sra(psum, v6us);
+
+        vdst = vec_ld(0, dst);
+        ppsum = (vector unsigned char)vec_packsu(psum, psum);
+        vfdst = vec_perm(vdst, ppsum, fperm);
+
+        OP_U8_ALTIVEC(fsum, vfdst, vdst);
+
+        vec_st(fsum, 0, dst);
+
+        vsrc0ssH = vsrc2ssH;
+        vsrc1ssH = vsrc3ssH;
+
+        dst += stride;
+        src += stride;
+      }
+    } else {
+        vector unsigned char vsrcDuc;
+      for (i = 0 ; i < h ; i++) {
+        vsrcCuc = vec_ld(stride + 0, src);
+        vsrcDuc = vec_ld(stride + 16, src);
+
+        vsrc2uc = vec_perm(vsrcCuc, vsrcDuc, vsrcperm0);
+        if (reallyBadAlign)
+          vsrc3uc = vsrcDuc;
+        else
+          vsrc3uc = vec_perm(vsrcCuc, vsrcDuc, vsrcperm1);
+
+        vsrc2ssH = (vector signed short)vec_mergeh((vector unsigned char)vzero,
+                                                (vector unsigned char)vsrc2uc);
+        vsrc3ssH = (vector signed short)vec_mergeh((vector unsigned char)vzero,
+                                                (vector unsigned char)vsrc3uc);
+
+        psum = vec_mladd(vA, vsrc0ssH, vec_splat_s16(0));
+        psum = vec_mladd(vB, vsrc1ssH, psum);
+        psum = vec_mladd(vC, vsrc2ssH, psum);
+        psum = vec_mladd(vD, vsrc3ssH, psum);
+        psum = vec_add(v32ss, psum);
+        psum = vec_sr(psum, v6us);
+
+        vdst = vec_ld(0, dst);
+        ppsum = (vector unsigned char)vec_pack(psum, psum);
+        vfdst = vec_perm(vdst, ppsum, fperm);
+
+        OP_U8_ALTIVEC(fsum, vfdst, vdst);
+
+        vec_st(fsum, 0, dst);
+
+        vsrc0ssH = vsrc2ssH;
+        vsrc1ssH = vsrc3ssH;
+
+        dst += stride;
+        src += stride;
+      }
+    }
+    POWERPC_PERF_STOP_COUNT(PREFIX_h264_chroma_mc8_num, 1);
+}
+
+/* this code assume stride % 16 == 0 */
+static void PREFIX_h264_qpel16_h_lowpass_altivec(uint8_t * dst, uint8_t * src, int dstStride, int srcStride) {
+  POWERPC_PERF_DECLARE(PREFIX_h264_qpel16_h_lowpass_num, 1);
+  register int i;
+
+  const vector signed int vzero = vec_splat_s32(0);
+  const vector unsigned char permM2 = vec_lvsl(-2, src);
+  const vector unsigned char permM1 = vec_lvsl(-1, src);
+  const vector unsigned char permP0 = vec_lvsl(+0, src);
+  const vector unsigned char permP1 = vec_lvsl(+1, src);
+  const vector unsigned char permP2 = vec_lvsl(+2, src);
+  const vector unsigned char permP3 = vec_lvsl(+3, src);
+  const vector signed short v5ss = vec_splat_s16(5);
+  const vector unsigned short v5us = vec_splat_u16(5);
+  const vector signed short v20ss = vec_sl(vec_splat_s16(5),vec_splat_u16(2));
+  const vector signed short v16ss = vec_sl(vec_splat_s16(1),vec_splat_u16(4));
+  const vector unsigned char dstperm = vec_lvsr(0, dst);
+  const vector unsigned char neg1 =
+                                (const vector unsigned char) vec_splat_s8(-1);
+
+  const vector unsigned char dstmask =
+                                vec_perm((const vector unsigned char)vzero,
+                                                               neg1, dstperm);
+
+  vector unsigned char srcM2, srcM1, srcP0, srcP1, srcP2, srcP3;
+
+  register int align = ((((unsigned long)src) - 2) % 16);
+
+  vector signed short srcP0A, srcP0B, srcP1A, srcP1B,
+                      srcP2A, srcP2B, srcP3A, srcP3B,
+                      srcM1A, srcM1B, srcM2A, srcM2B,
+                      sum1A, sum1B, sum2A, sum2B, sum3A, sum3B,
+                      pp1A, pp1B, pp2A, pp2B, pp3A, pp3B,
+                      psumA, psumB, sumA, sumB;
+
+  vector unsigned char sum, dst1, dst2, vdst, fsum,
+                       rsum, fdst1, fdst2;
+
+  POWERPC_PERF_START_COUNT(PREFIX_h264_qpel16_h_lowpass_num, 1);
+
+  for (i = 0 ; i < 16 ; i ++) {
+    vector unsigned char srcR1 = vec_ld(-2, src);
+    vector unsigned char srcR2 = vec_ld(14, src);
+
+    switch (align) {
+    default: {
+      srcM2 = vec_perm(srcR1, srcR2, permM2);
+      srcM1 = vec_perm(srcR1, srcR2, permM1);
+      srcP0 = vec_perm(srcR1, srcR2, permP0);
+      srcP1 = vec_perm(srcR1, srcR2, permP1);
+      srcP2 = vec_perm(srcR1, srcR2, permP2);
+      srcP3 = vec_perm(srcR1, srcR2, permP3);
+    } break;
+    case 11: {
+      srcM2 = vec_perm(srcR1, srcR2, permM2);
+      srcM1 = vec_perm(srcR1, srcR2, permM1);
+      srcP0 = vec_perm(srcR1, srcR2, permP0);
+      srcP1 = vec_perm(srcR1, srcR2, permP1);
+      srcP2 = vec_perm(srcR1, srcR2, permP2);
+      srcP3 = srcR2;
+    } break;
+    case 12: {
+      vector unsigned char srcR3 = vec_ld(30, src);
+      srcM2 = vec_perm(srcR1, srcR2, permM2);
+      srcM1 = vec_perm(srcR1, srcR2, permM1);
+      srcP0 = vec_perm(srcR1, srcR2, permP0);
+      srcP1 = vec_perm(srcR1, srcR2, permP1);
+      srcP2 = srcR2;
+      srcP3 = vec_perm(srcR2, srcR3, permP3);
+    } break;
+    case 13: {
+      vector unsigned char srcR3 = vec_ld(30, src);
+      srcM2 = vec_perm(srcR1, srcR2, permM2);
+      srcM1 = vec_perm(srcR1, srcR2, permM1);
+      srcP0 = vec_perm(srcR1, srcR2, permP0);
+      srcP1 = srcR2;
+      srcP2 = vec_perm(srcR2, srcR3, permP2);
+      srcP3 = vec_perm(srcR2, srcR3, permP3);
+    } break;
+    case 14: {
+      vector unsigned char srcR3 = vec_ld(30, src);
+      srcM2 = vec_perm(srcR1, srcR2, permM2);
+      srcM1 = vec_perm(srcR1, srcR2, permM1);
+      srcP0 = srcR2;
+      srcP1 = vec_perm(srcR2, srcR3, permP1);
+      srcP2 = vec_perm(srcR2, srcR3, permP2);
+      srcP3 = vec_perm(srcR2, srcR3, permP3);
+    } break;
+    case 15: {
+      vector unsigned char srcR3 = vec_ld(30, src);
+      srcM2 = vec_perm(srcR1, srcR2, permM2);
+      srcM1 = srcR2;
+      srcP0 = vec_perm(srcR2, srcR3, permP0);
+      srcP1 = vec_perm(srcR2, srcR3, permP1);
+      srcP2 = vec_perm(srcR2, srcR3, permP2);
+      srcP3 = vec_perm(srcR2, srcR3, permP3);
+    } break;
+    }
+
+    srcP0A = (vector signed short)
+                vec_mergeh((vector unsigned char)vzero, srcP0);
+    srcP0B = (vector signed short)
+                vec_mergel((vector unsigned char)vzero, srcP0);
+    srcP1A = (vector signed short)
+                vec_mergeh((vector unsigned char)vzero, srcP1);
+    srcP1B = (vector signed short)
+                vec_mergel((vector unsigned char)vzero, srcP1);
+
+    srcP2A = (vector signed short)
+                vec_mergeh((vector unsigned char)vzero, srcP2);
+    srcP2B = (vector signed short)
+                vec_mergel((vector unsigned char)vzero, srcP2);
+    srcP3A = (vector signed short)
+                vec_mergeh((vector unsigned char)vzero, srcP3);
+    srcP3B = (vector signed short)
+                vec_mergel((vector unsigned char)vzero, srcP3);
+
+    srcM1A = (vector signed short)
+                vec_mergeh((vector unsigned char)vzero, srcM1);
+    srcM1B = (vector signed short)
+                vec_mergel((vector unsigned char)vzero, srcM1);
+    srcM2A = (vector signed short)
+                vec_mergeh((vector unsigned char)vzero, srcM2);
+    srcM2B = (vector signed short)
+                vec_mergel((vector unsigned char)vzero, srcM2);
+
+    sum1A = vec_adds(srcP0A, srcP1A);
+    sum1B = vec_adds(srcP0B, srcP1B);
+    sum2A = vec_adds(srcM1A, srcP2A);
+    sum2B = vec_adds(srcM1B, srcP2B);
+    sum3A = vec_adds(srcM2A, srcP3A);
+    sum3B = vec_adds(srcM2B, srcP3B);
+
+    pp1A = vec_mladd(sum1A, v20ss, v16ss);
+    pp1B = vec_mladd(sum1B, v20ss, v16ss);
+
+    pp2A = vec_mladd(sum2A, v5ss, (vector signed short)vzero);
+    pp2B = vec_mladd(sum2B, v5ss, (vector signed short)vzero);
+
+    pp3A = vec_add(sum3A, pp1A);
+    pp3B = vec_add(sum3B, pp1B);
+
+    psumA = vec_sub(pp3A, pp2A);
+    psumB = vec_sub(pp3B, pp2B);
+
+    sumA = vec_sra(psumA, v5us);
+    sumB = vec_sra(psumB, v5us);
+
+    sum = vec_packsu(sumA, sumB);
+
+    dst1 = vec_ld(0, dst);
+    dst2 = vec_ld(16, dst);
+    vdst = vec_perm(dst1, dst2, vec_lvsl(0, dst));
+
+    OP_U8_ALTIVEC(fsum, sum, vdst);
+
+    rsum = vec_perm(fsum, fsum, dstperm);
+    fdst1 = vec_sel(dst1, rsum, dstmask);
+    fdst2 = vec_sel(rsum, dst2, dstmask);
+
+    vec_st(fdst1, 0, dst);
+    vec_st(fdst2, 16, dst);
+
+    src += srcStride;
+    dst += dstStride;
+  }
+POWERPC_PERF_STOP_COUNT(PREFIX_h264_qpel16_h_lowpass_num, 1);
+}
+
+/* this code assume stride % 16 == 0 */
+static void PREFIX_h264_qpel16_v_lowpass_altivec(uint8_t * dst, uint8_t * src, int dstStride, int srcStride) {
+  POWERPC_PERF_DECLARE(PREFIX_h264_qpel16_v_lowpass_num, 1);
+
+  register int i;
+
+  const vector signed int vzero = vec_splat_s32(0);
+  const vector unsigned char perm = vec_lvsl(0, src);
+  const vector signed short v20ss = vec_sl(vec_splat_s16(5),vec_splat_u16(2));
+  const vector unsigned short v5us = vec_splat_u16(5);
+  const vector signed short v5ss = vec_splat_s16(5);
+  const vector signed short v16ss = vec_sl(vec_splat_s16(1),vec_splat_u16(4));
+  const vector unsigned char dstperm = vec_lvsr(0, dst);
+  const vector unsigned char neg1 = (const vector unsigned char)vec_splat_s8(-1);
+  const vector unsigned char dstmask = vec_perm((const vector unsigned char)vzero, neg1, dstperm);
+
+  uint8_t *srcbis = src - (srcStride * 2);
+
+  const vector unsigned char srcM2a = vec_ld(0, srcbis);
+  const vector unsigned char srcM2b = vec_ld(16, srcbis);
+  const vector unsigned char srcM2 = vec_perm(srcM2a, srcM2b, perm);
+//  srcbis += srcStride;
+  const vector unsigned char srcM1a = vec_ld(0, srcbis += srcStride);
+  const vector unsigned char srcM1b = vec_ld(16, srcbis);
+  const vector unsigned char srcM1 = vec_perm(srcM1a, srcM1b, perm);
+//  srcbis += srcStride;
+  const vector unsigned char srcP0a = vec_ld(0, srcbis += srcStride);
+  const vector unsigned char srcP0b = vec_ld(16, srcbis);
+  const vector unsigned char srcP0 = vec_perm(srcP0a, srcP0b, perm);
+//  srcbis += srcStride;
+  const vector unsigned char srcP1a = vec_ld(0, srcbis += srcStride);
+  const vector unsigned char srcP1b = vec_ld(16, srcbis);
+  const vector unsigned char srcP1 = vec_perm(srcP1a, srcP1b, perm);
+//  srcbis += srcStride;
+  const vector unsigned char srcP2a = vec_ld(0, srcbis += srcStride);
+  const vector unsigned char srcP2b = vec_ld(16, srcbis);
+  const vector unsigned char srcP2 = vec_perm(srcP2a, srcP2b, perm);
+//  srcbis += srcStride;
+
+  vector signed short srcM2ssA = (vector signed short)
+                                vec_mergeh((vector unsigned char)vzero, srcM2);
+  vector signed short srcM2ssB = (vector signed short)
+                                vec_mergel((vector unsigned char)vzero, srcM2);
+  vector signed short srcM1ssA = (vector signed short)
+                                vec_mergeh((vector unsigned char)vzero, srcM1);
+  vector signed short srcM1ssB = (vector signed short)
+                                vec_mergel((vector unsigned char)vzero, srcM1);
+  vector signed short srcP0ssA = (vector signed short)
+                                vec_mergeh((vector unsigned char)vzero, srcP0);
+  vector signed short srcP0ssB = (vector signed short)
+                                vec_mergel((vector unsigned char)vzero, srcP0);
+  vector signed short srcP1ssA = (vector signed short)
+                                vec_mergeh((vector unsigned char)vzero, srcP1);
+  vector signed short srcP1ssB = (vector signed short)
+                                vec_mergel((vector unsigned char)vzero, srcP1);
+  vector signed short srcP2ssA = (vector signed short)
+                                vec_mergeh((vector unsigned char)vzero, srcP2);
+  vector signed short srcP2ssB = (vector signed short)
+                                vec_mergel((vector unsigned char)vzero, srcP2);
+
+  vector signed short pp1A, pp1B, pp2A, pp2B, pp3A, pp3B,
+                      psumA, psumB, sumA, sumB,
+                      srcP3ssA, srcP3ssB,
+                      sum1A, sum1B, sum2A, sum2B, sum3A, sum3B;
+
+  vector unsigned char sum, dst1, dst2, vdst, fsum, rsum, fdst1, fdst2,
+                       srcP3a, srcP3b, srcP3;
+
+  POWERPC_PERF_START_COUNT(PREFIX_h264_qpel16_v_lowpass_num, 1);
+
+  for (i = 0 ; i < 16 ; i++) {
+    srcP3a = vec_ld(0, srcbis += srcStride);
+    srcP3b = vec_ld(16, srcbis);
+    srcP3 = vec_perm(srcP3a, srcP3b, perm);
+    srcP3ssA = (vector signed short)
+                                vec_mergeh((vector unsigned char)vzero, srcP3);
+    srcP3ssB = (vector signed short)
+                                vec_mergel((vector unsigned char)vzero, srcP3);
+//    srcbis += srcStride;
+
+    sum1A = vec_adds(srcP0ssA, srcP1ssA);
+    sum1B = vec_adds(srcP0ssB, srcP1ssB);
+    sum2A = vec_adds(srcM1ssA, srcP2ssA);
+    sum2B = vec_adds(srcM1ssB, srcP2ssB);
+    sum3A = vec_adds(srcM2ssA, srcP3ssA);
+    sum3B = vec_adds(srcM2ssB, srcP3ssB);
+
+    srcM2ssA = srcM1ssA;
+    srcM2ssB = srcM1ssB;
+    srcM1ssA = srcP0ssA;
+    srcM1ssB = srcP0ssB;
+    srcP0ssA = srcP1ssA;
+    srcP0ssB = srcP1ssB;
+    srcP1ssA = srcP2ssA;
+    srcP1ssB = srcP2ssB;
+    srcP2ssA = srcP3ssA;
+    srcP2ssB = srcP3ssB;
+
+    pp1A = vec_mladd(sum1A, v20ss, v16ss);
+    pp1B = vec_mladd(sum1B, v20ss, v16ss);
+
+    pp2A = vec_mladd(sum2A, v5ss, (vector signed short)vzero);
+    pp2B = vec_mladd(sum2B, v5ss, (vector signed short)vzero);
+
+    pp3A = vec_add(sum3A, pp1A);
+    pp3B = vec_add(sum3B, pp1B);
+
+    psumA = vec_sub(pp3A, pp2A);
+    psumB = vec_sub(pp3B, pp2B);
+
+    sumA = vec_sra(psumA, v5us);
+    sumB = vec_sra(psumB, v5us);
+
+    sum = vec_packsu(sumA, sumB);
+
+    dst1 = vec_ld(0, dst);
+    dst2 = vec_ld(16, dst);
+    vdst = vec_perm(dst1, dst2, vec_lvsl(0, dst));
+
+    OP_U8_ALTIVEC(fsum, sum, vdst);
+
+    rsum = vec_perm(fsum, fsum, dstperm);
+    fdst1 = vec_sel(dst1, rsum, dstmask);
+    fdst2 = vec_sel(rsum, dst2, dstmask);
+
+    vec_st(fdst1, 0, dst);
+    vec_st(fdst2, 16, dst);
+
+    dst += dstStride;
+  }
+  POWERPC_PERF_STOP_COUNT(PREFIX_h264_qpel16_v_lowpass_num, 1);
+}
+
+/* this code assume stride % 16 == 0 *and* tmp is properly aligned */
+static void PREFIX_h264_qpel16_hv_lowpass_altivec(uint8_t * dst, int16_t * tmp, uint8_t * src, int dstStride, int tmpStride, int srcStride) {
+  POWERPC_PERF_DECLARE(PREFIX_h264_qpel16_hv_lowpass_num, 1);
+  register int i;
+  const vector signed int vzero = vec_splat_s32(0);
+  const vector unsigned char permM2 = vec_lvsl(-2, src);
+  const vector unsigned char permM1 = vec_lvsl(-1, src);
+  const vector unsigned char permP0 = vec_lvsl(+0, src);
+  const vector unsigned char permP1 = vec_lvsl(+1, src);
+  const vector unsigned char permP2 = vec_lvsl(+2, src);
+  const vector unsigned char permP3 = vec_lvsl(+3, src);
+  const vector signed short v20ss = vec_sl(vec_splat_s16(5),vec_splat_u16(2));
+  const vector unsigned int v10ui = vec_splat_u32(10);
+  const vector signed short v5ss = vec_splat_s16(5);
+  const vector signed short v1ss = vec_splat_s16(1);
+  const vector signed int v512si = vec_sl(vec_splat_s32(1),vec_splat_u32(9));
+  const vector unsigned int v16ui = vec_sl(vec_splat_u32(1),vec_splat_u32(4));
+
+  register int align = ((((unsigned long)src) - 2) % 16);
+
+  const vector unsigned char neg1 = (const vector unsigned char)
+                                                        vec_splat_s8(-1);
+
+  vector signed short srcP0A, srcP0B, srcP1A, srcP1B,
+                      srcP2A, srcP2B, srcP3A, srcP3B,
+                      srcM1A, srcM1B, srcM2A, srcM2B,
+                      sum1A, sum1B, sum2A, sum2B, sum3A, sum3B,
+                      pp1A, pp1B, pp2A, pp2B, psumA, psumB;
+
+  const vector unsigned char dstperm = vec_lvsr(0, dst);
+
+  const vector unsigned char dstmask = vec_perm((const vector unsigned char)vzero, neg1, dstperm);
+
+  const vector unsigned char mperm = (const vector unsigned char)
+    AVV(0x00, 0x08, 0x01, 0x09, 0x02, 0x0A, 0x03, 0x0B,
+        0x04, 0x0C, 0x05, 0x0D, 0x06, 0x0E, 0x07, 0x0F);
+  int16_t *tmpbis = tmp;
+
+  vector signed short tmpM1ssA, tmpM1ssB, tmpM2ssA, tmpM2ssB,
+                      tmpP0ssA, tmpP0ssB, tmpP1ssA, tmpP1ssB,
+                      tmpP2ssA, tmpP2ssB;
+
+  vector signed int pp1Ae, pp1Ao, pp1Be, pp1Bo, pp2Ae, pp2Ao, pp2Be, pp2Bo,
+                    pp3Ae, pp3Ao, pp3Be, pp3Bo, pp1cAe, pp1cAo, pp1cBe, pp1cBo,
+                    pp32Ae, pp32Ao, pp32Be, pp32Bo, sumAe, sumAo, sumBe, sumBo,
+                    ssumAe, ssumAo, ssumBe, ssumBo;
+  vector unsigned char fsum, sumv, sum, dst1, dst2, vdst,
+                       rsum, fdst1, fdst2;
+  vector signed short ssume, ssumo;
+
+  POWERPC_PERF_START_COUNT(PREFIX_h264_qpel16_hv_lowpass_num, 1);
+  src -= (2 * srcStride);
+  for (i = 0 ; i < 21 ; i ++) {
+    vector unsigned char srcM2, srcM1, srcP0, srcP1, srcP2, srcP3;
+    vector unsigned char srcR1 = vec_ld(-2, src);
+    vector unsigned char srcR2 = vec_ld(14, src);
+
+    switch (align) {
+    default: {
+      srcM2 = vec_perm(srcR1, srcR2, permM2);
+      srcM1 = vec_perm(srcR1, srcR2, permM1);
+      srcP0 = vec_perm(srcR1, srcR2, permP0);
+      srcP1 = vec_perm(srcR1, srcR2, permP1);
+      srcP2 = vec_perm(srcR1, srcR2, permP2);
+      srcP3 = vec_perm(srcR1, srcR2, permP3);
+    } break;
+    case 11: {
+      srcM2 = vec_perm(srcR1, srcR2, permM2);
+      srcM1 = vec_perm(srcR1, srcR2, permM1);
+      srcP0 = vec_perm(srcR1, srcR2, permP0);
+      srcP1 = vec_perm(srcR1, srcR2, permP1);
+      srcP2 = vec_perm(srcR1, srcR2, permP2);
+      srcP3 = srcR2;
+    } break;
+    case 12: {
+      vector unsigned char srcR3 = vec_ld(30, src);
+      srcM2 = vec_perm(srcR1, srcR2, permM2);
+      srcM1 = vec_perm(srcR1, srcR2, permM1);
+      srcP0 = vec_perm(srcR1, srcR2, permP0);
+      srcP1 = vec_perm(srcR1, srcR2, permP1);
+      srcP2 = srcR2;
+      srcP3 = vec_perm(srcR2, srcR3, permP3);
+    } break;
+    case 13: {
+      vector unsigned char srcR3 = vec_ld(30, src);
+      srcM2 = vec_perm(srcR1, srcR2, permM2);
+      srcM1 = vec_perm(srcR1, srcR2, permM1);
+      srcP0 = vec_perm(srcR1, srcR2, permP0);
+      srcP1 = srcR2;
+      srcP2 = vec_perm(srcR2, srcR3, permP2);
+      srcP3 = vec_perm(srcR2, srcR3, permP3);
+    } break;
+    case 14: {
+      vector unsigned char srcR3 = vec_ld(30, src);
+      srcM2 = vec_perm(srcR1, srcR2, permM2);
+      srcM1 = vec_perm(srcR1, srcR2, permM1);
+      srcP0 = srcR2;
+      srcP1 = vec_perm(srcR2, srcR3, permP1);
+      srcP2 = vec_perm(srcR2, srcR3, permP2);
+      srcP3 = vec_perm(srcR2, srcR3, permP3);
+    } break;
+    case 15: {
+      vector unsigned char srcR3 = vec_ld(30, src);
+      srcM2 = vec_perm(srcR1, srcR2, permM2);
+      srcM1 = srcR2;
+      srcP0 = vec_perm(srcR2, srcR3, permP0);
+      srcP1 = vec_perm(srcR2, srcR3, permP1);
+      srcP2 = vec_perm(srcR2, srcR3, permP2);
+      srcP3 = vec_perm(srcR2, srcR3, permP3);
+    } break;
+    }
+
+    srcP0A = (vector signed short)
+                            vec_mergeh((vector unsigned char)vzero, srcP0);
+    srcP0B = (vector signed short)
+                            vec_mergel((vector unsigned char)vzero, srcP0);
+    srcP1A = (vector signed short)
+                            vec_mergeh((vector unsigned char)vzero, srcP1);
+    srcP1B = (vector signed short)
+                            vec_mergel((vector unsigned char)vzero, srcP1);
+
+    srcP2A = (vector signed short)
+                            vec_mergeh((vector unsigned char)vzero, srcP2);
+    srcP2B = (vector signed short)
+                            vec_mergel((vector unsigned char)vzero, srcP2);
+    srcP3A = (vector signed short)
+                            vec_mergeh((vector unsigned char)vzero, srcP3);
+    srcP3B = (vector signed short)
+                            vec_mergel((vector unsigned char)vzero, srcP3);
+
+    srcM1A = (vector signed short)
+                            vec_mergeh((vector unsigned char)vzero, srcM1);
+    srcM1B = (vector signed short)
+                            vec_mergel((vector unsigned char)vzero, srcM1);
+    srcM2A = (vector signed short)
+                            vec_mergeh((vector unsigned char)vzero, srcM2);
+    srcM2B = (vector signed short)
+                            vec_mergel((vector unsigned char)vzero, srcM2);
+
+    sum1A = vec_adds(srcP0A, srcP1A);
+    sum1B = vec_adds(srcP0B, srcP1B);
+    sum2A = vec_adds(srcM1A, srcP2A);
+    sum2B = vec_adds(srcM1B, srcP2B);
+    sum3A = vec_adds(srcM2A, srcP3A);
+    sum3B = vec_adds(srcM2B, srcP3B);
+
+    pp1A = vec_mladd(sum1A, v20ss, sum3A);
+    pp1B = vec_mladd(sum1B, v20ss, sum3B);
+
+    pp2A = vec_mladd(sum2A, v5ss, (vector signed short)vzero);
+    pp2B = vec_mladd(sum2B, v5ss, (vector signed short)vzero);
+
+    psumA = vec_sub(pp1A, pp2A);
+    psumB = vec_sub(pp1B, pp2B);
+
+    vec_st(psumA, 0, tmp);
+    vec_st(psumB, 16, tmp);
+
+    src += srcStride;
+    tmp += tmpStride; /* int16_t*, and stride is 16, so it's OK here */
+  }
+
+  tmpM2ssA = vec_ld(0, tmpbis);
+  tmpM2ssB = vec_ld(16, tmpbis);
+  tmpbis += tmpStride;
+  tmpM1ssA = vec_ld(0, tmpbis);
+  tmpM1ssB = vec_ld(16, tmpbis);
+  tmpbis += tmpStride;
+  tmpP0ssA = vec_ld(0, tmpbis);
+  tmpP0ssB = vec_ld(16, tmpbis);
+  tmpbis += tmpStride;
+  tmpP1ssA = vec_ld(0, tmpbis);
+  tmpP1ssB = vec_ld(16, tmpbis);
+  tmpbis += tmpStride;
+  tmpP2ssA = vec_ld(0, tmpbis);
+  tmpP2ssB = vec_ld(16, tmpbis);
+  tmpbis += tmpStride;
+
+  for (i = 0 ; i < 16 ; i++) {
+    const vector signed short tmpP3ssA = vec_ld(0, tmpbis);
+    const vector signed short tmpP3ssB = vec_ld(16, tmpbis);
+
+    const vector signed short sum1A = vec_adds(tmpP0ssA, tmpP1ssA);
+    const vector signed short sum1B = vec_adds(tmpP0ssB, tmpP1ssB);
+    const vector signed short sum2A = vec_adds(tmpM1ssA, tmpP2ssA);
+    const vector signed short sum2B = vec_adds(tmpM1ssB, tmpP2ssB);
+    const vector signed short sum3A = vec_adds(tmpM2ssA, tmpP3ssA);
+    const vector signed short sum3B = vec_adds(tmpM2ssB, tmpP3ssB);
+
+    tmpbis += tmpStride;
+
+    tmpM2ssA = tmpM1ssA;
+    tmpM2ssB = tmpM1ssB;
+    tmpM1ssA = tmpP0ssA;
+    tmpM1ssB = tmpP0ssB;
+    tmpP0ssA = tmpP1ssA;
+    tmpP0ssB = tmpP1ssB;
+    tmpP1ssA = tmpP2ssA;
+    tmpP1ssB = tmpP2ssB;
+    tmpP2ssA = tmpP3ssA;
+    tmpP2ssB = tmpP3ssB;
+
+    pp1Ae = vec_mule(sum1A, v20ss);
+    pp1Ao = vec_mulo(sum1A, v20ss);
+    pp1Be = vec_mule(sum1B, v20ss);
+    pp1Bo = vec_mulo(sum1B, v20ss);
+
+    pp2Ae = vec_mule(sum2A, v5ss);
+    pp2Ao = vec_mulo(sum2A, v5ss);
+    pp2Be = vec_mule(sum2B, v5ss);
+    pp2Bo = vec_mulo(sum2B, v5ss);
+
+    pp3Ae = vec_sra((vector signed int)sum3A, v16ui);
+    pp3Ao = vec_mulo(sum3A, v1ss);
+    pp3Be = vec_sra((vector signed int)sum3B, v16ui);
+    pp3Bo = vec_mulo(sum3B, v1ss);
+
+    pp1cAe = vec_add(pp1Ae, v512si);
+    pp1cAo = vec_add(pp1Ao, v512si);
+    pp1cBe = vec_add(pp1Be, v512si);
+    pp1cBo = vec_add(pp1Bo, v512si);
+
+    pp32Ae = vec_sub(pp3Ae, pp2Ae);
+    pp32Ao = vec_sub(pp3Ao, pp2Ao);
+    pp32Be = vec_sub(pp3Be, pp2Be);
+    pp32Bo = vec_sub(pp3Bo, pp2Bo);
+
+    sumAe = vec_add(pp1cAe, pp32Ae);
+    sumAo = vec_add(pp1cAo, pp32Ao);
+    sumBe = vec_add(pp1cBe, pp32Be);
+    sumBo = vec_add(pp1cBo, pp32Bo);
+
+    ssumAe = vec_sra(sumAe, v10ui);
+    ssumAo = vec_sra(sumAo, v10ui);
+    ssumBe = vec_sra(sumBe, v10ui);
+    ssumBo = vec_sra(sumBo, v10ui);
+
+    ssume = vec_packs(ssumAe, ssumBe);
+    ssumo = vec_packs(ssumAo, ssumBo);
+
+    sumv = vec_packsu(ssume, ssumo);
+    sum = vec_perm(sumv, sumv, mperm);
+
+    dst1 = vec_ld(0, dst);
+    dst2 = vec_ld(16, dst);
+    vdst = vec_perm(dst1, dst2, vec_lvsl(0, dst));
+
+    OP_U8_ALTIVEC(fsum, sum, vdst);
+
+    rsum = vec_perm(fsum, fsum, dstperm);
+    fdst1 = vec_sel(dst1, rsum, dstmask);
+    fdst2 = vec_sel(rsum, dst2, dstmask);
+
+    vec_st(fdst1, 0, dst);
+    vec_st(fdst2, 16, dst);
+
+    dst += dstStride;
+  }
+  POWERPC_PERF_STOP_COUNT(PREFIX_h264_qpel16_hv_lowpass_num, 1);
+}

Modified: branches/avidemux_2.4_branch/avidemux/ADM_libraries/ADM_lavcodec/ppc/idct_altivec.c
===================================================================
--- branches/avidemux_2.4_branch/avidemux/ADM_libraries/ADM_lavcodec/ppc/idct_altivec.c	2007-12-03 12:38:06 UTC (rev 3719)
+++ branches/avidemux_2.4_branch/avidemux/ADM_libraries/ADM_lavcodec/ppc/idct_altivec.c	2007-12-04 00:19:48 UTC (rev 3720)
@@ -16,7 +16,6 @@
  * You should have received a copy of the GNU Lesser General Public
  * License along with FFmpeg; if not, write to the Free Software
  * Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA
- *
  */
 
 /*
@@ -39,7 +38,7 @@
 
 #include <stdlib.h>                                      /* malloc(), free() */
 #include <string.h>
-#include "../dsputil.h"
+#include "dsputil.h"
 
 #include "gcc_fixes.h"
 
@@ -171,7 +170,7 @@
 POWERPC_PERF_DECLARE(altivec_idct_put_num, 1);
     vector_u8_t tmp;
 
-#ifdef POWERPC_PERFORMANCE_REPORT
+#ifdef CONFIG_POWERPC_PERF
 POWERPC_PERF_START_COUNT(altivec_idct_put_num, 1);
 #endif
     IDCT
@@ -202,7 +201,7 @@
     vector_u8_t perm1;
     vector_u8_t p0, p1, p;
 
-#ifdef POWERPC_PERFORMANCE_REPORT
+#ifdef CONFIG_POWERPC_PERF
 POWERPC_PERF_START_COUNT(altivec_idct_add_num, 1);
 #endif
 

Added: branches/avidemux_2.4_branch/avidemux/ADM_libraries/ADM_lavcodec/ppc/int_altivec.c
===================================================================
--- branches/avidemux_2.4_branch/avidemux/ADM_libraries/ADM_lavcodec/ppc/int_altivec.c	2007-12-03 12:38:06 UTC (rev 3719)
+++ branches/avidemux_2.4_branch/avidemux/ADM_libraries/ADM_lavcodec/ppc/int_altivec.c	2007-12-04 00:19:48 UTC (rev 3720)
@@ -0,0 +1,80 @@
+/*
+ * Copyright (c) 2007 Luca Barbato <lu_zero at gentoo.org>
+ *
+ * This file is part of FFmpeg.
+ *
+ * FFmpeg is free software; you can redistribute it and/or
+ * modify it under the terms of the GNU Lesser General Public
+ * License as published by the Free Software Foundation; either
+ * version 2.1 of the License, or (at your option) any later version.
+ *
+ * FFmpeg is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+ * Lesser General Public License for more details.
+ *
+ * You should have received a copy of the GNU Lesser General Public
+ * License along with FFmpeg; if not, write to the Free Software
+ * Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA
+ */
+
+/**
+ ** @file int_altivec.c
+ ** integer misc ops.
+ **/
+
+#include "dsputil.h"
+
+#include "gcc_fixes.h"
+
+#include "dsputil_altivec.h"
+
+static int ssd_int8_vs_int16_altivec(const int8_t *pix1, const int16_t *pix2,
+                                     int size) {
+    int i, size16;
+    vector signed char vpix1;
+    vector signed short vpix2, vdiff, vpix1l,vpix1h;
+    union { vector signed int vscore;
+            int32_t score[4];
+           } u;
+    u.vscore = vec_splat_s32(0);
+//
+//XXX lazy way, fix it later
+
+#define vec_unaligned_load(b) \
+    vec_perm(vec_ld(0,b),vec_ld(15,b),vec_lvsl(0, b));
+
+    size16 = size >> 4;
+    while(size16) {
+//        score += (pix1[i]-pix2[i])*(pix1[i]-pix2[i]);
+        //load pix1 and the first batch of pix2
+
+        vpix1 = vec_unaligned_load(pix1);
+        vpix2 = vec_unaligned_load(pix2);
+        pix2 += 8;
+        //unpack
+        vpix1h = vec_unpackh(vpix1);
+        vdiff  = vec_sub(vpix1h, vpix2);
+        vpix1l = vec_unpackl(vpix1);
+        // load another batch from pix2
+        vpix2 = vec_unaligned_load(pix2);
+        u.vscore = vec_msum(vdiff, vdiff, u.vscore);
+        vdiff  = vec_sub(vpix1l, vpix2);
+        u.vscore = vec_msum(vdiff, vdiff, u.vscore);
+        pix1 += 16;
+        pix2 += 8;
+        size16--;
+    }
+    u.vscore = vec_sums(u.vscore, vec_splat_s32(0));
+
+    size %= 16;
+    for (i = 0; i < size; i++) {
+        u.score[3] += (pix1[i]-pix2[i])*(pix1[i]-pix2[i]);
+    }
+    return u.score[3];
+}
+
+void int_init_altivec(DSPContext* c, AVCodecContext *avctx)
+{
+    c->ssd_int8_vs_int16 = ssd_int8_vs_int16_altivec;
+}

Added: branches/avidemux_2.4_branch/avidemux/ADM_libraries/ADM_lavcodec/ppc/mathops.h
===================================================================
--- branches/avidemux_2.4_branch/avidemux/ADM_libraries/ADM_lavcodec/ppc/mathops.h	2007-12-03 12:38:06 UTC (rev 3719)
+++ branches/avidemux_2.4_branch/avidemux/ADM_libraries/ADM_lavcodec/ppc/mathops.h	2007-12-04 00:19:48 UTC (rev 3720)
@@ -0,0 +1,38 @@
+/*
+ * simple math operations
+ * Copyright (c) 2001, 2002 Fabrice Bellard.
+ * Copyright (c) 2006 Michael Niedermayer <michaelni at gmx.at> et al
+ *
+ * This file is part of FFmpeg.
+ *
+ * FFmpeg is free software; you can redistribute it and/or
+ * modify it under the terms of the GNU Lesser General Public
+ * License as published by the Free Software Foundation; either
+ * version 2.1 of the License, or (at your option) any later version.
+ *
+ * FFmpeg is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+ * Lesser General Public License for more details.
+ *
+ * You should have received a copy of the GNU Lesser General Public
+ * License along with FFmpeg; if not, write to the Free Software
+ * Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA
+ */
+
+#ifndef AVCODEC_PPC_MATHOPS_H
+#define AVCODEC_PPC_MATHOPS_H
+
+#if defined(ARCH_POWERPC_405)
+/* signed 16x16 -> 32 multiply add accumulate */
+#   define MAC16(rt, ra, rb) \
+        asm ("maclhw %0, %2, %3" : "=r" (rt) : "0" (rt), "r" (ra), "r" (rb));
+
+/* signed 16x16 -> 32 multiply */
+#   define MUL16(ra, rb) \
+        ({ int __rt;
+         asm ("mullhw %0, %1, %2" : "=r" (__rt) : "r" (ra), "r" (rb));
+         __rt; })
+#endif
+
+#endif // AVCODEC_PPC_MATHOPS_H

Modified: branches/avidemux_2.4_branch/avidemux/ADM_libraries/ADM_lavcodec/ppc/mpegvideo_altivec.c
===================================================================
--- branches/avidemux_2.4_branch/avidemux/ADM_libraries/ADM_lavcodec/ppc/mpegvideo_altivec.c	2007-12-03 12:38:06 UTC (rev 3719)
+++ branches/avidemux_2.4_branch/avidemux/ADM_libraries/ADM_lavcodec/ppc/mpegvideo_altivec.c	2007-12-04 00:19:48 UTC (rev 3720)
@@ -23,8 +23,8 @@
 
 #include <stdlib.h>
 #include <stdio.h>
-#include "../dsputil.h"
-#include "../mpegvideo.h"
+#include "dsputil.h"
+#include "mpegvideo.h"
 
 #include "gcc_fixes.h"
 
@@ -515,7 +515,7 @@
         }else
             qadd = 0;
         i = 1;
-        nCoeffs= 63; //does not allways use zigzag table
+        nCoeffs= 63; //does not always use zigzag table
     } else {
         i = 0;
         nCoeffs= s->intra_scantable.raster_end[ s->block_last_index[n] ];
@@ -523,17 +523,17 @@
 
     {
       register const_vector signed short vczero = (const_vector signed short)vec_splat_s16(0);
-      short __attribute__ ((aligned(16))) qmul8[] =
+      DECLARE_ALIGNED_16(short, qmul8[]) =
           {
             qmul, qmul, qmul, qmul,
             qmul, qmul, qmul, qmul
           };
-      short __attribute__ ((aligned(16))) qadd8[] =
+      DECLARE_ALIGNED_16(short, qadd8[]) =
           {
             qadd, qadd, qadd, qadd,
             qadd, qadd, qadd, qadd
           };
-      short __attribute__ ((aligned(16))) nqadd8[] =
+      DECLARE_ALIGNED_16(short, nqadd8[]) =
           {
             -qadd, -qadd, -qadd, -qadd,
             -qadd, -qadd, -qadd, -qadd

Modified: branches/avidemux_2.4_branch/avidemux/ADM_libraries/ADM_lavcodec/ppc/mpegvideo_ppc.c
===================================================================
--- branches/avidemux_2.4_branch/avidemux/ADM_libraries/ADM_lavcodec/ppc/mpegvideo_ppc.c	2007-12-03 12:38:06 UTC (rev 3719)
+++ branches/avidemux_2.4_branch/avidemux/ADM_libraries/ADM_lavcodec/ppc/mpegvideo_ppc.c	2007-12-04 00:19:48 UTC (rev 3720)
@@ -18,8 +18,8 @@
  * Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA
  */
 
-#include "../dsputil.h"
-#include "../mpegvideo.h"
+#include "dsputil.h"
+#include "mpegvideo.h"
 #include <time.h>
 
 #ifdef HAVE_ALTIVEC

Modified: branches/avidemux_2.4_branch/avidemux/ADM_libraries/ADM_lavcodec/ppc/snow_altivec.c
===================================================================
--- branches/avidemux_2.4_branch/avidemux/ADM_libraries/ADM_lavcodec/ppc/snow_altivec.c	2007-12-03 12:38:06 UTC (rev 3719)
+++ branches/avidemux_2.4_branch/avidemux/ADM_libraries/ADM_lavcodec/ppc/snow_altivec.c	2007-12-04 00:19:48 UTC (rev 3720)
@@ -1,788 +1,786 @@
-/*
- * Altivec optimized snow DSP utils
- * Copyright (c) 2006 Luca Barbato <lu_zero at gentoo.org>
- *
- * This file is part of FFmpeg.
- *
- * FFmpeg is free software; you can redistribute it and/or
- * modify it under the terms of the GNU Lesser General Public
- * License as published by the Free Software Foundation; either
- * version 2.1 of the License, or (at your option) any later version.
- *
- * FFmpeg is distributed in the hope that it will be useful,
- * but WITHOUT ANY WARRANTY; without even the implied warranty of
- * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
- * Lesser General Public License for more details.
- *
- * You should have received a copy of the GNU Lesser General Public
- * License along with FFmpeg; if not, write to the Free Software
- * Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA
- *
- *
- */
-
-#include "../dsputil.h"
-
-#include "gcc_fixes.h"
-#include "dsputil_altivec.h"
-#include "../snow.h"
-
-#undef NDEBUG
-#include <assert.h>
-
-
-
-//FIXME remove this replication
-#define slice_buffer_get_line(slice_buf, line_num) ((slice_buf)->line[line_num] ? (slice_buf)->line[line_num] : slice_buffer_load_line((slice_buf), (line_num)))
-
-static DWTELEM * slice_buffer_load_line(slice_buffer * buf, int line)
-{
-    int offset;
-    DWTELEM * buffer;
-
-//  av_log(NULL, AV_LOG_DEBUG, "Cache hit: %d\n", line);
-
-    assert(buf->data_stack_top >= 0);
-//  assert(!buf->line[line]);
-    if (buf->line[line])
-        return buf->line[line];
-
-    offset = buf->line_width * line;
-    buffer = buf->data_stack[buf->data_stack_top];
-    buf->data_stack_top--;
-    buf->line[line] = buffer;
-
-//  av_log(NULL, AV_LOG_DEBUG, "slice_buffer_load_line: line: %d remaining: %d\n", line, buf->data_stack_top + 1);
-
-    return buffer;
-}
-
-
-//altivec code
-
-void ff_snow_horizontal_compose97i_altivec(DWTELEM *b, int width)
-{
-    const int w2= (width+1)>>1;
-    DECLARE_ALIGNED_16(DWTELEM, temp[(width>>1)]);
-    const int w_l= (width>>1);
-    const int w_r= w2 - 1;
-    int i;
-    vector signed int t1, t2, x, y, tmp1, tmp2;
-    vector signed int *vbuf, *vtmp;
-    vector unsigned char align;
-
-
-
-    { // Lift 0
-        DWTELEM * const ref = b + w2 - 1;
-        DWTELEM b_0 = b[0];
-        vbuf = (vector signed int *)b;
-
-        tmp1 = vec_ld (0, ref);
-        align = vec_lvsl (0, ref);
-        tmp2 = vec_ld (15, ref);
-        t1= vec_perm(tmp1, tmp2, align);
-
-        i = 0;
-
-        for (i=0; i<w_l-15; i+=16) {
-#if 0
-        b[i+0] = b[i+0] - ((3 * (ref[i+0] + ref[i+1]) + 4) >> 3);
-        b[i+1] = b[i+1] - ((3 * (ref[i+1] + ref[i+2]) + 4) >> 3);
-        b[i+2] = b[i+2] - ((3 * (ref[i+2] + ref[i+3]) + 4) >> 3);
-        b[i+3] = b[i+3] - ((3 * (ref[i+3] + ref[i+4]) + 4) >> 3);
-#else
-
-        tmp1 = vec_ld (0, ref+4+i);
-        tmp2 = vec_ld (15, ref+4+i);
-
-        t2 = vec_perm(tmp1, tmp2, align);
-
-        y = vec_add(t1,vec_sld(t1,t2,4));
-        y = vec_add(vec_add(y,y),y);
-
-        tmp1 = vec_ld (0, ref+8+i);
-
-        y = vec_add(y, vec_splat_s32(4));
-        y = vec_sra(y, vec_splat_u32(3));
-
-        tmp2 = vec_ld (15, ref+8+i);
-
-        *vbuf = vec_sub(*vbuf, y);
-
-        t1=t2;
-
-        vbuf++;
-
-        t2 = vec_perm(tmp1, tmp2, align);
-
-        y = vec_add(t1,vec_sld(t1,t2,4));
-        y = vec_add(vec_add(y,y),y);
-
-        tmp1 = vec_ld (0, ref+12+i);
-
-        y = vec_add(y, vec_splat_s32(4));
-        y = vec_sra(y, vec_splat_u32(3));
-
-        tmp2 = vec_ld (15, ref+12+i);
-
-        *vbuf = vec_sub(*vbuf, y);
-
-        t1=t2;
-
-        vbuf++;
-
-        t2 = vec_perm(tmp1, tmp2, align);
-
-        y = vec_add(t1,vec_sld(t1,t2,4));
-        y = vec_add(vec_add(y,y),y);
-
-        tmp1 = vec_ld (0, ref+16+i);
-
-        y = vec_add(y, vec_splat_s32(4));
-        y = vec_sra(y, vec_splat_u32(3));
-
-        tmp2 = vec_ld (15, ref+16+i);
-
-        *vbuf = vec_sub(*vbuf, y);
-
-        t1=t2;
-
-        t2 = vec_perm(tmp1, tmp2, align);
-
-        y = vec_add(t1,vec_sld(t1,t2,4));
-        y = vec_add(vec_add(y,y),y);
-
-        vbuf++;
-
-        y = vec_add(y, vec_splat_s32(4));
-        y = vec_sra(y, vec_splat_u32(3));
-        *vbuf = vec_sub(*vbuf, y);
-
-        t1=t2;
-
-        vbuf++;
-
-#endif
-        }
-
-        snow_horizontal_compose_lift_lead_out(i, b, b, ref, width, w_l, 0, W_DM, W_DO, W_DS);
-        b[0] = b_0 - ((W_DM * 2 * ref[1]+W_DO)>>W_DS);
-    }
-
-    { // Lift 1
-        DWTELEM * const dst = b+w2;
-
-        i = 0;
-        for(; (((long)&dst[i]) & 0xF) && i<w_r; i++){
-            dst[i] = dst[i] - (b[i] + b[i + 1]);
-        }
-
-        align = vec_lvsl(0, b+i);
-        tmp1 = vec_ld(0, b+i);
-        vbuf = (vector signed int*) (dst + i);
-        tmp2 = vec_ld(15, b+i);
-
-        t1 = vec_perm(tmp1, tmp2, align);
-
-        for (; i<w_r-3; i+=4) {
-
-#if 0
-            dst[i]   = dst[i]   - (b[i]   + b[i + 1]);
-            dst[i+1] = dst[i+1] - (b[i+1] + b[i + 2]);
-            dst[i+2] = dst[i+2] - (b[i+2] + b[i + 3]);
-            dst[i+3] = dst[i+3] - (b[i+3] + b[i + 4]);
-#else
-
-        tmp1 = vec_ld(0, b+4+i);
-        tmp2 = vec_ld(15, b+4+i);
-
-        t2 = vec_perm(tmp1, tmp2, align);
-
-        y = vec_add(t1, vec_sld(t1,t2,4));
-        *vbuf = vec_sub (*vbuf, y);
-
-        vbuf++;
-
-        t1 = t2;
-
-#endif
-
-        }
-
-        snow_horizontal_compose_lift_lead_out(i, dst, dst, b, width, w_r, 1, W_CM, W_CO, W_CS);
-    }
-
-    { // Lift 2
-        DWTELEM * const ref = b+w2 - 1;
-        DWTELEM b_0 = b[0];
-        vbuf= (vector signed int *) b;
-
-        tmp1 = vec_ld (0, ref);
-        align = vec_lvsl (0, ref);
-        tmp2 = vec_ld (15, ref);
-        t1= vec_perm(tmp1, tmp2, align);
-
-        i = 0;
-        for (; i<w_l-15; i+=16) {
-#if 0
-            b[i]   = b[i]   - (((8 -(ref[i]   + ref[i+1])) - (b[i]  <<2)) >> 4);
-            b[i+1] = b[i+1] - (((8 -(ref[i+1] + ref[i+2])) - (b[i+1]<<2)) >> 4);
-            b[i+2] = b[i+2] - (((8 -(ref[i+2] + ref[i+3])) - (b[i+2]<<2)) >> 4);
-            b[i+3] = b[i+3] - (((8 -(ref[i+3] + ref[i+4])) - (b[i+3]<<2)) >> 4);
-#else
-            tmp1 = vec_ld (0, ref+4+i);
-            tmp2 = vec_ld (15, ref+4+i);
-
-            t2 = vec_perm(tmp1, tmp2, align);
-
-            y = vec_add(t1,vec_sld(t1,t2,4));
-            y = vec_sub(vec_splat_s32(8),y);
-
-            tmp1 = vec_ld (0, ref+8+i);
-
-            x = vec_sl(*vbuf,vec_splat_u32(2));
-            y = vec_sra(vec_sub(y,x),vec_splat_u32(4));
-
-            tmp2 = vec_ld (15, ref+8+i);
-
-            *vbuf = vec_sub( *vbuf, y);
-
-            t1 = t2;
-
-            vbuf++;
-
-            t2 = vec_perm(tmp1, tmp2, align);
-
-            y = vec_add(t1,vec_sld(t1,t2,4));
-            y = vec_sub(vec_splat_s32(8),y);
-
-            tmp1 = vec_ld (0, ref+12+i);
-
-            x = vec_sl(*vbuf,vec_splat_u32(2));
-            y = vec_sra(vec_sub(y,x),vec_splat_u32(4));
-
-            tmp2 = vec_ld (15, ref+12+i);
-
-            *vbuf = vec_sub( *vbuf, y);
-
-            t1 = t2;
-
-            vbuf++;
-
-            t2 = vec_perm(tmp1, tmp2, align);
-
-            y = vec_add(t1,vec_sld(t1,t2,4));
-            y = vec_sub(vec_splat_s32(8),y);
-
-            tmp1 = vec_ld (0, ref+16+i);
-
-            x = vec_sl(*vbuf,vec_splat_u32(2));
-            y = vec_sra(vec_sub(y,x),vec_splat_u32(4));
-
-            tmp2 = vec_ld (15, ref+16+i);
-
-            *vbuf = vec_sub( *vbuf, y);
-
-            t1 = t2;
-
-            vbuf++;
-
-            t2 = vec_perm(tmp1, tmp2, align);
-
-            y = vec_add(t1,vec_sld(t1,t2,4));
-            y = vec_sub(vec_splat_s32(8),y);
-
-            t1 = t2;
-
-            x = vec_sl(*vbuf,vec_splat_u32(2));
-            y = vec_sra(vec_sub(y,x),vec_splat_u32(4));
-            *vbuf = vec_sub( *vbuf, y);
-
-            vbuf++;
-
-#endif
-        }
-
-        snow_horizontal_compose_liftS_lead_out(i, b, b, ref, width, w_l);
-        b[0] = b_0 - (((-2 * ref[1] + W_BO) - 4 * b_0) >> W_BS);
-    }
-
-    { // Lift 3
-        DWTELEM * const src = b+w2;
-
-        vbuf = (vector signed int *)b;
-        vtmp = (vector signed int *)temp;
-
-        i = 0;
-        align = vec_lvsl(0, src);
-
-        for (; i<w_r-3; i+=4) {
-#if 0
-            temp[i] = src[i] - ((-3*(b[i] + b[i+1]))>>1);
-            temp[i+1] = src[i+1] - ((-3*(b[i+1] + b[i+2]))>>1);
-            temp[i+2] = src[i+2] - ((-3*(b[i+2] + b[i+3]))>>1);
-            temp[i+3] = src[i+3] - ((-3*(b[i+3] + b[i+4]))>>1);
-#else
-            tmp1 = vec_ld(0,src+i);
-            t1 = vec_add(vbuf[0],vec_sld(vbuf[0],vbuf[1],4));
-            tmp2 = vec_ld(15,src+i);
-            t1 = vec_sub(vec_splat_s32(0),t1); //bad!
-            t1 = vec_add(t1,vec_add(t1,t1));
-            t2 = vec_perm(tmp1 ,tmp2 ,align);
-            t1 = vec_sra(t1,vec_splat_u32(1));
-            vbuf++;
-            *vtmp = vec_sub(t2,t1);
-            vtmp++;
-
-#endif
-
-        }
-
-        snow_horizontal_compose_lift_lead_out(i, temp, src, b, width, w_r, 1, -3, 0, 1);
-    }
-
-    {
-    //Interleave
-        int a;
-        vector signed int *t = (vector signed int *)temp,
-                          *v = (vector signed int *)b;
-
-        snow_interleave_line_header(&i, width, b, temp);
-
-        for (; (i & 0xE) != 0xE; i-=2){
-            b[i+1] = temp[i>>1];
-            b[i] = b[i>>1];
-        }
-        for (i-=14; i>=0; i-=16){
-           a=i/4;
-
-           v[a+3]=vec_mergel(v[(a>>1)+1],t[(a>>1)+1]);
-           v[a+2]=vec_mergeh(v[(a>>1)+1],t[(a>>1)+1]);
-           v[a+1]=vec_mergel(v[a>>1],t[a>>1]);
-           v[a]=vec_mergeh(v[a>>1],t[a>>1]);
-
-        }
-
-    }
-}
-
-void ff_snow_vertical_compose97i_altivec(DWTELEM *b0, DWTELEM *b1, DWTELEM *b2, DWTELEM *b3, DWTELEM *b4, DWTELEM *b5, int width)
-{
-    int i, w4 = width/4;
-    vector signed int *v0, *v1,*v2,*v3,*v4,*v5;
-    vector signed int t1, t2;
-
-    v0=(vector signed int *)b0;
-    v1=(vector signed int *)b1;
-    v2=(vector signed int *)b2;
-    v3=(vector signed int *)b3;
-    v4=(vector signed int *)b4;
-    v5=(vector signed int *)b5;
-
-    for (i=0; i< w4;i++)
-    {
-
-    #if 0
-        b4[i] -= (3*(b3[i] + b5[i])+4)>>3;
-        b3[i] -= ((b2[i] + b4[i]));
-        b2[i] += ((b1[i] + b3[i])+4*b2[i]+8)>>4;
-        b1[i] += (3*(b0[i] + b2[i]))>>1;
-    #else
-        t1 = vec_add(v3[i], v5[i]);
-        t2 = vec_add(t1, vec_add(t1,t1));
-        t1 = vec_add(t2, vec_splat_s32(4));
-        v4[i] = vec_sub(v4[i], vec_sra(t1,vec_splat_u32(3)));
-
-        v3[i] = vec_sub(v3[i], vec_add(v2[i], v4[i]));
-
-        t1 = vec_add(vec_splat_s32(8), vec_add(v1[i], v3[i]));
-        t2 = vec_sl(v2[i], vec_splat_u32(2));
-        v2[i] = vec_add(v2[i], vec_sra(vec_add(t1,t2),vec_splat_u32(4)));
-        t1 = vec_add(v0[i], v2[i]);
-        t2 = vec_add(t1, vec_add(t1,t1));
-        v1[i] = vec_add(v1[i], vec_sra(t2,vec_splat_u32(1)));
-
-    #endif
-    }
-
-    for(i*=4; i < width; i++)
-    {
-        b4[i] -= (W_DM*(b3[i] + b5[i])+W_DO)>>W_DS;
-        b3[i] -= (W_CM*(b2[i] + b4[i])+W_CO)>>W_CS;
-        b2[i] += (W_BM*(b1[i] + b3[i])+4*b2[i]+W_BO)>>W_BS;
-        b1[i] += (W_AM*(b0[i] + b2[i])+W_AO)>>W_AS;
-    }
-}
-
-#define LOAD_BLOCKS \
-            tmp1 = vec_ld(0, &block[3][y*src_stride]);\
-            align = vec_lvsl(0, &block[3][y*src_stride]);\
-            tmp2 = vec_ld(15, &block[3][y*src_stride]);\
-\
-            b3 = vec_perm(tmp1,tmp2,align);\
-\
-            tmp1 = vec_ld(0, &block[2][y*src_stride]);\
-            align = vec_lvsl(0, &block[2][y*src_stride]);\
-            tmp2 = vec_ld(15, &block[2][y*src_stride]);\
-\
-            b2 = vec_perm(tmp1,tmp2,align);\
-\
-            tmp1 = vec_ld(0, &block[1][y*src_stride]);\
-            align = vec_lvsl(0, &block[1][y*src_stride]);\
-            tmp2 = vec_ld(15, &block[1][y*src_stride]);\
-\
-            b1 = vec_perm(tmp1,tmp2,align);\
-\
-            tmp1 = vec_ld(0, &block[0][y*src_stride]);\
-            align = vec_lvsl(0, &block[0][y*src_stride]);\
-            tmp2 = vec_ld(15, &block[0][y*src_stride]);\
-\
-            b0 = vec_perm(tmp1,tmp2,align);
-
-#define LOAD_OBMCS \
-            tmp1 = vec_ld(0, obmc1);\
-            align = vec_lvsl(0, obmc1);\
-            tmp2 = vec_ld(15, obmc1);\
-\
-            ob1 = vec_perm(tmp1,tmp2,align);\
-\
-            tmp1 = vec_ld(0, obmc2);\
-            align = vec_lvsl(0, obmc2);\
-            tmp2 = vec_ld(15, obmc2);\
-\
-            ob2 = vec_perm(tmp1,tmp2,align);\
-\
-            tmp1 = vec_ld(0, obmc3);\
-            align = vec_lvsl(0, obmc3);\
-            tmp2 = vec_ld(15, obmc3);\
-\
-            ob3 = vec_perm(tmp1,tmp2,align);\
-\
-            tmp1 = vec_ld(0, obmc4);\
-            align = vec_lvsl(0, obmc4);\
-            tmp2 = vec_ld(15, obmc4);\
-\
-            ob4 = vec_perm(tmp1,tmp2,align);
-
-/* interleave logic
- * h1 <- [ a,b,a,b, a,b,a,b, a,b,a,b, a,b,a,b ]
- * h2 <- [ c,d,c,d, c,d,c,d, c,d,c,d, c,d,c,d ]
- * h  <- [ a,b,c,d, a,b,c,d, a,b,c,d, a,b,c,d ]
- */
-
-#define STEPS_0_1\
-            h1 = (vector unsigned short)\
-                 vec_mergeh(ob1, ob2);\
-\
-            h2 = (vector unsigned short)\
-                 vec_mergeh(ob3, ob4);\
-\
-            ih = (vector unsigned char)\
-                 vec_mergeh(h1,h2);\
-\
-            l1 = (vector unsigned short) vec_mergeh(b3, b2);\
-\
-            ih1 = (vector unsigned char) vec_mergel(h1, h2);\
-\
-            l2 = (vector unsigned short) vec_mergeh(b1, b0);\
-\
-            il = (vector unsigned char) vec_mergeh(l1, l2);\
-\
-            v[0] = (vector signed int) vec_msum(ih, il, vec_splat_u32(0));\
-\
-            il1 = (vector unsigned char) vec_mergel(l1, l2);\
-\
-            v[1] = (vector signed int) vec_msum(ih1, il1, vec_splat_u32(0));
-
-#define FINAL_STEP_SCALAR\
-        for(x=0; x<b_w; x++)\
-            if(add){\
-                vbuf[x] += dst[x + src_x];\
-                vbuf[x] = (vbuf[x] + (1<<(FRAC_BITS-1))) >> FRAC_BITS;\
-                if(vbuf[x]&(~255)) vbuf[x]= ~(vbuf[x]>>31);\
-                dst8[x + y*src_stride] = vbuf[x];\
-            }else{\
-                dst[x + src_x] -= vbuf[x];\
-            }
-
-static void inner_add_yblock_bw_8_obmc_16_altivec(uint8_t *obmc,
-                                             const int obmc_stride,
-                                             uint8_t * * block, int b_w,
-                                             int b_h, int src_x, int src_y,
-                                             int src_stride, slice_buffer * sb,
-                                             int add, uint8_t * dst8)
-{
-    int y, x;
-    DWTELEM * dst;
-    vector unsigned short h1, h2, l1, l2;
-    vector unsigned char ih, il, ih1, il1, tmp1, tmp2, align;
-    vector unsigned char b0,b1,b2,b3;
-    vector unsigned char ob1,ob2,ob3,ob4;
-
-    DECLARE_ALIGNED_16(int, vbuf[16]);
-    vector signed int *v = (vector signed int *)vbuf, *d;
-
-    for(y=0; y<b_h; y++){
-        //FIXME ugly missue of obmc_stride
-
-        uint8_t *obmc1= obmc + y*obmc_stride;
-        uint8_t *obmc2= obmc1+ (obmc_stride>>1);
-        uint8_t *obmc3= obmc1+ obmc_stride*(obmc_stride>>1);
-        uint8_t *obmc4= obmc3+ (obmc_stride>>1);
-
-        dst = slice_buffer_get_line(sb, src_y + y);
-        d = (vector signed int *)(dst + src_x);
-
-//FIXME i could avoid some loads!
-
-        // load blocks
-        LOAD_BLOCKS
-
-        // load obmcs
-        LOAD_OBMCS
-
-        // steps 0 1
-        STEPS_0_1
-
-        FINAL_STEP_SCALAR
-
-       }
-
-}
-
-#define STEPS_2_3\
-            h1 = (vector unsigned short) vec_mergel(ob1, ob2);\
-\
-            h2 = (vector unsigned short) vec_mergel(ob3, ob4);\
-\
-            ih = (vector unsigned char) vec_mergeh(h1,h2);\
-\
-            l1 = (vector unsigned short) vec_mergel(b3, b2);\
-\
-            l2 = (vector unsigned short) vec_mergel(b1, b0);\
-\
-            ih1 = (vector unsigned char) vec_mergel(h1,h2);\
-\
-            il = (vector unsigned char) vec_mergeh(l1,l2);\
-\
-            v[2] = (vector signed int) vec_msum(ih, il, vec_splat_u32(0));\
-\
-            il1 = (vector unsigned char) vec_mergel(l1,l2);\
-\
-            v[3] = (vector signed int) vec_msum(ih1, il1, vec_splat_u32(0));
-
-
-static void inner_add_yblock_bw_16_obmc_32_altivec(uint8_t *obmc,
-                                             const int obmc_stride,
-                                             uint8_t * * block, int b_w,
-                                             int b_h, int src_x, int src_y,
-                                             int src_stride, slice_buffer * sb,
-                                             int add, uint8_t * dst8)
-{
-    int y, x;
-    DWTELEM * dst;
-    vector unsigned short h1, h2, l1, l2;
-    vector unsigned char ih, il, ih1, il1, tmp1, tmp2, align;
-    vector unsigned char b0,b1,b2,b3;
-    vector unsigned char ob1,ob2,ob3,ob4;
-    DECLARE_ALIGNED_16(int, vbuf[b_w]);
-    vector signed int *v = (vector signed int *)vbuf, *d;
-
-    for(y=0; y<b_h; y++){
-        //FIXME ugly missue of obmc_stride
-
-        uint8_t *obmc1= obmc + y*obmc_stride;
-        uint8_t *obmc2= obmc1+ (obmc_stride>>1);
-        uint8_t *obmc3= obmc1+ obmc_stride*(obmc_stride>>1);
-        uint8_t *obmc4= obmc3+ (obmc_stride>>1);
-
-        dst = slice_buffer_get_line(sb, src_y + y);
-        d = (vector signed int *)(dst + src_x);
-
-        // load blocks
-        LOAD_BLOCKS
-
-        // load obmcs
-        LOAD_OBMCS
-
-        // steps 0 1 2 3
-        STEPS_0_1
-
-        STEPS_2_3
-
-        FINAL_STEP_SCALAR
-
-    }
-}
-
-#define FINAL_STEP_VEC \
-\
-    if(add)\
-        {\
-            for(x=0; x<b_w/4; x++)\
-            {\
-                v[x] = vec_add(v[x], d[x]);\
-                v[x] = vec_sra(vec_add(v[x],\
-                                       vec_sl( vec_splat_s32(1),\
-                                               vec_splat_u32(7))),\
-                               vec_splat_u32(8));\
-\
-                mask = (vector bool int) vec_sl((vector signed int)\
-                        vec_cmpeq(v[x],v[x]),vec_splat_u32(8));\
-                mask = (vector bool int) vec_and(v[x],vec_nor(mask,mask));\
-\
-                mask = (vector bool int)\
-                        vec_cmpeq((vector signed int)mask,\
-                                  (vector signed int)vec_splat_u32(0));\
-\
-                vs = vec_sra(v[x],vec_splat_u32(8));\
-                vs = vec_sra(v[x],vec_splat_u32(8));\
-                vs = vec_sra(v[x],vec_splat_u32(15));\
-\
-                vs = vec_nor(vs,vs);\
-\
-                v[x]= vec_sel(v[x],vs,mask);\
-            }\
-\
-            for(x=0; x<b_w; x++)\
-                dst8[x + y*src_stride] = vbuf[x];\
-\
-        }\
-         else\
-            for(x=0; x<b_w/4; x++)\
-                d[x] = vec_sub(d[x], v[x]);
-
-static void inner_add_yblock_a_bw_8_obmc_16_altivec(uint8_t *obmc,
-                                             const int obmc_stride,
-                                             uint8_t * * block, int b_w,
-                                             int b_h, int src_x, int src_y,
-                                             int src_stride, slice_buffer * sb,
-                                             int add, uint8_t * dst8)
-{
-    int y, x;
-    DWTELEM * dst;
-    vector bool int mask;
-    vector signed int vs;
-    vector unsigned short h1, h2, l1, l2;
-    vector unsigned char ih, il, ih1, il1, tmp1, tmp2, align;
-    vector unsigned char b0,b1,b2,b3;
-    vector unsigned char ob1,ob2,ob3,ob4;
-
-    DECLARE_ALIGNED_16(int, vbuf[16]);
-    vector signed int *v = (vector signed int *)vbuf, *d;
-
-    for(y=0; y<b_h; y++){
-        //FIXME ugly missue of obmc_stride
-
-        uint8_t *obmc1= obmc + y*obmc_stride;
-        uint8_t *obmc2= obmc1+ (obmc_stride>>1);
-        uint8_t *obmc3= obmc1+ obmc_stride*(obmc_stride>>1);
-        uint8_t *obmc4= obmc3+ (obmc_stride>>1);
-
-        dst = slice_buffer_get_line(sb, src_y + y);
-        d = (vector signed int *)(dst + src_x);
-
-//FIXME i could avoid some loads!
-
-        // load blocks
-        LOAD_BLOCKS
-
-        // load obmcs
-        LOAD_OBMCS
-
-        // steps 0 1
-        STEPS_0_1
-
-        FINAL_STEP_VEC
-
-       }
-
-}
-
-static void inner_add_yblock_a_bw_16_obmc_32_altivec(uint8_t *obmc,
-                                             const int obmc_stride,
-                                             uint8_t * * block, int b_w,
-                                             int b_h, int src_x, int src_y,
-                                             int src_stride, slice_buffer * sb,
-                                             int add, uint8_t * dst8)
-{
-    int y, x;
-    DWTELEM * dst;
-    vector bool int mask;
-    vector signed int vs;
-    vector unsigned short h1, h2, l1, l2;
-    vector unsigned char ih, il, ih1, il1, tmp1, tmp2, align;
-    vector unsigned char b0,b1,b2,b3;
-    vector unsigned char ob1,ob2,ob3,ob4;
-    DECLARE_ALIGNED_16(int, vbuf[b_w]);
-    vector signed int *v = (vector signed int *)vbuf, *d;
-
-    for(y=0; y<b_h; y++){
-        //FIXME ugly missue of obmc_stride
-
-        uint8_t *obmc1= obmc + y*obmc_stride;
-        uint8_t *obmc2= obmc1+ (obmc_stride>>1);
-        uint8_t *obmc3= obmc1+ obmc_stride*(obmc_stride>>1);
-        uint8_t *obmc4= obmc3+ (obmc_stride>>1);
-
-        dst = slice_buffer_get_line(sb, src_y + y);
-        d = (vector signed int *)(dst + src_x);
-
-        // load blocks
-        LOAD_BLOCKS
-
-        // load obmcs
-        LOAD_OBMCS
-
-        // steps 0 1 2 3
-        STEPS_0_1
-
-        STEPS_2_3
-
-        FINAL_STEP_VEC
-
-    }
-}
-
-
-void ff_snow_inner_add_yblock_altivec(uint8_t *obmc, const int obmc_stride,
-                                      uint8_t * * block, int b_w, int b_h,
-                                      int src_x, int src_y, int src_stride,
-                                      slice_buffer * sb, int add,
-                                      uint8_t * dst8)
-{
-    if (src_x&15) {
-        if (b_w == 16)
-            inner_add_yblock_bw_16_obmc_32_altivec(obmc, obmc_stride, block,
-                                                   b_w, b_h, src_x, src_y,
-                                                   src_stride, sb, add, dst8);
-        else if (b_w == 8)
-            inner_add_yblock_bw_8_obmc_16_altivec(obmc, obmc_stride, block,
-                                                  b_w, b_h, src_x, src_y,
-                                                  src_stride, sb, add, dst8);
-        else
-            ff_snow_inner_add_yblock(obmc, obmc_stride, block, b_w, b_h, src_x,
-                                     src_y, src_stride, sb, add, dst8);
-    } else {
-        if (b_w == 16)
-            inner_add_yblock_a_bw_16_obmc_32_altivec(obmc, obmc_stride, block,
-                                                     b_w, b_h, src_x, src_y,
-                                                     src_stride, sb, add, dst8);
-        else if (b_w == 8)
-            inner_add_yblock_a_bw_8_obmc_16_altivec(obmc, obmc_stride, block,
-                                                    b_w, b_h, src_x, src_y,
-                                                    src_stride, sb, add, dst8);
-        else
-            ff_snow_inner_add_yblock(obmc, obmc_stride, block, b_w, b_h, src_x,
-                                     src_y, src_stride, sb, add, dst8);
-    }
-}
-
-
-void snow_init_altivec(DSPContext* c, AVCodecContext *avctx)
-{
-        c->horizontal_compose97i = ff_snow_horizontal_compose97i_altivec;
-        c->vertical_compose97i = ff_snow_vertical_compose97i_altivec;
-        c->inner_add_yblock = ff_snow_inner_add_yblock_altivec;
-}
+/*
+ * Altivec optimized snow DSP utils
+ * Copyright (c) 2006 Luca Barbato <lu_zero at gentoo.org>
+ *
+ * This file is part of FFmpeg.
+ *
+ * FFmpeg is free software; you can redistribute it and/or
+ * modify it under the terms of the GNU Lesser General Public
+ * License as published by the Free Software Foundation; either
+ * version 2.1 of the License, or (at your option) any later version.
+ *
+ * FFmpeg is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+ * Lesser General Public License for more details.
+ *
+ * You should have received a copy of the GNU Lesser General Public
+ * License along with FFmpeg; if not, write to the Free Software
+ * Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA
+ */
+
+#include "dsputil.h"
+
+#include "gcc_fixes.h"
+#include "dsputil_altivec.h"
+#include "snow.h"
+
+#undef NDEBUG
+#include <assert.h>
+
+
+
+//FIXME remove this replication
+#define slice_buffer_get_line(slice_buf, line_num) ((slice_buf)->line[line_num] ? (slice_buf)->line[line_num] : slice_buffer_load_line((slice_buf), (line_num)))
+
+static DWTELEM * slice_buffer_load_line(slice_buffer * buf, int line)
+{
+    int offset;
+    DWTELEM * buffer;
+
+//  av_log(NULL, AV_LOG_DEBUG, "Cache hit: %d\n", line);
+
+    assert(buf->data_stack_top >= 0);
+//  assert(!buf->line[line]);
+    if (buf->line[line])
+        return buf->line[line];
+
+    offset = buf->line_width * line;
+    buffer = buf->data_stack[buf->data_stack_top];
+    buf->data_stack_top--;
+    buf->line[line] = buffer;
+
+//  av_log(NULL, AV_LOG_DEBUG, "slice_buffer_load_line: line: %d remaining: %d\n", line, buf->data_stack_top + 1);
+
+    return buffer;
+}
+
+
+//altivec code
+
+void ff_snow_horizontal_compose97i_altivec(DWTELEM *b, int width)
+{
+    const int w2= (width+1)>>1;
+    DECLARE_ALIGNED_16(DWTELEM, temp[(width>>1)]);
+    const int w_l= (width>>1);
+    const int w_r= w2 - 1;
+    int i;
+    vector signed int t1, t2, x, y, tmp1, tmp2;
+    vector signed int *vbuf, *vtmp;
+    vector unsigned char align;
+
+
+
+    { // Lift 0
+        DWTELEM * const ref = b + w2 - 1;
+        DWTELEM b_0 = b[0];
+        vbuf = (vector signed int *)b;
+
+        tmp1 = vec_ld (0, ref);
+        align = vec_lvsl (0, ref);
+        tmp2 = vec_ld (15, ref);
+        t1= vec_perm(tmp1, tmp2, align);
+
+        i = 0;
+
+        for (i=0; i<w_l-15; i+=16) {
+#if 0
+        b[i+0] = b[i+0] - ((3 * (ref[i+0] + ref[i+1]) + 4) >> 3);
+        b[i+1] = b[i+1] - ((3 * (ref[i+1] + ref[i+2]) + 4) >> 3);
+        b[i+2] = b[i+2] - ((3 * (ref[i+2] + ref[i+3]) + 4) >> 3);
+        b[i+3] = b[i+3] - ((3 * (ref[i+3] + ref[i+4]) + 4) >> 3);
+#else
+
+        tmp1 = vec_ld (0, ref+4+i);
+        tmp2 = vec_ld (15, ref+4+i);
+
+        t2 = vec_perm(tmp1, tmp2, align);
+
+        y = vec_add(t1,vec_sld(t1,t2,4));
+        y = vec_add(vec_add(y,y),y);
+
+        tmp1 = vec_ld (0, ref+8+i);
+
+        y = vec_add(y, vec_splat_s32(4));
+        y = vec_sra(y, vec_splat_u32(3));
+
+        tmp2 = vec_ld (15, ref+8+i);
+
+        *vbuf = vec_sub(*vbuf, y);
+
+        t1=t2;
+
+        vbuf++;
+
+        t2 = vec_perm(tmp1, tmp2, align);
+
+        y = vec_add(t1,vec_sld(t1,t2,4));
+        y = vec_add(vec_add(y,y),y);
+
+        tmp1 = vec_ld (0, ref+12+i);
+
+        y = vec_add(y, vec_splat_s32(4));
+        y = vec_sra(y, vec_splat_u32(3));
+
+        tmp2 = vec_ld (15, ref+12+i);
+
+        *vbuf = vec_sub(*vbuf, y);
+
+        t1=t2;
+
+        vbuf++;
+
+        t2 = vec_perm(tmp1, tmp2, align);
+
+        y = vec_add(t1,vec_sld(t1,t2,4));
+        y = vec_add(vec_add(y,y),y);
+
+        tmp1 = vec_ld (0, ref+16+i);
+
+        y = vec_add(y, vec_splat_s32(4));
+        y = vec_sra(y, vec_splat_u32(3));
+
+        tmp2 = vec_ld (15, ref+16+i);
+
+        *vbuf = vec_sub(*vbuf, y);
+
+        t1=t2;
+
+        t2 = vec_perm(tmp1, tmp2, align);
+
+        y = vec_add(t1,vec_sld(t1,t2,4));
+        y = vec_add(vec_add(y,y),y);
+
+        vbuf++;
+
+        y = vec_add(y, vec_splat_s32(4));
+        y = vec_sra(y, vec_splat_u32(3));
+        *vbuf = vec_sub(*vbuf, y);
+
+        t1=t2;
+
+        vbuf++;
+
+#endif
+        }
+
+        snow_horizontal_compose_lift_lead_out(i, b, b, ref, width, w_l, 0, W_DM, W_DO, W_DS);
+        b[0] = b_0 - ((W_DM * 2 * ref[1]+W_DO)>>W_DS);
+    }
+
+    { // Lift 1
+        DWTELEM * const dst = b+w2;
+
+        i = 0;
+        for(; (((long)&dst[i]) & 0xF) && i<w_r; i++){
+            dst[i] = dst[i] - (b[i] + b[i + 1]);
+        }
+
+        align = vec_lvsl(0, b+i);
+        tmp1 = vec_ld(0, b+i);
+        vbuf = (vector signed int*) (dst + i);
+        tmp2 = vec_ld(15, b+i);
+
+        t1 = vec_perm(tmp1, tmp2, align);
+
+        for (; i<w_r-3; i+=4) {
+
+#if 0
+            dst[i]   = dst[i]   - (b[i]   + b[i + 1]);
+            dst[i+1] = dst[i+1] - (b[i+1] + b[i + 2]);
+            dst[i+2] = dst[i+2] - (b[i+2] + b[i + 3]);
+            dst[i+3] = dst[i+3] - (b[i+3] + b[i + 4]);
+#else
+
+        tmp1 = vec_ld(0, b+4+i);
+        tmp2 = vec_ld(15, b+4+i);
+
+        t2 = vec_perm(tmp1, tmp2, align);
+
+        y = vec_add(t1, vec_sld(t1,t2,4));
+        *vbuf = vec_sub (*vbuf, y);
+
+        vbuf++;
+
+        t1 = t2;
+
+#endif
+
+        }
+
+        snow_horizontal_compose_lift_lead_out(i, dst, dst, b, width, w_r, 1, W_CM, W_CO, W_CS);
+    }
+
+    { // Lift 2
+        DWTELEM * const ref = b+w2 - 1;
+        DWTELEM b_0 = b[0];
+        vbuf= (vector signed int *) b;
+
+        tmp1 = vec_ld (0, ref);
+        align = vec_lvsl (0, ref);
+        tmp2 = vec_ld (15, ref);
+        t1= vec_perm(tmp1, tmp2, align);
+
+        i = 0;
+        for (; i<w_l-15; i+=16) {
+#if 0
+            b[i]   = b[i]   - (((8 -(ref[i]   + ref[i+1])) - (b[i]  <<2)) >> 4);
+            b[i+1] = b[i+1] - (((8 -(ref[i+1] + ref[i+2])) - (b[i+1]<<2)) >> 4);
+            b[i+2] = b[i+2] - (((8 -(ref[i+2] + ref[i+3])) - (b[i+2]<<2)) >> 4);
+            b[i+3] = b[i+3] - (((8 -(ref[i+3] + ref[i+4])) - (b[i+3]<<2)) >> 4);
+#else
+            tmp1 = vec_ld (0, ref+4+i);
+            tmp2 = vec_ld (15, ref+4+i);
+
+            t2 = vec_perm(tmp1, tmp2, align);
+
+            y = vec_add(t1,vec_sld(t1,t2,4));
+            y = vec_sub(vec_splat_s32(8),y);
+
+            tmp1 = vec_ld (0, ref+8+i);
+
+            x = vec_sl(*vbuf,vec_splat_u32(2));
+            y = vec_sra(vec_sub(y,x),vec_splat_u32(4));
+
+            tmp2 = vec_ld (15, ref+8+i);
+
+            *vbuf = vec_sub( *vbuf, y);
+
+            t1 = t2;
+
+            vbuf++;
+
+            t2 = vec_perm(tmp1, tmp2, align);
+
+            y = vec_add(t1,vec_sld(t1,t2,4));
+            y = vec_sub(vec_splat_s32(8),y);
+
+            tmp1 = vec_ld (0, ref+12+i);
+
+            x = vec_sl(*vbuf,vec_splat_u32(2));
+            y = vec_sra(vec_sub(y,x),vec_splat_u32(4));
+
+            tmp2 = vec_ld (15, ref+12+i);
+
+            *vbuf = vec_sub( *vbuf, y);
+
+            t1 = t2;
+
+            vbuf++;
+
+            t2 = vec_perm(tmp1, tmp2, align);
+
+            y = vec_add(t1,vec_sld(t1,t2,4));
+            y = vec_sub(vec_splat_s32(8),y);
+
+            tmp1 = vec_ld (0, ref+16+i);
+
+            x = vec_sl(*vbuf,vec_splat_u32(2));
+            y = vec_sra(vec_sub(y,x),vec_splat_u32(4));
+
+            tmp2 = vec_ld (15, ref+16+i);
+
+            *vbuf = vec_sub( *vbuf, y);
+
+            t1 = t2;
+
+            vbuf++;
+
+            t2 = vec_perm(tmp1, tmp2, align);
+
+            y = vec_add(t1,vec_sld(t1,t2,4));
+            y = vec_sub(vec_splat_s32(8),y);
+
+            t1 = t2;
+
+            x = vec_sl(*vbuf,vec_splat_u32(2));
+            y = vec_sra(vec_sub(y,x),vec_splat_u32(4));
+            *vbuf = vec_sub( *vbuf, y);
+
+            vbuf++;
+
+#endif
+        }
+
+        snow_horizontal_compose_liftS_lead_out(i, b, b, ref, width, w_l);
+        b[0] = b_0 - (((-2 * ref[1] + W_BO) - 4 * b_0) >> W_BS);
+    }
+
+    { // Lift 3
+        DWTELEM * const src = b+w2;
+
+        vbuf = (vector signed int *)b;
+        vtmp = (vector signed int *)temp;
+
+        i = 0;
+        align = vec_lvsl(0, src);
+
+        for (; i<w_r-3; i+=4) {
+#if 0
+            temp[i] = src[i] - ((-3*(b[i] + b[i+1]))>>1);
+            temp[i+1] = src[i+1] - ((-3*(b[i+1] + b[i+2]))>>1);
+            temp[i+2] = src[i+2] - ((-3*(b[i+2] + b[i+3]))>>1);
+            temp[i+3] = src[i+3] - ((-3*(b[i+3] + b[i+4]))>>1);
+#else
+            tmp1 = vec_ld(0,src+i);
+            t1 = vec_add(vbuf[0],vec_sld(vbuf[0],vbuf[1],4));
+            tmp2 = vec_ld(15,src+i);
+            t1 = vec_sub(vec_splat_s32(0),t1); //bad!
+            t1 = vec_add(t1,vec_add(t1,t1));
+            t2 = vec_perm(tmp1 ,tmp2 ,align);
+            t1 = vec_sra(t1,vec_splat_u32(1));
+            vbuf++;
+            *vtmp = vec_sub(t2,t1);
+            vtmp++;
+
+#endif
+
+        }
+
+        snow_horizontal_compose_lift_lead_out(i, temp, src, b, width, w_r, 1, -3, 0, 1);
+    }
+
+    {
+    //Interleave
+        int a;
+        vector signed int *t = (vector signed int *)temp,
+                          *v = (vector signed int *)b;
+
+        snow_interleave_line_header(&i, width, b, temp);
+
+        for (; (i & 0xE) != 0xE; i-=2){
+            b[i+1] = temp[i>>1];
+            b[i] = b[i>>1];
+        }
+        for (i-=14; i>=0; i-=16){
+           a=i/4;
+
+           v[a+3]=vec_mergel(v[(a>>1)+1],t[(a>>1)+1]);
+           v[a+2]=vec_mergeh(v[(a>>1)+1],t[(a>>1)+1]);
+           v[a+1]=vec_mergel(v[a>>1],t[a>>1]);
+           v[a]=vec_mergeh(v[a>>1],t[a>>1]);
+
+        }
+
+    }
+}
+
+void ff_snow_vertical_compose97i_altivec(DWTELEM *b0, DWTELEM *b1, DWTELEM *b2, DWTELEM *b3, DWTELEM *b4, DWTELEM *b5, int width)
+{
+    int i, w4 = width/4;
+    vector signed int *v0, *v1,*v2,*v3,*v4,*v5;
+    vector signed int t1, t2;
+
+    v0=(vector signed int *)b0;
+    v1=(vector signed int *)b1;
+    v2=(vector signed int *)b2;
+    v3=(vector signed int *)b3;
+    v4=(vector signed int *)b4;
+    v5=(vector signed int *)b5;
+
+    for (i=0; i< w4;i++)
+    {
+
+    #if 0
+        b4[i] -= (3*(b3[i] + b5[i])+4)>>3;
+        b3[i] -= ((b2[i] + b4[i]));
+        b2[i] += ((b1[i] + b3[i])+4*b2[i]+8)>>4;
+        b1[i] += (3*(b0[i] + b2[i]))>>1;
+    #else
+        t1 = vec_add(v3[i], v5[i]);
+        t2 = vec_add(t1, vec_add(t1,t1));
+        t1 = vec_add(t2, vec_splat_s32(4));
+        v4[i] = vec_sub(v4[i], vec_sra(t1,vec_splat_u32(3)));
+
+        v3[i] = vec_sub(v3[i], vec_add(v2[i], v4[i]));
+
+        t1 = vec_add(vec_splat_s32(8), vec_add(v1[i], v3[i]));
+        t2 = vec_sl(v2[i], vec_splat_u32(2));
+        v2[i] = vec_add(v2[i], vec_sra(vec_add(t1,t2),vec_splat_u32(4)));
+        t1 = vec_add(v0[i], v2[i]);
+        t2 = vec_add(t1, vec_add(t1,t1));
+        v1[i] = vec_add(v1[i], vec_sra(t2,vec_splat_u32(1)));
+
+    #endif
+    }
+
+    for(i*=4; i < width; i++)
+    {
+        b4[i] -= (W_DM*(b3[i] + b5[i])+W_DO)>>W_DS;
+        b3[i] -= (W_CM*(b2[i] + b4[i])+W_CO)>>W_CS;
+        b2[i] += (W_BM*(b1[i] + b3[i])+4*b2[i]+W_BO)>>W_BS;
+        b1[i] += (W_AM*(b0[i] + b2[i])+W_AO)>>W_AS;
+    }
+}
+
+#define LOAD_BLOCKS \
+            tmp1 = vec_ld(0, &block[3][y*src_stride]);\
+            align = vec_lvsl(0, &block[3][y*src_stride]);\
+            tmp2 = vec_ld(15, &block[3][y*src_stride]);\
+\
+            b3 = vec_perm(tmp1,tmp2,align);\
+\
+            tmp1 = vec_ld(0, &block[2][y*src_stride]);\
+            align = vec_lvsl(0, &block[2][y*src_stride]);\
+            tmp2 = vec_ld(15, &block[2][y*src_stride]);\
+\
+            b2 = vec_perm(tmp1,tmp2,align);\
+\
+            tmp1 = vec_ld(0, &block[1][y*src_stride]);\
+            align = vec_lvsl(0, &block[1][y*src_stride]);\
+            tmp2 = vec_ld(15, &block[1][y*src_stride]);\
+\
+            b1 = vec_perm(tmp1,tmp2,align);\
+\
+            tmp1 = vec_ld(0, &block[0][y*src_stride]);\
+            align = vec_lvsl(0, &block[0][y*src_stride]);\
+            tmp2 = vec_ld(15, &block[0][y*src_stride]);\
+\
+            b0 = vec_perm(tmp1,tmp2,align);
+
+#define LOAD_OBMCS \
+            tmp1 = vec_ld(0, obmc1);\
+            align = vec_lvsl(0, obmc1);\
+            tmp2 = vec_ld(15, obmc1);\
+\
+            ob1 = vec_perm(tmp1,tmp2,align);\
+\
+            tmp1 = vec_ld(0, obmc2);\
+            align = vec_lvsl(0, obmc2);\
+            tmp2 = vec_ld(15, obmc2);\
+\
+            ob2 = vec_perm(tmp1,tmp2,align);\
+\
+            tmp1 = vec_ld(0, obmc3);\
+            align = vec_lvsl(0, obmc3);\
+            tmp2 = vec_ld(15, obmc3);\
+\
+            ob3 = vec_perm(tmp1,tmp2,align);\
+\
+            tmp1 = vec_ld(0, obmc4);\
+            align = vec_lvsl(0, obmc4);\
+            tmp2 = vec_ld(15, obmc4);\
+\
+            ob4 = vec_perm(tmp1,tmp2,align);
+
+/* interleave logic
+ * h1 <- [ a,b,a,b, a,b,a,b, a,b,a,b, a,b,a,b ]
+ * h2 <- [ c,d,c,d, c,d,c,d, c,d,c,d, c,d,c,d ]
+ * h  <- [ a,b,c,d, a,b,c,d, a,b,c,d, a,b,c,d ]
+ */
+
+#define STEPS_0_1\
+            h1 = (vector unsigned short)\
+                 vec_mergeh(ob1, ob2);\
+\
+            h2 = (vector unsigned short)\
+                 vec_mergeh(ob3, ob4);\
+\
+            ih = (vector unsigned char)\
+                 vec_mergeh(h1,h2);\
+\
+            l1 = (vector unsigned short) vec_mergeh(b3, b2);\
+\
+            ih1 = (vector unsigned char) vec_mergel(h1, h2);\
+\
+            l2 = (vector unsigned short) vec_mergeh(b1, b0);\
+\
+            il = (vector unsigned char) vec_mergeh(l1, l2);\
+\
+            v[0] = (vector signed int) vec_msum(ih, il, vec_splat_u32(0));\
+\
+            il1 = (vector unsigned char) vec_mergel(l1, l2);\
+\
+            v[1] = (vector signed int) vec_msum(ih1, il1, vec_splat_u32(0));
+
+#define FINAL_STEP_SCALAR\
+        for(x=0; x<b_w; x++)\
+            if(add){\
+                vbuf[x] += dst[x + src_x];\
+                vbuf[x] = (vbuf[x] + (1<<(FRAC_BITS-1))) >> FRAC_BITS;\
+                if(vbuf[x]&(~255)) vbuf[x]= ~(vbuf[x]>>31);\
+                dst8[x + y*src_stride] = vbuf[x];\
+            }else{\
+                dst[x + src_x] -= vbuf[x];\
+            }
+
+static void inner_add_yblock_bw_8_obmc_16_altivec(uint8_t *obmc,
+                                             const int obmc_stride,
+                                             uint8_t * * block, int b_w,
+                                             int b_h, int src_x, int src_y,
+                                             int src_stride, slice_buffer * sb,
+                                             int add, uint8_t * dst8)
+{
+    int y, x;
+    DWTELEM * dst;
+    vector unsigned short h1, h2, l1, l2;
+    vector unsigned char ih, il, ih1, il1, tmp1, tmp2, align;
+    vector unsigned char b0,b1,b2,b3;
+    vector unsigned char ob1,ob2,ob3,ob4;
+
+    DECLARE_ALIGNED_16(int, vbuf[16]);
+    vector signed int *v = (vector signed int *)vbuf, *d;
+
+    for(y=0; y<b_h; y++){
+        //FIXME ugly missue of obmc_stride
+
+        uint8_t *obmc1= obmc + y*obmc_stride;
+        uint8_t *obmc2= obmc1+ (obmc_stride>>1);
+        uint8_t *obmc3= obmc1+ obmc_stride*(obmc_stride>>1);
+        uint8_t *obmc4= obmc3+ (obmc_stride>>1);
+
+        dst = slice_buffer_get_line(sb, src_y + y);
+        d = (vector signed int *)(dst + src_x);
+
+//FIXME i could avoid some loads!
+
+        // load blocks
+        LOAD_BLOCKS
+
+        // load obmcs
+        LOAD_OBMCS
+
+        // steps 0 1
+        STEPS_0_1
+
+        FINAL_STEP_SCALAR
+
+       }
+
+}
+
+#define STEPS_2_3\
+            h1 = (vector unsigned short) vec_mergel(ob1, ob2);\
+\
+            h2 = (vector unsigned short) vec_mergel(ob3, ob4);\
+\
+            ih = (vector unsigned char) vec_mergeh(h1,h2);\
+\
+            l1 = (vector unsigned short) vec_mergel(b3, b2);\
+\
+            l2 = (vector unsigned short) vec_mergel(b1, b0);\
+\
+            ih1 = (vector unsigned char) vec_mergel(h1,h2);\
+\
+            il = (vector unsigned char) vec_mergeh(l1,l2);\
+\
+            v[2] = (vector signed int) vec_msum(ih, il, vec_splat_u32(0));\
+\
+            il1 = (vector unsigned char) vec_mergel(l1,l2);\
+\
+            v[3] = (vector signed int) vec_msum(ih1, il1, vec_splat_u32(0));
+
+
+static void inner_add_yblock_bw_16_obmc_32_altivec(uint8_t *obmc,
+                                             const int obmc_stride,
+                                             uint8_t * * block, int b_w,
+                                             int b_h, int src_x, int src_y,
+                                             int src_stride, slice_buffer * sb,
+                                             int add, uint8_t * dst8)
+{
+    int y, x;
+    DWTELEM * dst;
+    vector unsigned short h1, h2, l1, l2;
+    vector unsigned char ih, il, ih1, il1, tmp1, tmp2, align;
+    vector unsigned char b0,b1,b2,b3;
+    vector unsigned char ob1,ob2,ob3,ob4;
+    DECLARE_ALIGNED_16(int, vbuf[b_w]);
+    vector signed int *v = (vector signed int *)vbuf, *d;
+
+    for(y=0; y<b_h; y++){
+        //FIXME ugly missue of obmc_stride
+
+        uint8_t *obmc1= obmc + y*obmc_stride;
+        uint8_t *obmc2= obmc1+ (obmc_stride>>1);
+        uint8_t *obmc3= obmc1+ obmc_stride*(obmc_stride>>1);
+        uint8_t *obmc4= obmc3+ (obmc_stride>>1);
+
+        dst = slice_buffer_get_line(sb, src_y + y);
+        d = (vector signed int *)(dst + src_x);
+
+        // load blocks
+        LOAD_BLOCKS
+
+        // load obmcs
+        LOAD_OBMCS
+
+        // steps 0 1 2 3
+        STEPS_0_1
+
+        STEPS_2_3
+
+        FINAL_STEP_SCALAR
+
+    }
+}
+
+#define FINAL_STEP_VEC \
+\
+    if(add)\
+        {\
+            for(x=0; x<b_w/4; x++)\
+            {\
+                v[x] = vec_add(v[x], d[x]);\
+                v[x] = vec_sra(vec_add(v[x],\
+                                       vec_sl( vec_splat_s32(1),\
+                                               vec_splat_u32(7))),\
+                               vec_splat_u32(8));\
+\
+                mask = (vector bool int) vec_sl((vector signed int)\
+                        vec_cmpeq(v[x],v[x]),vec_splat_u32(8));\
+                mask = (vector bool int) vec_and(v[x],vec_nor(mask,mask));\
+\
+                mask = (vector bool int)\
+                        vec_cmpeq((vector signed int)mask,\
+                                  (vector signed int)vec_splat_u32(0));\
+\
+                vs = vec_sra(v[x],vec_splat_u32(8));\
+                vs = vec_sra(v[x],vec_splat_u32(8));\
+                vs = vec_sra(v[x],vec_splat_u32(15));\
+\
+                vs = vec_nor(vs,vs);\
+\
+                v[x]= vec_sel(v[x],vs,mask);\
+            }\
+\
+            for(x=0; x<b_w; x++)\
+                dst8[x + y*src_stride] = vbuf[x];\
+\
+        }\
+         else\
+            for(x=0; x<b_w/4; x++)\
+                d[x] = vec_sub(d[x], v[x]);
+
+static void inner_add_yblock_a_bw_8_obmc_16_altivec(uint8_t *obmc,
+                                             const int obmc_stride,
+                                             uint8_t * * block, int b_w,
+                                             int b_h, int src_x, int src_y,
+                                             int src_stride, slice_buffer * sb,
+                                             int add, uint8_t * dst8)
+{
+    int y, x;
+    DWTELEM * dst;
+    vector bool int mask;
+    vector signed int vs;
+    vector unsigned short h1, h2, l1, l2;
+    vector unsigned char ih, il, ih1, il1, tmp1, tmp2, align;
+    vector unsigned char b0,b1,b2,b3;
+    vector unsigned char ob1,ob2,ob3,ob4;
+
+    DECLARE_ALIGNED_16(int, vbuf[16]);
+    vector signed int *v = (vector signed int *)vbuf, *d;
+
+    for(y=0; y<b_h; y++){
+        //FIXME ugly missue of obmc_stride
+
+        uint8_t *obmc1= obmc + y*obmc_stride;
+        uint8_t *obmc2= obmc1+ (obmc_stride>>1);
+        uint8_t *obmc3= obmc1+ obmc_stride*(obmc_stride>>1);
+        uint8_t *obmc4= obmc3+ (obmc_stride>>1);
+
+        dst = slice_buffer_get_line(sb, src_y + y);
+        d = (vector signed int *)(dst + src_x);
+
+//FIXME i could avoid some loads!
+
+        // load blocks
+        LOAD_BLOCKS
+
+        // load obmcs
+        LOAD_OBMCS
+
+        // steps 0 1
+        STEPS_0_1
+
+        FINAL_STEP_VEC
+
+       }
+
+}
+
+static void inner_add_yblock_a_bw_16_obmc_32_altivec(uint8_t *obmc,
+                                             const int obmc_stride,
+                                             uint8_t * * block, int b_w,
+                                             int b_h, int src_x, int src_y,
+                                             int src_stride, slice_buffer * sb,
+                                             int add, uint8_t * dst8)
+{
+    int y, x;
+    DWTELEM * dst;
+    vector bool int mask;
+    vector signed int vs;
+    vector unsigned short h1, h2, l1, l2;
+    vector unsigned char ih, il, ih1, il1, tmp1, tmp2, align;
+    vector unsigned char b0,b1,b2,b3;
+    vector unsigned char ob1,ob2,ob3,ob4;
+    DECLARE_ALIGNED_16(int, vbuf[b_w]);
+    vector signed int *v = (vector signed int *)vbuf, *d;
+
+    for(y=0; y<b_h; y++){
+        //FIXME ugly missue of obmc_stride
+
+        uint8_t *obmc1= obmc + y*obmc_stride;
+        uint8_t *obmc2= obmc1+ (obmc_stride>>1);
+        uint8_t *obmc3= obmc1+ obmc_stride*(obmc_stride>>1);
+        uint8_t *obmc4= obmc3+ (obmc_stride>>1);
+
+        dst = slice_buffer_get_line(sb, src_y + y);
+        d = (vector signed int *)(dst + src_x);
+
+        // load blocks
+        LOAD_BLOCKS
+
+        // load obmcs
+        LOAD_OBMCS
+
+        // steps 0 1 2 3
+        STEPS_0_1
+
+        STEPS_2_3
+
+        FINAL_STEP_VEC
+
+    }
+}
+
+
+void ff_snow_inner_add_yblock_altivec(uint8_t *obmc, const int obmc_stride,
+                                      uint8_t * * block, int b_w, int b_h,
+                                      int src_x, int src_y, int src_stride,
+                                      slice_buffer * sb, int add,
+                                      uint8_t * dst8)
+{
+    if (src_x&15) {
+        if (b_w == 16)
+            inner_add_yblock_bw_16_obmc_32_altivec(obmc, obmc_stride, block,
+                                                   b_w, b_h, src_x, src_y,
+                                                   src_stride, sb, add, dst8);
+        else if (b_w == 8)
+            inner_add_yblock_bw_8_obmc_16_altivec(obmc, obmc_stride, block,
+                                                  b_w, b_h, src_x, src_y,
+                                                  src_stride, sb, add, dst8);
+        else
+            ff_snow_inner_add_yblock(obmc, obmc_stride, block, b_w, b_h, src_x,
+                                     src_y, src_stride, sb, add, dst8);
+    } else {
+        if (b_w == 16)
+            inner_add_yblock_a_bw_16_obmc_32_altivec(obmc, obmc_stride, block,
+                                                     b_w, b_h, src_x, src_y,
+                                                     src_stride, sb, add, dst8);
+        else if (b_w == 8)
+            inner_add_yblock_a_bw_8_obmc_16_altivec(obmc, obmc_stride, block,
+                                                    b_w, b_h, src_x, src_y,
+                                                    src_stride, sb, add, dst8);
+        else
+            ff_snow_inner_add_yblock(obmc, obmc_stride, block, b_w, b_h, src_x,
+                                     src_y, src_stride, sb, add, dst8);
+    }
+}
+
+
+void snow_init_altivec(DSPContext* c, AVCodecContext *avctx)
+{
+        c->horizontal_compose97i = ff_snow_horizontal_compose97i_altivec;
+        c->vertical_compose97i = ff_snow_vertical_compose97i_altivec;
+        c->inner_add_yblock = ff_snow_inner_add_yblock_altivec;
+}

Added: branches/avidemux_2.4_branch/avidemux/ADM_libraries/ADM_lavcodec/ppc/types_altivec.h
===================================================================
--- branches/avidemux_2.4_branch/avidemux/ADM_libraries/ADM_lavcodec/ppc/types_altivec.h	2007-12-03 12:38:06 UTC (rev 3719)
+++ branches/avidemux_2.4_branch/avidemux/ADM_libraries/ADM_lavcodec/ppc/types_altivec.h	2007-12-04 00:19:48 UTC (rev 3720)
@@ -0,0 +1,46 @@
+/*
+ * Copyright (c) 2006 Guillaume Poirier <gpoirier at mplayerhq.hu>
+ *
+ * This file is part of FFmpeg.
+ *
+ * FFmpeg is free software; you can redistribute it and/or
+ * modify it under the terms of the GNU Lesser General Public
+ * License as published by the Free Software Foundation; either
+ * version 2.1 of the License, or (at your option) any later version.
+ *
+ * FFmpeg is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+ * Lesser General Public License for more details.
+ *
+ * You should have received a copy of the GNU Lesser General Public
+ * License along with FFmpeg; if not, write to the Free Software
+ * Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA
+ */
+
+#ifndef AVCODEC_TYPES_ALTIVEC_H
+#define AVCODEC_TYPES_ALTIVEC_H
+
+/***********************************************************************
+ * Vector types
+ **********************************************************************/
+#define vec_u8_t  vector unsigned char
+#define vec_s8_t  vector signed char
+#define vec_u16_t vector unsigned short
+#define vec_s16_t vector signed short
+#define vec_u32_t vector unsigned int
+#define vec_s32_t vector signed int
+
+/***********************************************************************
+ * Null vector
+ **********************************************************************/
+#define LOAD_ZERO const vec_u8_t zerov = vec_splat_u8( 0 )
+
+#define zero_u8v  (vec_u8_t)  zerov
+#define zero_s8v  (vec_s8_t)  zerov
+#define zero_u16v (vec_u16_t) zerov
+#define zero_s16v (vec_s16_t) zerov
+#define zero_u32v (vec_u32_t) zerov
+#define zero_s32v (vec_s32_t) zerov
+
+#endif // AVCODEC_TYPES_ALTIVEC_H

Added: branches/avidemux_2.4_branch/avidemux/ADM_libraries/ADM_lavcodec/ppc/vc1dsp_altivec.c
===================================================================
--- branches/avidemux_2.4_branch/avidemux/ADM_libraries/ADM_lavcodec/ppc/vc1dsp_altivec.c	2007-12-03 12:38:06 UTC (rev 3719)
+++ branches/avidemux_2.4_branch/avidemux/ADM_libraries/ADM_lavcodec/ppc/vc1dsp_altivec.c	2007-12-04 00:19:48 UTC (rev 3720)
@@ -0,0 +1,337 @@
+/*
+ * VC-1 and WMV3 decoder - DSP functions AltiVec-optimized
+ * Copyright (c) 2006 Konstantin Shishkov
+ *
+ * This file is part of FFmpeg.
+ *
+ * FFmpeg is free software; you can redistribute it and/or
+ * modify it under the terms of the GNU Lesser General Public
+ * License as published by the Free Software Foundation; either
+ * version 2.1 of the License, or (at your option) any later version.
+ *
+ * FFmpeg is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+ * Lesser General Public License for more details.
+ *
+ * You should have received a copy of the GNU Lesser General Public
+ * License along with FFmpeg; if not, write to the Free Software
+ * Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA
+ */
+
+#include "dsputil.h"
+
+#include "gcc_fixes.h"
+
+#include "dsputil_altivec.h"
+
+// main steps of 8x8 transform
+#define STEP8(s0, s1, s2, s3, s4, s5, s6, s7, vec_rnd) \
+do { \
+    t0 = vec_sl(vec_add(s0, s4), vec_2); \
+    t0 = vec_add(vec_sl(t0, vec_1), t0); \
+    t0 = vec_add(t0, vec_rnd); \
+    t1 = vec_sl(vec_sub(s0, s4), vec_2); \
+    t1 = vec_add(vec_sl(t1, vec_1), t1); \
+    t1 = vec_add(t1, vec_rnd); \
+    t2 = vec_add(vec_sl(s6, vec_2), vec_sl(s6, vec_1)); \
+    t2 = vec_add(t2, vec_sl(s2, vec_4)); \
+    t3 = vec_add(vec_sl(s2, vec_2), vec_sl(s2, vec_1)); \
+    t3 = vec_sub(t3, vec_sl(s6, vec_4)); \
+    t4 = vec_add(t0, t2); \
+    t5 = vec_add(t1, t3); \
+    t6 = vec_sub(t1, t3); \
+    t7 = vec_sub(t0, t2); \
+\
+    t0 = vec_sl(vec_add(s1, s3), vec_4); \
+    t0 = vec_add(t0, vec_sl(s5, vec_3)); \
+    t0 = vec_add(t0, vec_sl(s7, vec_2)); \
+    t0 = vec_add(t0, vec_sub(s5, s3)); \
+\
+    t1 = vec_sl(vec_sub(s1, s5), vec_4); \
+    t1 = vec_sub(t1, vec_sl(s7, vec_3)); \
+    t1 = vec_sub(t1, vec_sl(s3, vec_2)); \
+    t1 = vec_sub(t1, vec_add(s1, s7)); \
+\
+    t2 = vec_sl(vec_sub(s7, s3), vec_4); \
+    t2 = vec_add(t2, vec_sl(s1, vec_3)); \
+    t2 = vec_add(t2, vec_sl(s5, vec_2)); \
+    t2 = vec_add(t2, vec_sub(s1, s7)); \
+\
+    t3 = vec_sl(vec_sub(s5, s7), vec_4); \
+    t3 = vec_sub(t3, vec_sl(s3, vec_3)); \
+    t3 = vec_add(t3, vec_sl(s1, vec_2)); \
+    t3 = vec_sub(t3, vec_add(s3, s5)); \
+\
+    s0 = vec_add(t4, t0); \
+    s1 = vec_add(t5, t1); \
+    s2 = vec_add(t6, t2); \
+    s3 = vec_add(t7, t3); \
+    s4 = vec_sub(t7, t3); \
+    s5 = vec_sub(t6, t2); \
+    s6 = vec_sub(t5, t1); \
+    s7 = vec_sub(t4, t0); \
+}while(0)
+
+#define SHIFT_HOR8(s0, s1, s2, s3, s4, s5, s6, s7) \
+do { \
+    s0 = vec_sra(s0, vec_3); \
+    s1 = vec_sra(s1, vec_3); \
+    s2 = vec_sra(s2, vec_3); \
+    s3 = vec_sra(s3, vec_3); \
+    s4 = vec_sra(s4, vec_3); \
+    s5 = vec_sra(s5, vec_3); \
+    s6 = vec_sra(s6, vec_3); \
+    s7 = vec_sra(s7, vec_3); \
+}while(0)
+
+#define SHIFT_VERT8(s0, s1, s2, s3, s4, s5, s6, s7) \
+do { \
+    s0 = vec_sra(s0, vec_7); \
+    s1 = vec_sra(s1, vec_7); \
+    s2 = vec_sra(s2, vec_7); \
+    s3 = vec_sra(s3, vec_7); \
+    s4 = vec_sra(vec_add(s4, vec_1s), vec_7); \
+    s5 = vec_sra(vec_add(s5, vec_1s), vec_7); \
+    s6 = vec_sra(vec_add(s6, vec_1s), vec_7); \
+    s7 = vec_sra(vec_add(s7, vec_1s), vec_7); \
+}while(0)
+
+/* main steps of 4x4 transform */
+#define STEP4(s0, s1, s2, s3, vec_rnd) \
+do { \
+    t1 = vec_add(vec_sl(s0, vec_4), s0); \
+    t1 = vec_add(t1, vec_rnd); \
+    t2 = vec_add(vec_sl(s2, vec_4), s2); \
+    t0 = vec_add(t1, t2); \
+    t1 = vec_sub(t1, t2); \
+    t3 = vec_sl(vec_sub(s3, s1), vec_1); \
+    t3 = vec_add(t3, vec_sl(t3, vec_2)); \
+    t2 = vec_add(t3, vec_sl(s1, vec_5)); \
+    t3 = vec_add(t3, vec_sl(s3, vec_3)); \
+    t3 = vec_add(t3, vec_sl(s3, vec_2)); \
+    s0 = vec_add(t0, t2); \
+    s1 = vec_sub(t1, t3); \
+    s2 = vec_add(t1, t3); \
+    s3 = vec_sub(t0, t2); \
+}while (0)
+
+#define SHIFT_HOR4(s0, s1, s2, s3) \
+    s0 = vec_sra(s0, vec_3); \
+    s1 = vec_sra(s1, vec_3); \
+    s2 = vec_sra(s2, vec_3); \
+    s3 = vec_sra(s3, vec_3);
+
+#define SHIFT_VERT4(s0, s1, s2, s3) \
+    s0 = vec_sra(s0, vec_7); \
+    s1 = vec_sra(s1, vec_7); \
+    s2 = vec_sra(s2, vec_7); \
+    s3 = vec_sra(s3, vec_7);
+
+/** Do inverse transform on 8x8 block
+*/
+static void vc1_inv_trans_8x8_altivec(DCTELEM block[64])
+{
+    vector signed short src0, src1, src2, src3, src4, src5, src6, src7;
+    vector signed int s0, s1, s2, s3, s4, s5, s6, s7;
+    vector signed int s8, s9, sA, sB, sC, sD, sE, sF;
+    vector signed int t0, t1, t2, t3, t4, t5, t6, t7;
+    const vector signed int vec_64 = vec_sl(vec_splat_s32(4), vec_splat_u32(4));
+    const vector unsigned int vec_7 = vec_splat_u32(7);
+    const vector unsigned int vec_5 = vec_splat_u32(5);
+    const vector unsigned int vec_4 = vec_splat_u32(4);
+    const vector  signed int vec_4s = vec_splat_s32(4);
+    const vector unsigned int vec_3 = vec_splat_u32(3);
+    const vector unsigned int vec_2 = vec_splat_u32(2);
+    const vector  signed int vec_1s = vec_splat_s32(1);
+    const vector unsigned int vec_1 = vec_splat_u32(1);
+
+
+    src0 = vec_ld(  0, block);
+    src1 = vec_ld( 16, block);
+    src2 = vec_ld( 32, block);
+    src3 = vec_ld( 48, block);
+    src4 = vec_ld( 64, block);
+    src5 = vec_ld( 80, block);
+    src6 = vec_ld( 96, block);
+    src7 = vec_ld(112, block);
+
+    TRANSPOSE8(src0, src1, src2, src3, src4, src5, src6, src7);
+    s0 = vec_unpackl(src0);
+    s1 = vec_unpackl(src1);
+    s2 = vec_unpackl(src2);
+    s3 = vec_unpackl(src3);
+    s4 = vec_unpackl(src4);
+    s5 = vec_unpackl(src5);
+    s6 = vec_unpackl(src6);
+    s7 = vec_unpackl(src7);
+    s8 = vec_unpackh(src0);
+    s9 = vec_unpackh(src1);
+    sA = vec_unpackh(src2);
+    sB = vec_unpackh(src3);
+    sC = vec_unpackh(src4);
+    sD = vec_unpackh(src5);
+    sE = vec_unpackh(src6);
+    sF = vec_unpackh(src7);
+    STEP8(s0, s1, s2, s3, s4, s5, s6, s7, vec_4s);
+    SHIFT_HOR8(s0, s1, s2, s3, s4, s5, s6, s7);
+    STEP8(s8, s9, sA, sB, sC, sD, sE, sF, vec_4s);
+    SHIFT_HOR8(s8, s9, sA, sB, sC, sD, sE, sF);
+    src0 = vec_pack(s8, s0);
+    src1 = vec_pack(s9, s1);
+    src2 = vec_pack(sA, s2);
+    src3 = vec_pack(sB, s3);
+    src4 = vec_pack(sC, s4);
+    src5 = vec_pack(sD, s5);
+    src6 = vec_pack(sE, s6);
+    src7 = vec_pack(sF, s7);
+    TRANSPOSE8(src0, src1, src2, src3, src4, src5, src6, src7);
+
+    s0 = vec_unpackl(src0);
+    s1 = vec_unpackl(src1);
+    s2 = vec_unpackl(src2);
+    s3 = vec_unpackl(src3);
+    s4 = vec_unpackl(src4);
+    s5 = vec_unpackl(src5);
+    s6 = vec_unpackl(src6);
+    s7 = vec_unpackl(src7);
+    s8 = vec_unpackh(src0);
+    s9 = vec_unpackh(src1);
+    sA = vec_unpackh(src2);
+    sB = vec_unpackh(src3);
+    sC = vec_unpackh(src4);
+    sD = vec_unpackh(src5);
+    sE = vec_unpackh(src6);
+    sF = vec_unpackh(src7);
+    STEP8(s0, s1, s2, s3, s4, s5, s6, s7, vec_64);
+    SHIFT_VERT8(s0, s1, s2, s3, s4, s5, s6, s7);
+    STEP8(s8, s9, sA, sB, sC, sD, sE, sF, vec_64);
+    SHIFT_VERT8(s8, s9, sA, sB, sC, sD, sE, sF);
+    src0 = vec_pack(s8, s0);
+    src1 = vec_pack(s9, s1);
+    src2 = vec_pack(sA, s2);
+    src3 = vec_pack(sB, s3);
+    src4 = vec_pack(sC, s4);
+    src5 = vec_pack(sD, s5);
+    src6 = vec_pack(sE, s6);
+    src7 = vec_pack(sF, s7);
+
+    vec_st(src0,  0, block);
+    vec_st(src1, 16, block);
+    vec_st(src2, 32, block);
+    vec_st(src3, 48, block);
+    vec_st(src4, 64, block);
+    vec_st(src5, 80, block);
+    vec_st(src6, 96, block);
+    vec_st(src7,112, block);
+}
+
+/** Do inverse transform on 8x4 part of block
+*/
+static void vc1_inv_trans_8x4_altivec(DCTELEM block[64], int n)
+{
+    vector signed short src0, src1, src2, src3, src4, src5, src6, src7;
+    vector signed int s0, s1, s2, s3, s4, s5, s6, s7;
+    vector signed int s8, s9, sA, sB, sC, sD, sE, sF;
+    vector signed int t0, t1, t2, t3, t4, t5, t6, t7;
+    const vector signed int vec_64 = vec_sl(vec_splat_s32(4), vec_splat_u32(4));
+    const vector unsigned int vec_7 = vec_splat_u32(7);
+    const vector unsigned int vec_5 = vec_splat_u32(5);
+    const vector unsigned int vec_4 = vec_splat_u32(4);
+    const vector  signed int vec_4s = vec_splat_s32(4);
+    const vector unsigned int vec_3 = vec_splat_u32(3);
+    const vector unsigned int vec_2 = vec_splat_u32(2);
+    const vector unsigned int vec_1 = vec_splat_u32(1);
+
+    src0 = vec_ld(  0, block);
+    src1 = vec_ld( 16, block);
+    src2 = vec_ld( 32, block);
+    src3 = vec_ld( 48, block);
+    src4 = vec_ld( 64, block);
+    src5 = vec_ld( 80, block);
+    src6 = vec_ld( 96, block);
+    src7 = vec_ld(112, block);
+
+    TRANSPOSE8(src0, src1, src2, src3, src4, src5, src6, src7);
+    s0 = vec_unpackl(src0);
+    s1 = vec_unpackl(src1);
+    s2 = vec_unpackl(src2);
+    s3 = vec_unpackl(src3);
+    s4 = vec_unpackl(src4);
+    s5 = vec_unpackl(src5);
+    s6 = vec_unpackl(src6);
+    s7 = vec_unpackl(src7);
+    s8 = vec_unpackh(src0);
+    s9 = vec_unpackh(src1);
+    sA = vec_unpackh(src2);
+    sB = vec_unpackh(src3);
+    sC = vec_unpackh(src4);
+    sD = vec_unpackh(src5);
+    sE = vec_unpackh(src6);
+    sF = vec_unpackh(src7);
+    STEP8(s0, s1, s2, s3, s4, s5, s6, s7, vec_4s);
+    SHIFT_HOR8(s0, s1, s2, s3, s4, s5, s6, s7);
+    STEP8(s8, s9, sA, sB, sC, sD, sE, sF, vec_4s);
+    SHIFT_HOR8(s8, s9, sA, sB, sC, sD, sE, sF);
+    src0 = vec_pack(s8, s0);
+    src1 = vec_pack(s9, s1);
+    src2 = vec_pack(sA, s2);
+    src3 = vec_pack(sB, s3);
+    src4 = vec_pack(sC, s4);
+    src5 = vec_pack(sD, s5);
+    src6 = vec_pack(sE, s6);
+    src7 = vec_pack(sF, s7);
+    TRANSPOSE8(src0, src1, src2, src3, src4, src5, src6, src7);
+
+    if(!n){ // upper half of block
+        s0 = vec_unpackh(src0);
+        s1 = vec_unpackh(src1);
+        s2 = vec_unpackh(src2);
+        s3 = vec_unpackh(src3);
+        s8 = vec_unpackl(src0);
+        s9 = vec_unpackl(src1);
+        sA = vec_unpackl(src2);
+        sB = vec_unpackl(src3);
+        STEP4(s0, s1, s2, s3, vec_64);
+        SHIFT_VERT4(s0, s1, s2, s3);
+        STEP4(s8, s9, sA, sB, vec_64);
+        SHIFT_VERT4(s8, s9, sA, sB);
+        src0 = vec_pack(s0, s8);
+        src1 = vec_pack(s1, s9);
+        src2 = vec_pack(s2, sA);
+        src3 = vec_pack(s3, sB);
+
+        vec_st(src0,  0, block);
+        vec_st(src1, 16, block);
+        vec_st(src2, 32, block);
+        vec_st(src3, 48, block);
+    } else { //lower half of block
+        s0 = vec_unpackh(src4);
+        s1 = vec_unpackh(src5);
+        s2 = vec_unpackh(src6);
+        s3 = vec_unpackh(src7);
+        s8 = vec_unpackl(src4);
+        s9 = vec_unpackl(src5);
+        sA = vec_unpackl(src6);
+        sB = vec_unpackl(src7);
+        STEP4(s0, s1, s2, s3, vec_64);
+        SHIFT_VERT4(s0, s1, s2, s3);
+        STEP4(s8, s9, sA, sB, vec_64);
+        SHIFT_VERT4(s8, s9, sA, sB);
+        src4 = vec_pack(s0, s8);
+        src5 = vec_pack(s1, s9);
+        src6 = vec_pack(s2, sA);
+        src7 = vec_pack(s3, sB);
+
+        vec_st(src4, 64, block);
+        vec_st(src5, 80, block);
+        vec_st(src6, 96, block);
+        vec_st(src7,112, block);
+    }
+}
+
+
+void vc1dsp_init_altivec(DSPContext* dsp, AVCodecContext *avctx) {
+    dsp->vc1_inv_trans_8x8 = vc1_inv_trans_8x8_altivec;
+    dsp->vc1_inv_trans_8x4 = vc1_inv_trans_8x4_altivec;
+}

Modified: branches/avidemux_2.4_branch/avidemux/ADM_libraries/ADM_libmpeg2enc/CMakeLists.txt
===================================================================
--- branches/avidemux_2.4_branch/avidemux/ADM_libraries/ADM_libmpeg2enc/CMakeLists.txt	2007-12-03 12:38:06 UTC (rev 3719)
+++ branches/avidemux_2.4_branch/avidemux/ADM_libraries/ADM_libmpeg2enc/CMakeLists.txt	2007-12-04 00:19:48 UTC (rev 3720)
@@ -14,6 +14,18 @@
 fdct_mmx.cc           ioio.c                mblock_sumsq_mmx.cc         mpegconsts.cc    puthdr.cc         quant_mmx2.cc    writepic.cc
 fdctref.cc            macroblock.cc         motion.cc                   predcomp_mmx.cc  putmpg.cc         ratectl.cc       ADM_mpe2enc.cpp
 )
+
+IF (BUILD_ALTIVEC)
+	SET(${ADM_LIB}_SRCS ${${ADM_LIB}_SRCS}
+		altivec/add_pred.c   altivec/amber.c  altivec/benchmark.c  altivec/bsad.c  altivec/bsumsq.c  altivec/bsumsq_sub22.c  altivec/build_sub22_mests.c
+		altivec/build_sub44_mests.c  altivec/detect.c  altivec/fdct.c  altivec/fdct_idct.c  altivec/field_dct_best.c  altivec/find_best_one_pel.c
+		altivec/idct.c  altivec/iquant_intra.c  altivec/iquant_non_intra.c  altivec/motion.c  altivec/pred_comp.c  altivec/quant_non_intra.c
+		altivec/quant_weight_coeff_sum.c  altivec/quantize.c  altivec/sad_00.c  altivec/sad_01.c  altivec/sad_10.c  altivec/sad_11.c
+		altivec/sub_mean_reduction.c  altivec/sub_mean_reduction_ppc.S  altivec/sub_pred.c  altivec/subsample_image.c  altivec/sumsq.c
+		altivec/sumsq_sub22.c  altivec/variance.c
+	)
+ENDIF (BUILD_ALTIVEC)
+
 ADD_LIBRARY(${ADM_LIB} STATIC ${${ADM_LIB}_SRCS})
 ADD_ADM_LIB(${ADM_LIB} ADM_libraries)
 

Modified: branches/avidemux_2.4_branch/config.h.cmake
===================================================================
--- branches/avidemux_2.4_branch/config.h.cmake	2007-12-03 12:38:06 UTC (rev 3719)
+++ branches/avidemux_2.4_branch/config.h.cmake	2007-12-04 00:19:48 UTC (rev 3720)
@@ -1,276 +1,277 @@
-/* config.h.in.  Generated from configure.in by autoheader.  */
-
-/* Jog Shuttle */
-#cmakedefine USE_JOG
-
-/* MPEG2DEC */
-#cmakedefine ACCEL_DETECT
-
-/* Big endian CPU - SPARC or PowerPC */
-#cmakedefine ADM_BIG_ENDIAN
-
-#define PACKAGE   "avidemux"
-#define ADMLOCALE "${ADM_LOCALE}"
-
-/* BSD OS specific ifdef */
-#cmakedefine ADM_BSD_FAMILY
-
-#cmakedefine HAVE_AUDIO
-
-/* Sparc workstations */
-#cmakedefine ADM_SPARC
-
-/* Build for Windows 32bits */
-#cmakedefine ADM_WIN32
-
-/* ALSA is 1.0 */
-#cmakedefine ALSA_1_0_SUPPORT
-
-/* use ALSA as possible audio device */
-#cmakedefine ALSA_SUPPORT
-
-/* AMR_NB */
-#cmakedefine AMR_NB
-
-/* X86_64 AMD64 assembly */
-#cmakedefine ARCH_64_BITS
-
-/* Enable PowerPC optim */
-#cmakedefine ARCH_POWERPC
-
-/* AltiVec for libmpeg2 */
-#cmakedefine ARCH_PPC
-
-/* post proc */
-#cmakedefine ARCH_X86
-
-/* X86_32 assembly */
-#cmakedefine ARCH_X86_32
-
-/* X86_64 AMD64 assembly */
-#cmakedefine ARCH_X86_64
-
-/* FFMPEG */
-#cmakedefine CONFIG_ENCODERS
-#cmakedefine CONFIG_DVVIDEO_ENCODER
-
-#cmakedefine CONFIG_DECODERS
-#cmakedefine CONFIG_DVVIDEO_DECODER
-#cmakedefine CONFIG_H263_DECODER
-#cmakedefine CONFIG_MPEG4_DECODER
-#cmakedefine CONFIG_MPEGAUDIO_HP
-#cmakedefine CONFIG_SNOW_DECODER
-#cmakedefine CONFIG_VC1_DECODER
-#cmakedefine CONFIG_WMV2_DECODER
-#cmakedefine CONFIG_WMV3_DECODER
-#cmakedefine CONFIG_ZLIB
-
-#cmakedefine CONFIG_MUXERS
-#cmakedefine CONFIG_MOV_MUXER
-#cmakedefine CONFIG_MP4_MUXER
-#cmakedefine CONFIG_PSP_MUXER
-#cmakedefine CONFIG_TG2_MUXER
-#cmakedefine CONFIG_TGP_MUXER
-
-#define ENABLE_MMX ${ENABLE_MMX}
-#cmakedefine ENABLE_THREADS ${ENABLE_THREADS}
-#cmakedefine HAVE_FAST_UNALIGNED
-#cmakedefine HAVE_LRINTF
-#cmakedefine HAVE_MMX
-#cmakedefine HAVE_THREADS
-#cmakedefine RUNTIME_CPUDETECT
-
-#cmakedefine HAVE_FAST_64BIT
-#cmakedefine HAVE_SSSE3
-#cmakedefine CONFIG_DARWIN
-
-/* Name mangling */
-#cmakedefine CYG_MANGLING
-
-/* Mad */
-#cmakedefine FPM_DEFAULT
-#cmakedefine FPM_INTEL
-#cmakedefine FPM_PPC
-#cmakedefine FPM_SPARC
-
-/* Using GCC 2.9x.x */
-#cmakedefine GCC_2_95_X
-
-/* gettext package name */
-#cmakedefine GETTEXT_PACKAGE
-
-/* AltiVec for mpeg2enc */
-#cmakedefine HAVE_ALTIVEC
-
-/* Enable AltiVec by default */
-#cmakedefine HAVE_ALTIVEC_H
-
-/* Enable AltiVec by default */
-#cmakedefine HAVE_BUILTIN_VECTOR
-
-/* FontConfig detected */
-#cmakedefine HAVE_FONTCONFIG
-
-/* Define if the GNU gettext() function is already present or preinstalled. */
-#cmakedefine HAVE_GETTEXT
-
-/* Define to 1 if you have the `gettimeofday' function. */
-#cmakedefine HAVE_GETTIMEOFDAY
-
-/* Define to 1 if you have the <inttypes.h> header file. */
-#cmakedefine HAVE_INTTYPES_H
-
-/* Define to 1 if you have the `mp3lame' library (-lmp3lame). */
-#cmakedefine HAVE_LIBMP3LAME
-
-/* Use malloc.h */
-#cmakedefine HAVE_MALLOC_H
-
-/* Define to 1 if you have the <stdint.h> header file. */
-#cmakedefine HAVE_STDINT_H
-
-/* Define to 1 if you have the <stdlib.h> header file. */
-#cmakedefine HAVE_STDLIB_H
-
-/* Define to 1 if you have the <string.h> header file. */
-#cmakedefine HAVE_STRING_H
-
-/* Define to 1 if you have the <sys/stat.h> header file. */
-#cmakedefine HAVE_SYS_STAT_H
-
-/* Define to 1 if you have the <sys/types.h> header file. */
-#cmakedefine HAVE_SYS_TYPES_H
-
-/* Define to 1 if you have the <unistd.h> header file. */
-#cmakedefine HAVE_UNISTD_H
-
-/* stricter prototyping */
-#cmakedefine ICONV_NEED_CONST
-
-/* use classing FAAD support */
-#cmakedefine OLD_FAAD_PROTO
-
-/* OSS detected */
-#cmakedefine OSS_SUPPORT
-
-/* Name of package */
-#cmakedefine PACKAGE
-
-/* Define to the address where bug reports for this package should be sent. */
-#cmakedefine PACKAGE_BUGREPORT
-
-/* Define to the full name of this package. */
-#cmakedefine PACKAGE_NAME
-
-/* Define to the full name and version of this package. */
-#cmakedefine PACKAGE_STRING
-
-/* Define to the one symbol short name of this package. */
-#cmakedefine PACKAGE_TARNAME
-
-/* Define to the version of this package. */
-#cmakedefine PACKAGE_VERSION
-
-/* use liba52 */
-#cmakedefine USE_AC3
-
-/* use Aften AC3 encoder */
-#cmakedefine USE_AFTEN
-
-/* Use Aften 0.07 */
-#cmakedefine USE_AFTEN_07
-
-/* Use Aften 0.08 */
-#cmakedefine USE_AFTEN_08
-
-/* AltiVec for mpeg2enc */
-#cmakedefine USE_ALTIVEC
-
-/* Tell avidemux to use libamrnb */
-#cmakedefine USE_AMR_NB
-
-/* aRts detected */
-#cmakedefine USE_ARTS
-
-/* ESD detected */
-#cmakedefine USE_ESD
-
-/* Jack detected */
-#cmakedefine USE_JACK
-
-/* Use faac audio enccoder */
-#cmakedefine USE_FAAC
-
-/* FAAD2 detected */
-#cmakedefine USE_FAAD
-
-/* FFmpeg */
-#cmakedefine USE_FFMPEG
-
-/* FontConfig detected */
-#cmakedefine USE_FONTCONFIG
-
-/* FreeType2 detected */
-#cmakedefine USE_FREETYPE
-
-/* use late binding of selected libraries */
-#cmakedefine USE_LATE_BINDING
-
-/* libdca detected */
-#cmakedefine USE_LIBDCA
-
-/* Libxml2 is available */
-#cmakedefine USE_LIBXML2
-
-/* MJPEG */
-#cmakedefine USE_MJPEG
-
-/* use libmad */
-#cmakedefine USE_MP3
-
-/* libpng is available */
-#cmakedefine USE_PNG
-
-/* use libsamplerate */
-#cmakedefine USE_SRC
-
-/* SDL detected */
-#cmakedefine USE_SDL
-
-/* Vorbis detected */
-#cmakedefine USE_VORBIS
-
-/* use x264 encoder */
-#cmakedefine USE_X264
-
-/* XVideo detected */
-#cmakedefine USE_XV
-
-/* use Xvid 1.x API */
-#cmakedefine USE_XVID_4
-
-/* use Xvid 0.9 API */
-#cmakedefine USE_XX_XVID
-
-/* Version number of package */
-#define  VERSION "${VERSION}"
-
-/* Big endian CPU - SPARC or PowerPC */
-#cmakedefine WORDS_BIGENDIAN
-
-/* use Nvwa memory leak detector */
-#cmakedefine FIND_LEAKS
-
-#cmakedefine ADM_OS_APPLE
-#cmakedefine ADM_OS_UNIX
-#cmakedefine ADM_OS_WINDOWS
-
-#cmakedefine ADM_CPU_X86
-#cmakedefine ADM_CPU_X86_64
-
-#ifdef ADM_OS_WINDOWS
-#define rindex strrchr
-#define index strchr
-#define ftello ftello64
-#define fseeko fseeko64
-#endif
+/* config.h.in.  Generated from configure.in by autoheader.  */
+
+/* Jog Shuttle */
+#cmakedefine USE_JOG
+
+/* MPEG2DEC */
+#cmakedefine ACCEL_DETECT
+
+/* Big endian CPU - SPARC or PowerPC */
+#cmakedefine ADM_BIG_ENDIAN
+
+#define PACKAGE   "avidemux"
+#define ADMLOCALE "${ADM_LOCALE}"
+
+/* BSD OS specific ifdef */
+#cmakedefine ADM_BSD_FAMILY
+
+#cmakedefine HAVE_AUDIO
+
+/* Sparc workstations */
+#cmakedefine ADM_SPARC
+
+/* Build for Windows 32bits */
+#cmakedefine ADM_WIN32
+
+/* ALSA is 1.0 */
+#cmakedefine ALSA_1_0_SUPPORT
+
+/* use ALSA as possible audio device */
+#cmakedefine ALSA_SUPPORT
+
+/* AMR_NB */
+#cmakedefine AMR_NB
+
+/* X86_64 AMD64 assembly */
+#cmakedefine ARCH_64_BITS
+
+/* Enable PowerPC optim */
+#cmakedefine ARCH_POWERPC
+
+/* AltiVec for libmpeg2 */
+#cmakedefine ARCH_PPC
+
+/* post proc */
+#cmakedefine ARCH_X86
+
+/* X86_32 assembly */
+#cmakedefine ARCH_X86_32
+
+/* X86_64 AMD64 assembly */
+#cmakedefine ARCH_X86_64
+
+/* FFMPEG */
+#cmakedefine CONFIG_ENCODERS
+#cmakedefine CONFIG_DVVIDEO_ENCODER
+
+#cmakedefine CONFIG_DECODERS
+#cmakedefine CONFIG_DVVIDEO_DECODER
+#cmakedefine CONFIG_H263_DECODER
+#cmakedefine CONFIG_MPEG4_DECODER
+#cmakedefine CONFIG_MPEGAUDIO_HP
+#cmakedefine CONFIG_SNOW_DECODER
+#cmakedefine CONFIG_VC1_DECODER
+#cmakedefine CONFIG_WMV2_DECODER
+#cmakedefine CONFIG_WMV3_DECODER
+#cmakedefine CONFIG_ZLIB
+
+#cmakedefine CONFIG_MUXERS
+#cmakedefine CONFIG_MOV_MUXER
+#cmakedefine CONFIG_MP4_MUXER
+#cmakedefine CONFIG_PSP_MUXER
+#cmakedefine CONFIG_TG2_MUXER
+#cmakedefine CONFIG_TGP_MUXER
+
+#define ENABLE_MMX ${ENABLE_MMX}
+#cmakedefine ENABLE_THREADS ${ENABLE_THREADS}
+#cmakedefine HAVE_FAST_UNALIGNED
+#cmakedefine HAVE_LRINTF
+#cmakedefine HAVE_MMX
+#cmakedefine HAVE_THREADS
+#cmakedefine RUNTIME_CPUDETECT
+
+#cmakedefine HAVE_FAST_64BIT
+#cmakedefine HAVE_SSSE3
+#cmakedefine CONFIG_DARWIN
+
+/* Name mangling */
+#cmakedefine CYG_MANGLING
+
+/* Mad */
+#cmakedefine FPM_DEFAULT
+#cmakedefine FPM_INTEL
+#cmakedefine FPM_PPC
+#cmakedefine FPM_SPARC
+
+/* Using GCC 2.9x.x */
+#cmakedefine GCC_2_95_X
+
+/* gettext package name */
+#cmakedefine GETTEXT_PACKAGE
+
+/* AltiVec for mpeg2enc */
+#cmakedefine HAVE_ALTIVEC
+
+/* Enable AltiVec by default */
+#cmakedefine HAVE_ALTIVEC_H
+
+/* Enable AltiVec by default */
+#cmakedefine HAVE_BUILTIN_VECTOR
+
+/* FontConfig detected */
+#cmakedefine HAVE_FONTCONFIG
+
+/* Define if the GNU gettext() function is already present or preinstalled. */
+#cmakedefine HAVE_GETTEXT
+
+/* Define to 1 if you have the `gettimeofday' function. */
+#cmakedefine HAVE_GETTIMEOFDAY
+
+/* Define to 1 if you have the <inttypes.h> header file. */
+#cmakedefine HAVE_INTTYPES_H
+
+/* Define to 1 if you have the `mp3lame' library (-lmp3lame). */
+#cmakedefine HAVE_LIBMP3LAME
+
+/* Use malloc.h */
+#cmakedefine HAVE_MALLOC_H
+
+/* Define to 1 if you have the <stdint.h> header file. */
+#cmakedefine HAVE_STDINT_H
+
+/* Define to 1 if you have the <stdlib.h> header file. */
+#cmakedefine HAVE_STDLIB_H
+
+/* Define to 1 if you have the <string.h> header file. */
+#cmakedefine HAVE_STRING_H
+
+/* Define to 1 if you have the <sys/stat.h> header file. */
+#cmakedefine HAVE_SYS_STAT_H
+
+/* Define to 1 if you have the <sys/types.h> header file. */
+#cmakedefine HAVE_SYS_TYPES_H
+
+/* Define to 1 if you have the <unistd.h> header file. */
+#cmakedefine HAVE_UNISTD_H
+
+/* stricter prototyping */
+#cmakedefine ICONV_NEED_CONST
+
+/* use classing FAAD support */
+#cmakedefine OLD_FAAD_PROTO
+
+/* OSS detected */
+#cmakedefine OSS_SUPPORT
+
+/* Name of package */
+#cmakedefine PACKAGE
+
+/* Define to the address where bug reports for this package should be sent. */
+#cmakedefine PACKAGE_BUGREPORT
+
+/* Define to the full name of this package. */
+#cmakedefine PACKAGE_NAME
+
+/* Define to the full name and version of this package. */
+#cmakedefine PACKAGE_STRING
+
+/* Define to the one symbol short name of this package. */
+#cmakedefine PACKAGE_TARNAME
+
+/* Define to the version of this package. */
+#cmakedefine PACKAGE_VERSION
+
+/* use liba52 */
+#cmakedefine USE_AC3
+
+/* use Aften AC3 encoder */
+#cmakedefine USE_AFTEN
+
+/* Use Aften 0.07 */
+#cmakedefine USE_AFTEN_07
+
+/* Use Aften 0.08 */
+#cmakedefine USE_AFTEN_08
+
+/* AltiVec for mpeg2enc */
+#cmakedefine USE_ALTIVEC
+
+/* Tell avidemux to use libamrnb */
+#cmakedefine USE_AMR_NB
+
+/* aRts detected */
+#cmakedefine USE_ARTS
+
+/* ESD detected */
+#cmakedefine USE_ESD
+
+/* Jack detected */
+#cmakedefine USE_JACK
+
+/* Use faac audio enccoder */
+#cmakedefine USE_FAAC
+
+/* FAAD2 detected */
+#cmakedefine USE_FAAD
+
+/* FFmpeg */
+#cmakedefine USE_FFMPEG
+
+/* FontConfig detected */
+#cmakedefine USE_FONTCONFIG
+
+/* FreeType2 detected */
+#cmakedefine USE_FREETYPE
+
+/* use late binding of selected libraries */
+#cmakedefine USE_LATE_BINDING
+
+/* libdca detected */
+#cmakedefine USE_LIBDCA
+
+/* Libxml2 is available */
+#cmakedefine USE_LIBXML2
+
+/* MJPEG */
+#cmakedefine USE_MJPEG
+
+/* use libmad */
+#cmakedefine USE_MP3
+
+/* libpng is available */
+#cmakedefine USE_PNG
+
+/* use libsamplerate */
+#cmakedefine USE_SRC
+
+/* SDL detected */
+#cmakedefine USE_SDL
+
+/* Vorbis detected */
+#cmakedefine USE_VORBIS
+
+/* use x264 encoder */
+#cmakedefine USE_X264
+
+/* XVideo detected */
+#cmakedefine USE_XV
+
+/* use Xvid 1.x API */
+#cmakedefine USE_XVID_4
+
+/* use Xvid 0.9 API */
+#cmakedefine USE_XX_XVID
+
+/* Version number of package */
+#define  VERSION "${VERSION}"
+
+/* Big endian CPU - SPARC or PowerPC */
+#cmakedefine WORDS_BIGENDIAN
+
+/* use Nvwa memory leak detector */
+#cmakedefine FIND_LEAKS
+
+#cmakedefine ADM_OS_APPLE
+#cmakedefine ADM_OS_UNIX
+#cmakedefine ADM_OS_WINDOWS
+
+#cmakedefine ADM_CPU_PPC
+#cmakedefine ADM_CPU_X86
+#cmakedefine ADM_CPU_X86_64
+
+#ifdef ADM_OS_WINDOWS
+#define rindex strrchr
+#define index strchr
+#define ftello ftello64
+#define fseeko fseeko64
+#endif



From gruntster at mail.berlios.de  Tue Dec  4 14:43:51 2007
From: gruntster at mail.berlios.de (gruntster at mail.berlios.de)
Date: Tue, 4 Dec 2007 14:43:51 +0100
Subject: [Avidemux-svn-commit] r3721 - in branches/avidemux_2.4_branch: .
	avidemux/ADM_libraries/ADM_libswscale
	avidemux/ADM_libraries/ADM_mplex
Message-ID: <200712041343.lB4DhpOo030463@sheep.berlios.de>

Author: gruntster
Date: 2007-12-04 14:43:28 +0100 (Tue, 04 Dec 2007)
New Revision: 3721

Modified:
   branches/avidemux_2.4_branch/ConfigureChecks.cmake
   branches/avidemux_2.4_branch/avidemux/ADM_libraries/ADM_libswscale/yuv2rgb_altivec.c
   branches/avidemux_2.4_branch/avidemux/ADM_libraries/ADM_mplex/CMakeLists.txt
Log:
[AltiVec] update swscale's AltiVec code + fixes

Modified: branches/avidemux_2.4_branch/ConfigureChecks.cmake
===================================================================
--- branches/avidemux_2.4_branch/ConfigureChecks.cmake	2007-12-04 00:19:48 UTC (rev 3720)
+++ branches/avidemux_2.4_branch/ConfigureChecks.cmake	2007-12-04 13:43:28 UTC (rev 3721)
@@ -58,6 +58,10 @@
 			SET(CMAKE_C_FLAGS "${CMAKE_C_FLAGS} -faltivec -force_cpusubtype_ALL")
 			SET(CMAKE_CXX_FLAGS "${CMAKE_CXX_FLAGS} -faltivec -force_cpusubtype_ALL")
 		ENDIF (ADM_OS_APPLE)
+
+		IF (ADM_OS_UNIX)
+			SET(HAVE_ALTIVEC_H 1)
+		ENDIF (ADM_OS_UNIX)
 	ENDIF (ALTIVEC)
 ENDIF (ADM_CPU_X86)
 

Modified: branches/avidemux_2.4_branch/avidemux/ADM_libraries/ADM_libswscale/yuv2rgb_altivec.c
===================================================================
--- branches/avidemux_2.4_branch/avidemux/ADM_libraries/ADM_libswscale/yuv2rgb_altivec.c	2007-12-04 00:19:48 UTC (rev 3720)
+++ branches/avidemux_2.4_branch/avidemux/ADM_libraries/ADM_libswscale/yuv2rgb_altivec.c	2007-12-04 13:43:28 UTC (rev 3721)
@@ -1,965 +1,969 @@
-/*
-  marc.hoffman at analog.com    March 8, 2004
-
-  Altivec Acceleration for Color Space Conversion revision 0.2
-
-  convert I420 YV12 to RGB in various formats,
-    it rejects images that are not in 420 formats
-    it rejects images that don't have widths of multiples of 16
-    it rejects images that don't have heights of multiples of 2
-  reject defers to C simulation codes.
-
-  lots of optimizations to be done here
-
-  1. need to fix saturation code, I just couldn't get it to fly with packs and adds.
-     so we currently use max min to clip
-
-  2. the inefficient use of chroma loading needs a bit of brushing up
-
-  3. analysis of pipeline stalls needs to be done, use shark to identify pipeline stalls
-
-
-  MODIFIED to calculate coeffs from currently selected color space.
-  MODIFIED core to be a macro which you spec the output format.
-  ADDED UYVY conversion which is never called due to some thing in SWSCALE.
-  CORRECTED algorithim selection to be strict on input formats.
-  ADDED runtime detection of altivec.
-
-  ADDED altivec_yuv2packedX vertical scl + RGB converter
-
-  March 27,2004
-  PERFORMANCE ANALYSIS
-
-  The C version use 25% of the processor or ~250Mips for D1 video rawvideo used as test
-  The ALTIVEC version uses 10% of the processor or ~100Mips for D1 video same sequence
-
-  720*480*30  ~10MPS
-
-  so we have roughly 10clocks per pixel this is too high something has to be wrong.
-
-  OPTIMIZED clip codes to utilize vec_max and vec_packs removing the need for vec_min.
-
-  OPTIMIZED DST OUTPUT cache/dma controls. we are pretty much
-  guaranteed to have the input video frame it was just decompressed so
-  it probably resides in L1 caches.  However we are creating the
-  output video stream this needs to use the DSTST instruction to
-  optimize for the cache.  We couple this with the fact that we are
-  not going to be visiting the input buffer again so we mark it Least
-  Recently Used.  This shaves 25% of the processor cycles off.
-
-  Now MEMCPY is the largest mips consumer in the system, probably due
-  to the inefficient X11 stuff.
-
-  GL libraries seem to be very slow on this machine 1.33Ghz PB running
-  Jaguar, this is not the case for my 1Ghz PB.  I thought it might be
-  a versioning issues, however i have libGL.1.2.dylib for both
-  machines. ((We need to figure this out now))
-
-  GL2 libraries work now with patch for RGB32
-
-  NOTE quartz vo driver ARGB32_to_RGB24 consumes 30% of the processor
-
-  Integrated luma prescaling adjustment for saturation/contrast/brightness adjustment. 
-*/
-
-/*
- * This file is part of FFmpeg.
- *
- * FFmpeg is free software; you can redistribute it and/or modify
- * it under the terms of the GNU General Public License as published by
- * the Free Software Foundation; either version 2 of the License, or
- * (at your option) any later version.
- *
- * FFmpeg is distributed in the hope that it will be useful,
- * but WITHOUT ANY WARRANTY; without even the implied warranty of
- * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
- * GNU General Public License for more details.
- *
- * You should have received a copy of the GNU General Public License
- * along with FFmpeg; if not, write to the Free Software
- * Foundation, Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA
- */
-#include "config.h"
-#ifdef USE_ALTIVEC
-#include <stdio.h>
-#include <stdlib.h>
-#include <string.h>
-#include <inttypes.h>
-#include <assert.h>
-#include "config.h"
-#ifdef HAVE_MALLOC_H
-#include <malloc.h>
-#endif
-#include "rgb2rgb.h"
-#include "swscale.h"
-#include "swscale_internal.h"
-
-#undef PROFILE_THE_BEAST
-#undef INC_SCALING
-
-typedef unsigned char ubyte;
-typedef signed char   sbyte;
-
-
-/* RGB interleaver, 16 planar pels 8-bit samples per channel in
-   homogeneous vector registers x0,x1,x2 are interleaved with the
-   following technique:
-
-      o0 = vec_mergeh (x0,x1);
-      o1 = vec_perm (o0, x2, perm_rgb_0);
-      o2 = vec_perm (o0, x2, perm_rgb_1);
-      o3 = vec_mergel (x0,x1);
-      o4 = vec_perm (o3,o2,perm_rgb_2);
-      o5 = vec_perm (o3,o2,perm_rgb_3);
-
-  perm_rgb_0:   o0(RG).h v1(B) --> o1*
-              0   1  2   3   4
-             rgbr|gbrg|brgb|rgbr
-             0010 0100 1001 0010
-             0102 3145 2673 894A
-
-  perm_rgb_1:   o0(RG).h v1(B) --> o2
-              0   1  2   3   4
-             gbrg|brgb|bbbb|bbbb
-             0100 1001 1111 1111
-             B5CD 6EF7 89AB CDEF
-
-  perm_rgb_2:   o3(RG).l o2(rgbB.l) --> o4*
-              0   1  2   3   4
-             gbrg|brgb|rgbr|gbrg
-             1111 1111 0010 0100
-             89AB CDEF 0182 3945
-
-  perm_rgb_2:   o3(RG).l o2(rgbB.l) ---> o5*
-              0   1  2   3   4
-             brgb|rgbr|gbrg|brgb
-             1001 0010 0100 1001
-             a67b 89cA BdCD eEFf
-
-*/
-static
-const vector unsigned char
-  perm_rgb_0 = (const vector unsigned char)AVV(0x00,0x01,0x10,0x02,0x03,0x11,0x04,0x05,
-				      0x12,0x06,0x07,0x13,0x08,0x09,0x14,0x0a),
-  perm_rgb_1 = (const vector unsigned char)AVV(0x0b,0x15,0x0c,0x0d,0x16,0x0e,0x0f,0x17,
-				      0x18,0x19,0x1a,0x1b,0x1c,0x1d,0x1e,0x1f),
-  perm_rgb_2 = (const vector unsigned char)AVV(0x10,0x11,0x12,0x13,0x14,0x15,0x16,0x17,
-				      0x00,0x01,0x18,0x02,0x03,0x19,0x04,0x05),
-  perm_rgb_3 = (const vector unsigned char)AVV(0x1a,0x06,0x07,0x1b,0x08,0x09,0x1c,0x0a,
-				      0x0b,0x1d,0x0c,0x0d,0x1e,0x0e,0x0f,0x1f);
-
-#define vec_merge3(x2,x1,x0,y0,y1,y2)    \
-do {					 \
-  typeof(x0) o0,o2,o3;			 \
-      o0 = vec_mergeh (x0,x1);		 \
-      y0 = vec_perm (o0, x2, perm_rgb_0);\
-      o2 = vec_perm (o0, x2, perm_rgb_1);\
-      o3 = vec_mergel (x0,x1);		 \
-      y1 = vec_perm (o3,o2,perm_rgb_2);	 \
-      y2 = vec_perm (o3,o2,perm_rgb_3);	 \
-} while(0)
-
-#define vec_mstbgr24(x0,x1,x2,ptr)        \
-do {					 \
-  typeof(x0) _0,_1,_2;			 \
-  vec_merge3 (x0,x1,x2,_0,_1,_2);	 \
-  vec_st (_0, 0, ptr++);		 \
-  vec_st (_1, 0, ptr++);		 \
-  vec_st (_2, 0, ptr++);		 \
-}  while (0);
-
-#define vec_mstrgb24(x0,x1,x2,ptr)       \
-do {					 \
-  typeof(x0) _0,_1,_2;			 \
-  vec_merge3 (x2,x1,x0,_0,_1,_2);	 \
-  vec_st (_0, 0, ptr++);		 \
-  vec_st (_1, 0, ptr++);		 \
-  vec_st (_2, 0, ptr++);		 \
-}  while (0);
-
-/* pack the pixels in rgb0 format
-   msb R
-   lsb 0
-*/
-#define vec_mstrgb32(T,x0,x1,x2,x3,ptr)						       \
-do {										       \
-  T _0,_1,_2,_3;								       \
-  _0 = vec_mergeh (x0,x1);							       \
-  _1 = vec_mergeh (x2,x3);					    	  	       \
-  _2 = (T)vec_mergeh ((vector unsigned short)_0,(vector unsigned short)_1);            \
-  _3 = (T)vec_mergel ((vector unsigned short)_0,(vector unsigned short)_1);            \
-  vec_st (_2, 0*16, (T *)ptr);						               \
-  vec_st (_3, 1*16, (T *)ptr);						       	       \
-  _0 = vec_mergel (x0,x1);							       \
-  _1 = vec_mergel (x2,x3);						       	       \
-  _2 = (T)vec_mergeh ((vector unsigned short)_0,(vector unsigned short)_1); 	       \
-  _3 = (T)vec_mergel ((vector unsigned short)_0,(vector unsigned short)_1); 	       \
-  vec_st (_2, 2*16, (T *)ptr);						       	       \
-  vec_st (_3, 3*16, (T *)ptr);						       	       \
-  ptr += 4;									       \
-}  while (0);
-
-/*
-
-  | 1     0       1.4021   | | Y |
-  | 1    -0.3441 -0.7142   |x| Cb|
-  | 1     1.7718  0	   | | Cr|
-
-
-  Y:      [-128 127]
-  Cb/Cr : [-128 127]
-
-  typical yuv conversion work on Y: 0-255 this version has been optimized for jpeg decode.
-
-*/
-
-
-
-
-#define vec_unh(x) \
-  (vector signed short) \
-    vec_perm(x,(typeof(x))AVV(0),\
-             (vector unsigned char)AVV(0x10,0x00,0x10,0x01,0x10,0x02,0x10,0x03,\
-                                    0x10,0x04,0x10,0x05,0x10,0x06,0x10,0x07))
-#define vec_unl(x) \
-  (vector signed short) \
-    vec_perm(x,(typeof(x))AVV(0),\
-             (vector unsigned char)AVV(0x10,0x08,0x10,0x09,0x10,0x0A,0x10,0x0B,\
-                                    0x10,0x0C,0x10,0x0D,0x10,0x0E,0x10,0x0F))
-
-#define vec_clip_s16(x) \
-  vec_max (vec_min (x, (vector signed short)AVV(235,235,235,235,235,235,235,235)),\
-                       (vector signed short)AVV(16, 16, 16, 16, 16, 16, 16, 16 ))
-
-#define vec_packclp(x,y) \
-  (vector unsigned char)vec_packs \
-      ((vector unsigned short)vec_max (x,(vector signed short) AVV(0)), \
-       (vector unsigned short)vec_max (y,(vector signed short) AVV(0)))
-
-//#define out_pixels(a,b,c,ptr) vec_mstrgb32(typeof(a),((typeof (a))AVV(0)),a,a,a,ptr)
-
-
-static inline void cvtyuvtoRGB (SwsContext *c,
-			   vector signed short Y, vector signed short U, vector signed short V,
-			   vector signed short *R, vector signed short *G, vector signed short *B)
-{
-  vector signed   short vx,ux,uvx;
-
-  Y = vec_mradds (Y, c->CY, c->OY);
-  U  = vec_sub (U,(vector signed short)
-  			vec_splat((vector signed short)AVV(128),0));
-  V  = vec_sub (V,(vector signed short)
-  			vec_splat((vector signed short)AVV(128),0));
-
-  //   ux  = (CBU*(u<<c->CSHIFT)+0x4000)>>15;
-  ux = vec_sl (U, c->CSHIFT);
-  *B = vec_mradds (ux, c->CBU, Y);
-
-  // vx  = (CRV*(v<<c->CSHIFT)+0x4000)>>15;
-  vx = vec_sl (V, c->CSHIFT);
-  *R = vec_mradds (vx, c->CRV, Y);
-
-  // uvx = ((CGU*u) + (CGV*v))>>15;
-  uvx = vec_mradds (U, c->CGU, Y);
-  *G = vec_mradds (V, c->CGV, uvx);
-}
-
-
-/*
-  ------------------------------------------------------------------------------
-  CS converters
-  ------------------------------------------------------------------------------
-*/
-
-
-#define DEFCSP420_CVT(name,out_pixels)                                     \
-static int altivec_##name (SwsContext *c,                                  \
-				unsigned char **in, int *instrides,	   \
-				int srcSliceY,	int srcSliceH,		   \
-				unsigned char **oplanes, int *outstrides)  \
-{									   \
-  int w = c->srcW;							   \
-  int h = srcSliceH;							   \
-  int i,j;								   \
-  int instrides_scl[3];							   \
-  vector unsigned char y0,y1;						   \
-									   \
-  vector signed char  u,v;						   \
-									   \
-  vector signed short Y0,Y1,Y2,Y3;					   \
-  vector signed short U,V;						   \
-  vector signed short vx,ux,uvx;					   \
-  vector signed short vx0,ux0,uvx0;					   \
-  vector signed short vx1,ux1,uvx1;					   \
-  vector signed short R0,G0,B0;						   \
-  vector signed short R1,G1,B1;						   \
-  vector unsigned char R,G,B;						   \
-									   \
-  vector unsigned char *y1ivP, *y2ivP, *uivP, *vivP;			   \
-  vector unsigned char align_perm;					   \
-									   \
-  vector signed short 							   \
-    lCY  = c->CY,							   \
-    lOY  = c->OY,							   \
-    lCRV = c->CRV,							   \
-    lCBU = c->CBU,							   \
-    lCGU = c->CGU,							   \
-    lCGV = c->CGV;							   \
-									   \
-  vector unsigned short lCSHIFT = c->CSHIFT;				   \
-									   \
-  ubyte *y1i   = in[0];							   \
-  ubyte *y2i   = in[0]+instrides[0];					   \
-  ubyte *ui    = in[1];							   \
-  ubyte *vi    = in[2];							   \
-									   \
-  vector unsigned char *oute						   \
-    = (vector unsigned char *)						   \
-        (oplanes[0]+srcSliceY*outstrides[0]);				   \
-  vector unsigned char *outo						   \
-    = (vector unsigned char *)						   \
-        (oplanes[0]+srcSliceY*outstrides[0]+outstrides[0]);		   \
-									   \
-									   \
-  instrides_scl[0] = instrides[0]*2-w;  /* the loop moves y{1,2}i by w */  \
-  instrides_scl[1] = instrides[1]-w/2;  /* the loop moves ui by w/2 */	   \
-  instrides_scl[2] = instrides[2]-w/2;  /* the loop moves vi by w/2 */	   \
-									   \
-									   \
-  for (i=0;i<h/2;i++) {							   \
-    vec_dstst (outo, (0x02000002|(((w*3+32)/32)<<16)), 0);                 \
-    vec_dstst (oute, (0x02000002|(((w*3+32)/32)<<16)), 1);                 \
-									   \
-    for (j=0;j<w/16;j++) {						   \
-									   \
-      y1ivP = (vector unsigned char *)y1i;				   \
-      y2ivP = (vector unsigned char *)y2i;				   \
-      uivP = (vector unsigned char *)ui;				   \
-      vivP = (vector unsigned char *)vi;				   \
-									   \
-      align_perm = vec_lvsl (0, y1i);					   \
-      y0 = (vector unsigned char)vec_perm (y1ivP[0], y1ivP[1], align_perm);\
-									   \
-      align_perm = vec_lvsl (0, y2i);					   \
-      y1 = (vector unsigned char)vec_perm (y2ivP[0], y2ivP[1], align_perm);\
-									   \
-      align_perm = vec_lvsl (0, ui);					   \
-      u = (vector signed char)vec_perm (uivP[0], uivP[1], align_perm);	   \
-									   \
-      align_perm = vec_lvsl (0, vi);					   \
-      v = (vector signed char)vec_perm (vivP[0], vivP[1], align_perm);	   \
-									   \
-      u  = (vector signed char)						   \
-     		vec_sub (u,(vector signed char)                            \
-				vec_splat((vector signed char)AVV(128),0));\
-      v  = (vector signed char)						   \
-     		vec_sub (v,(vector signed char)				   \
-				vec_splat((vector signed char)AVV(128),0));\
-									   \
-      U  = vec_unpackh (u);						   \
-      V  = vec_unpackh (v);						   \
-									   \
-									   \
-	Y0 = vec_unh (y0);						   \
-	Y1 = vec_unl (y0);						   \
-	Y2 = vec_unh (y1);						   \
-	Y3 = vec_unl (y1);						   \
-									   \
-        Y0 = vec_mradds (Y0, lCY, lOY);					   \
-        Y1 = vec_mradds (Y1, lCY, lOY);					   \
-        Y2 = vec_mradds (Y2, lCY, lOY);					   \
-        Y3 = vec_mradds (Y3, lCY, lOY);					   \
-									   \
-	/*   ux  = (CBU*(u<<CSHIFT)+0x4000)>>15 */			   \
-	ux = vec_sl (U, lCSHIFT);					   \
-	ux = vec_mradds (ux, lCBU, (vector signed short)AVV(0));		   \
-	ux0  = vec_mergeh (ux,ux);					   \
-	ux1  = vec_mergel (ux,ux);					   \
-									   \
-	/* vx  = (CRV*(v<<CSHIFT)+0x4000)>>15;	*/			   \
-	vx = vec_sl (V, lCSHIFT);					   \
-	vx = vec_mradds (vx, lCRV, (vector signed short)AVV(0));		   \
-	vx0  = vec_mergeh (vx,vx);					   \
-	vx1  = vec_mergel (vx,vx);					   \
-									   \
-	/* uvx = ((CGU*u) + (CGV*v))>>15 */				   \
-	uvx = vec_mradds (U, lCGU, (vector signed short)AVV(0));		   \
-	uvx = vec_mradds (V, lCGV, uvx);				   \
-	uvx0 = vec_mergeh (uvx,uvx);					   \
-	uvx1 = vec_mergel (uvx,uvx);					   \
-									   \
-	R0 = vec_add (Y0,vx0);						   \
-	G0 = vec_add (Y0,uvx0);						   \
-	B0 = vec_add (Y0,ux0);						   \
-	R1 = vec_add (Y1,vx1);						   \
-	G1 = vec_add (Y1,uvx1);						   \
-	B1 = vec_add (Y1,ux1);						   \
-									   \
-	R  = vec_packclp (R0,R1);					   \
-	G  = vec_packclp (G0,G1);					   \
-	B  = vec_packclp (B0,B1);					   \
-									   \
-	out_pixels(R,G,B,oute);						   \
-									   \
-	R0 = vec_add (Y2,vx0);						   \
-	G0 = vec_add (Y2,uvx0);						   \
-	B0 = vec_add (Y2,ux0);						   \
-	R1 = vec_add (Y3,vx1);						   \
-	G1 = vec_add (Y3,uvx1);						   \
-	B1 = vec_add (Y3,ux1);						   \
-	R  = vec_packclp (R0,R1);					   \
-	G  = vec_packclp (G0,G1);					   \
-	B  = vec_packclp (B0,B1);					   \
-									   \
-									   \
-	out_pixels(R,G,B,outo);						   \
-									   \
-      y1i  += 16;							   \
-      y2i  += 16;							   \
-      ui   += 8;							   \
-      vi   += 8;							   \
-									   \
-    }									   \
-									   \
-    outo += (outstrides[0])>>4;					           \
-    oute += (outstrides[0])>>4;					           \
-									   \
-    ui    += instrides_scl[1];						   \
-    vi    += instrides_scl[2];						   \
-    y1i   += instrides_scl[0];						   \
-    y2i   += instrides_scl[0];						   \
-  }									   \
-  return srcSliceH;							   \
-}
-
-
-#define out_abgr(a,b,c,ptr)  vec_mstrgb32(typeof(a),((typeof (a))AVV(0)),c,b,a,ptr)
-#define out_bgra(a,b,c,ptr)  vec_mstrgb32(typeof(a),c,b,a,((typeof (a))AVV(0)),ptr)
-#define out_rgba(a,b,c,ptr)  vec_mstrgb32(typeof(a),a,b,c,((typeof (a))AVV(0)),ptr)
-#define out_argb(a,b,c,ptr)  vec_mstrgb32(typeof(a),((typeof (a))AVV(0)),a,b,c,ptr)
-#define out_rgb24(a,b,c,ptr) vec_mstrgb24(a,b,c,ptr)
-#define out_bgr24(a,b,c,ptr) vec_mstbgr24(a,b,c,ptr)
-
-DEFCSP420_CVT (yuv2_abgr, out_abgr)
-#if 1
-DEFCSP420_CVT (yuv2_bgra, out_bgra)
-#else
-static int altivec_yuv2_bgra32 (SwsContext *c,                                  
-				unsigned char **in, int *instrides,	   
-				int srcSliceY,	int srcSliceH,		   
-				unsigned char **oplanes, int *outstrides)  
-{									   
-  int w = c->srcW;							   
-  int h = srcSliceH;							   
-  int i,j;								   
-  int instrides_scl[3];							   
-  vector unsigned char y0,y1;						   
-									   
-  vector signed char  u,v;						   
-									   
-  vector signed short Y0,Y1,Y2,Y3;					   
-  vector signed short U,V;						   
-  vector signed short vx,ux,uvx;					   
-  vector signed short vx0,ux0,uvx0;					   
-  vector signed short vx1,ux1,uvx1;					   
-  vector signed short R0,G0,B0;						   
-  vector signed short R1,G1,B1;						   
-  vector unsigned char R,G,B;						   
-									   
-  vector unsigned char *uivP, *vivP;			   		   
-  vector unsigned char align_perm;					   
-									   
-  vector signed short 							   
-    lCY  = c->CY,							   
-    lOY  = c->OY,							   
-    lCRV = c->CRV,							   
-    lCBU = c->CBU,							   
-    lCGU = c->CGU,							   
-    lCGV = c->CGV;							   
-									   
-  vector unsigned short lCSHIFT = c->CSHIFT;				   
-									   
-  ubyte *y1i   = in[0];							   
-  ubyte *y2i   = in[0]+w;						   
-  ubyte *ui    = in[1];							   
-  ubyte *vi    = in[2];							   
-									   
-  vector unsigned char *oute						   
-    = (vector unsigned char *)						   
-        (oplanes[0]+srcSliceY*outstrides[0]);				   
-  vector unsigned char *outo						   
-    = (vector unsigned char *)						   
-        (oplanes[0]+srcSliceY*outstrides[0]+outstrides[0]);		   
-									   
-									   
-  instrides_scl[0] = instrides[0];					   
-  instrides_scl[1] = instrides[1]-w/2;  /* the loop moves ui by w/2 */	   
-  instrides_scl[2] = instrides[2]-w/2;  /* the loop moves vi by w/2 */	   
-									   
-									   
-  for (i=0;i<h/2;i++) {							   
-    vec_dstst (outo, (0x02000002|(((w*3+32)/32)<<16)), 0);                 
-    vec_dstst (oute, (0x02000002|(((w*3+32)/32)<<16)), 1);                 
-									   
-    for (j=0;j<w/16;j++) {						   
-									   
-      y0 = vec_ldl (0,y1i);						   
-      y1 = vec_ldl (0,y2i);						   
-      uivP = (vector unsigned char *)ui;				   
-      vivP = (vector unsigned char *)vi;				   
-									   
-      align_perm = vec_lvsl (0, ui);					   
-      u = (vector signed char)vec_perm (uivP[0], uivP[1], align_perm);	   
-									   
-      align_perm = vec_lvsl (0, vi);					   
-      v = (vector signed char)vec_perm (vivP[0], vivP[1], align_perm);
-      u  = (vector signed char)
-     		vec_sub (u,(vector signed char)
-				vec_splat((vector signed char)AVV(128),0));
-      
-      v  = (vector signed char)
-      		vec_sub (v, (vector signed char)
-				vec_splat((vector signed char)AVV(128),0));
-      
-      U  = vec_unpackh (u);						   
-      V  = vec_unpackh (v);						   
-									   
-									   
-	Y0 = vec_unh (y0);						   
-	Y1 = vec_unl (y0);						   
-	Y2 = vec_unh (y1);						   
-	Y3 = vec_unl (y1);						   
-									   
-        Y0 = vec_mradds (Y0, lCY, lOY);					   
-        Y1 = vec_mradds (Y1, lCY, lOY);					   
-        Y2 = vec_mradds (Y2, lCY, lOY);					   
-        Y3 = vec_mradds (Y3, lCY, lOY);					   
-									   
-	/*   ux  = (CBU*(u<<CSHIFT)+0x4000)>>15 */			   
-	ux = vec_sl (U, lCSHIFT);					   
-	ux = vec_mradds (ux, lCBU, (vector signed short)AVV(0));
-	ux0  = vec_mergeh (ux,ux);					   
-	ux1  = vec_mergel (ux,ux);					   
-									   
-	/* vx  = (CRV*(v<<CSHIFT)+0x4000)>>15;	*/			   
-	vx = vec_sl (V, lCSHIFT);					   
-	vx = vec_mradds (vx, lCRV, (vector signed short)AVV(0));
-	vx0  = vec_mergeh (vx,vx);
-	vx1  = vec_mergel (vx,vx);
-	/* uvx = ((CGU*u) + (CGV*v))>>15 */
-	uvx = vec_mradds (U, lCGU, (vector signed short)AVV(0));
-	uvx = vec_mradds (V, lCGV, uvx);
-	uvx0 = vec_mergeh (uvx,uvx);
-	uvx1 = vec_mergel (uvx,uvx);
-	R0 = vec_add (Y0,vx0);
-	G0 = vec_add (Y0,uvx0);
-	B0 = vec_add (Y0,ux0);
-	R1 = vec_add (Y1,vx1);
-	G1 = vec_add (Y1,uvx1);
-	B1 = vec_add (Y1,ux1);
-	R  = vec_packclp (R0,R1);
-	G  = vec_packclp (G0,G1);
-	B  = vec_packclp (B0,B1);
-	
-	out_argb(R,G,B,oute);
-	R0 = vec_add (Y2,vx0);
-	G0 = vec_add (Y2,uvx0);
-	B0 = vec_add (Y2,ux0);
-	R1 = vec_add (Y3,vx1);
-	G1 = vec_add (Y3,uvx1);
-	B1 = vec_add (Y3,ux1);
-	R  = vec_packclp (R0,R1);
-	G  = vec_packclp (G0,G1);
-	B  = vec_packclp (B0,B1);
-	
-	out_argb(R,G,B,outo);
-	y1i  += 16;							   
-	y2i  += 16;							   
-	ui   += 8;
-	vi   += 8;							   
-									   
-    }									   
-									   
-    outo += (outstrides[0])>>4;					           
-    oute += (outstrides[0])>>4;					           
-									   
-    ui    += instrides_scl[1];						   
-    vi    += instrides_scl[2];						   
-    y1i   += instrides_scl[0];						   
-    y2i   += instrides_scl[0];						   
-  }									   
-  return srcSliceH;							   
-}
-
-#endif
-
-
-DEFCSP420_CVT (yuv2_rgba, out_rgba)
-DEFCSP420_CVT (yuv2_argb, out_argb)
-DEFCSP420_CVT (yuv2_rgb24,  out_rgb24)
-DEFCSP420_CVT (yuv2_bgr24,  out_bgr24)
-
-
-// uyvy|uyvy|uyvy|uyvy
-// 0123 4567 89ab cdef
-static
-const vector unsigned char
-  demux_u = (const vector unsigned char)AVV(0x10,0x00,0x10,0x00,
-				   0x10,0x04,0x10,0x04,
-				   0x10,0x08,0x10,0x08,
-				   0x10,0x0c,0x10,0x0c),
-  demux_v = (const vector unsigned char)AVV(0x10,0x02,0x10,0x02,
-				   0x10,0x06,0x10,0x06,
-				   0x10,0x0A,0x10,0x0A,
-				   0x10,0x0E,0x10,0x0E),
-  demux_y = (const vector unsigned char)AVV(0x10,0x01,0x10,0x03,
-				   0x10,0x05,0x10,0x07,
-				   0x10,0x09,0x10,0x0B,
-				   0x10,0x0D,0x10,0x0F);
-
-/*
-  this is so I can play live CCIR raw video
-*/
-static int altivec_uyvy_rgb32 (SwsContext *c,
-			       unsigned char **in, int *instrides,
-			       int srcSliceY,	int srcSliceH,
-			       unsigned char **oplanes, int *outstrides)
-{
-  int w = c->srcW;
-  int h = srcSliceH;
-  int i,j;
-  vector unsigned char uyvy;
-  vector signed   short Y,U,V;
-  vector signed   short R0,G0,B0,R1,G1,B1;
-  vector unsigned char  R,G,B;
-  vector unsigned char *out;
-  ubyte *img;
-
-  img = in[0];
-  out = (vector unsigned char *)(oplanes[0]+srcSliceY*outstrides[0]);
-
-  for (i=0;i<h;i++) {
-    for (j=0;j<w/16;j++) {
-      uyvy = vec_ld (0, img);
-      U = (vector signed short)
-	vec_perm (uyvy, (vector unsigned char)AVV(0), demux_u);
-
-      V = (vector signed short)
-	vec_perm (uyvy, (vector unsigned char)AVV(0), demux_v);
-
-      Y = (vector signed short)
-	vec_perm (uyvy, (vector unsigned char)AVV(0), demux_y);
-
-      cvtyuvtoRGB (c, Y,U,V,&R0,&G0,&B0);
-
-      uyvy = vec_ld (16, img);
-      U = (vector signed short)
-	vec_perm (uyvy, (vector unsigned char)AVV(0), demux_u);
-
-      V = (vector signed short)
-	vec_perm (uyvy, (vector unsigned char)AVV(0), demux_v);
-
-      Y = (vector signed short)
-	vec_perm (uyvy, (vector unsigned char)AVV(0), demux_y);
-
-      cvtyuvtoRGB (c, Y,U,V,&R1,&G1,&B1);
-
-      R  = vec_packclp (R0,R1);
-      G  = vec_packclp (G0,G1);
-      B  = vec_packclp (B0,B1);
-
-      //      vec_mstbgr24 (R,G,B, out);
-      out_rgba (R,G,B,out);
-
-      img += 32;
-    }
-  }
-  return srcSliceH;
-}
-
-
-
-/* Ok currently the acceleration routine only supports
-   inputs of widths a multiple of 16
-   and heights a multiple 2
-
-   So we just fall back to the C codes for this.
-*/
-SwsFunc yuv2rgb_init_altivec (SwsContext *c)
-{
-  if (!(c->flags & SWS_CPU_CAPS_ALTIVEC))    
-    return NULL;
-
-  /*
-    and this seems not to matter too much I tried a bunch of 
-    videos with abnormal widths and mplayer crashes else where.
-    mplayer -vo x11 -rawvideo on:w=350:h=240 raw-350x240.eyuv 
-    boom with X11 bad match.
-    
-  */
-  if ((c->srcW & 0xf) != 0)    return NULL;
-
-  switch (c->srcFormat) {
-  case PIX_FMT_YUV410P:
-  case PIX_FMT_YUV420P:
-  /*case IMGFMT_CLPL:	??? */
-  case PIX_FMT_GRAY8:
-  case PIX_FMT_NV12:
-  case PIX_FMT_NV21:
-    if ((c->srcH & 0x1) != 0)
-      return NULL;
-
-    switch(c->dstFormat){
-    case PIX_FMT_RGB24:
-      MSG_WARN("ALTIVEC: Color Space RGB24\n");
-      return altivec_yuv2_rgb24;
-    case PIX_FMT_BGR24:
-      MSG_WARN("ALTIVEC: Color Space BGR24\n");
-      return altivec_yuv2_bgr24;
-    case PIX_FMT_ARGB:
-      MSG_WARN("ALTIVEC: Color Space ARGB\n");
-      return altivec_yuv2_argb;
-    case PIX_FMT_ABGR:
-      MSG_WARN("ALTIVEC: Color Space ABGR\n");
-      return altivec_yuv2_abgr;
-    case PIX_FMT_RGBA:
-      MSG_WARN("ALTIVEC: Color Space RGBA\n");
-      return altivec_yuv2_rgba;
-    case PIX_FMT_BGRA:
-      MSG_WARN("ALTIVEC: Color Space BGRA\n");
-      return altivec_yuv2_bgra;
-    default: return NULL;
-    }
-    break;
-
-  case PIX_FMT_UYVY422:
-    switch(c->dstFormat){
-    case PIX_FMT_BGR32:
-      MSG_WARN("ALTIVEC: Color Space UYVY -> RGB32\n");
-      return altivec_uyvy_rgb32;
-    default: return NULL;
-    }
-    break;
-
-  }
-  return NULL;
-}
-
-static uint16_t roundToInt16(int64_t f){
-	int r= (f + (1<<15))>>16;
-	     if(r<-0x7FFF) return 0x8000;
-	else if(r> 0x7FFF) return 0x7FFF;
-	else               return r;
-}
-
-void yuv2rgb_altivec_init_tables (SwsContext *c, const int inv_table[4],int brightness,int contrast, int saturation)
-{
-  union {
-  	signed short tmp[8] __attribute__ ((aligned(16)));
-	vector signed short vec;
-	} buf;
-
-  buf.tmp[0] =  ( (0xffffLL) * contrast>>8 )>>9;			//cy
-  buf.tmp[1] =  -256*brightness;					//oy
-  buf.tmp[2] =  (inv_table[0]>>3) *(contrast>>16)*(saturation>>16);	//crv
-  buf.tmp[3] =  (inv_table[1]>>3) *(contrast>>16)*(saturation>>16);	//cbu
-  buf.tmp[4] = -((inv_table[2]>>1)*(contrast>>16)*(saturation>>16));	//cgu
-  buf.tmp[5] = -((inv_table[3]>>1)*(contrast>>16)*(saturation>>16));	//cgv
-
-
-  c->CSHIFT = (vector unsigned short)vec_splat_u16(2);
-  c->CY  = vec_splat ((vector signed short)buf.vec, 0);
-  c->OY  = vec_splat ((vector signed short)buf.vec, 1);
-  c->CRV  = vec_splat ((vector signed short)buf.vec, 2);
-  c->CBU  = vec_splat ((vector signed short)buf.vec, 3);
-  c->CGU  = vec_splat ((vector signed short)buf.vec, 4);
-  c->CGV  = vec_splat ((vector signed short)buf.vec, 5);
-#if 0
-{
-int i;
-char *v[6]={"cy","oy","crv","cbu","cgu","cgv"};
-for (i=0; i<6;i++)
-  printf("%s %d ", v[i],buf.tmp[i] );
-  printf("\n");
-}
-#endif
- return;
-}
-
-
-void
-altivec_yuv2packedX (SwsContext *c,
-		       int16_t *lumFilter, int16_t **lumSrc, int lumFilterSize,
-		       int16_t *chrFilter, int16_t **chrSrc, int chrFilterSize,
-		       uint8_t *dest, int dstW, int dstY)
-{
-  int i,j;
-  vector signed short X,X0,X1,Y0,U0,V0,Y1,U1,V1,U,V;
-  vector signed short R0,G0,B0,R1,G1,B1;
-
-  vector unsigned char R,G,B;
-  vector unsigned char *out,*nout;
-
-  vector signed short   RND = vec_splat_s16(1<<3);
-  vector unsigned short SCL = vec_splat_u16(4);
-  unsigned long scratch[16] __attribute__ ((aligned (16)));
-
-  vector signed short *YCoeffs, *CCoeffs;
-
-  YCoeffs = c->vYCoeffsBank+dstY*lumFilterSize;
-  CCoeffs = c->vCCoeffsBank+dstY*chrFilterSize;
-
-  out = (vector unsigned char *)dest;
-
-  for(i=0; i<dstW; i+=16){
-    Y0 = RND;
-    Y1 = RND;
-    /* extract 16 coeffs from lumSrc */
-    for(j=0; j<lumFilterSize; j++) {
-      X0 = vec_ld (0,  &lumSrc[j][i]);
-      X1 = vec_ld (16, &lumSrc[j][i]);
-      Y0 = vec_mradds (X0, YCoeffs[j], Y0);
-      Y1 = vec_mradds (X1, YCoeffs[j], Y1);
-    }
-
-    U = RND;
-    V = RND;
-    /* extract 8 coeffs from U,V */
-    for(j=0; j<chrFilterSize; j++) {
-      X  = vec_ld (0, &chrSrc[j][i/2]);
-      U  = vec_mradds (X, CCoeffs[j], U);
-      X  = vec_ld (0, &chrSrc[j][i/2+2048]);
-      V  = vec_mradds (X, CCoeffs[j], V);
-    }
-
-    /* scale and clip signals */
-    Y0 = vec_sra (Y0, SCL);
-    Y1 = vec_sra (Y1, SCL);
-    U  = vec_sra (U,  SCL);
-    V  = vec_sra (V,  SCL);
-
-    Y0 = vec_clip_s16 (Y0);
-    Y1 = vec_clip_s16 (Y1);
-    U  = vec_clip_s16 (U);
-    V  = vec_clip_s16 (V);
-
-    /* now we have
-      Y0= y0 y1 y2 y3 y4 y5 y6 y7     Y1= y8 y9 y10 y11 y12 y13 y14 y15
-      U= u0 u1 u2 u3 u4 u5 u6 u7      V= v0 v1 v2 v3 v4 v5 v6 v7
-
-      Y0= y0 y1 y2 y3 y4 y5 y6 y7    Y1= y8 y9 y10 y11 y12 y13 y14 y15
-      U0= u0 u0 u1 u1 u2 u2 u3 u3    U1= u4 u4 u5 u5 u6 u6 u7 u7
-      V0= v0 v0 v1 v1 v2 v2 v3 v3    V1= v4 v4 v5 v5 v6 v6 v7 v7
-    */
-
-    U0 = vec_mergeh (U,U);
-    V0 = vec_mergeh (V,V);
-
-    U1 = vec_mergel (U,U);
-    V1 = vec_mergel (V,V);
-
-    cvtyuvtoRGB (c, Y0,U0,V0,&R0,&G0,&B0);
-    cvtyuvtoRGB (c, Y1,U1,V1,&R1,&G1,&B1);
-
-    R  = vec_packclp (R0,R1);
-    G  = vec_packclp (G0,G1);
-    B  = vec_packclp (B0,B1);
-
-    switch(c->dstFormat) {
-      case PIX_FMT_ABGR: out_abgr (R,G,B,out); break;
-      case PIX_FMT_BGRA: out_bgra (R,G,B,out); break;
-      case PIX_FMT_RGBA: out_rgba (R,G,B,out); break;
-      case PIX_FMT_ARGB: out_argb (R,G,B,out); break;
-      case PIX_FMT_RGB24: out_rgb24 (R,G,B,out); break;
-      case PIX_FMT_BGR24: out_bgr24 (R,G,B,out); break;
-      default:
-        {
-          /* If this is reached, the caller should have called yuv2packedXinC
-             instead. */
-          static int printed_error_message;
-          if(!printed_error_message) {
-            MSG_ERR("altivec_yuv2packedX doesn't support %s output\n",
-                    sws_format_name(c->dstFormat));
-            printed_error_message=1;
-          }
-          return;
-        }
-    }
-  }
-
-  if (i < dstW) {
-    i -= 16;
-
-    Y0 = RND;
-    Y1 = RND;
-    /* extract 16 coeffs from lumSrc */
-    for(j=0; j<lumFilterSize; j++) {
-      X0 = vec_ld (0,  &lumSrc[j][i]);
-      X1 = vec_ld (16, &lumSrc[j][i]);
-      Y0 = vec_mradds (X0, YCoeffs[j], Y0);
-      Y1 = vec_mradds (X1, YCoeffs[j], Y1);
-    }
-
-    U = RND;
-    V = RND;
-    /* extract 8 coeffs from U,V */
-    for(j=0; j<chrFilterSize; j++) {
-      X  = vec_ld (0, &chrSrc[j][i/2]);
-      U  = vec_mradds (X, CCoeffs[j], U);
-      X  = vec_ld (0, &chrSrc[j][i/2+2048]);
-      V  = vec_mradds (X, CCoeffs[j], V);
-    }
-
-    /* scale and clip signals */
-    Y0 = vec_sra (Y0, SCL);
-    Y1 = vec_sra (Y1, SCL);
-    U  = vec_sra (U,  SCL);
-    V  = vec_sra (V,  SCL);
-
-    Y0 = vec_clip_s16 (Y0);
-    Y1 = vec_clip_s16 (Y1);
-    U  = vec_clip_s16 (U);
-    V  = vec_clip_s16 (V);
-
-    /* now we have
-       Y0= y0 y1 y2 y3 y4 y5 y6 y7     Y1= y8 y9 y10 y11 y12 y13 y14 y15
-       U= u0 u1 u2 u3 u4 u5 u6 u7      V= v0 v1 v2 v3 v4 v5 v6 v7
-
-       Y0= y0 y1 y2 y3 y4 y5 y6 y7    Y1= y8 y9 y10 y11 y12 y13 y14 y15
-       U0= u0 u0 u1 u1 u2 u2 u3 u3    U1= u4 u4 u5 u5 u6 u6 u7 u7
-       V0= v0 v0 v1 v1 v2 v2 v3 v3    V1= v4 v4 v5 v5 v6 v6 v7 v7
-    */
-
-    U0 = vec_mergeh (U,U);
-    V0 = vec_mergeh (V,V);
-
-    U1 = vec_mergel (U,U);
-    V1 = vec_mergel (V,V);
-
-    cvtyuvtoRGB (c, Y0,U0,V0,&R0,&G0,&B0);
-    cvtyuvtoRGB (c, Y1,U1,V1,&R1,&G1,&B1);
-
-    R  = vec_packclp (R0,R1);
-    G  = vec_packclp (G0,G1);
-    B  = vec_packclp (B0,B1);
-
-    nout = (vector unsigned char *)scratch;
-    switch(c->dstFormat) {
-      case PIX_FMT_ABGR: out_abgr (R,G,B,nout); break;
-      case PIX_FMT_BGRA: out_bgra (R,G,B,nout); break;
-      case PIX_FMT_RGBA: out_rgba (R,G,B,nout); break;
-      case PIX_FMT_ARGB: out_argb (R,G,B,nout); break;
-      case PIX_FMT_RGB24: out_rgb24 (R,G,B,nout); break;
-      case PIX_FMT_BGR24: out_bgr24 (R,G,B,nout); break;
-      default:
-        /* Unreachable, I think. */
-        MSG_ERR("altivec_yuv2packedX doesn't support %s output\n",
-                sws_format_name(c->dstFormat));
-        return;
-    }
-
-    memcpy (&((uint32_t*)dest)[i], scratch, (dstW-i)/4);
-  }
-
-}
-#endif //MEANX
+/*
+  marc.hoffman at analog.com    March 8, 2004
+
+  Altivec Acceleration for Color Space Conversion revision 0.2
+
+  convert I420 YV12 to RGB in various formats,
+    it rejects images that are not in 420 formats
+    it rejects images that don't have widths of multiples of 16
+    it rejects images that don't have heights of multiples of 2
+  reject defers to C simulation codes.
+
+  lots of optimizations to be done here
+
+  1. need to fix saturation code, I just couldn't get it to fly with packs and adds.
+     so we currently use max min to clip
+
+  2. the inefficient use of chroma loading needs a bit of brushing up
+
+  3. analysis of pipeline stalls needs to be done, use shark to identify pipeline stalls
+
+
+  MODIFIED to calculate coeffs from currently selected color space.
+  MODIFIED core to be a macro which you spec the output format.
+  ADDED UYVY conversion which is never called due to some thing in SWSCALE.
+  CORRECTED algorithim selection to be strict on input formats.
+  ADDED runtime detection of altivec.
+
+  ADDED altivec_yuv2packedX vertical scl + RGB converter
+
+  March 27,2004
+  PERFORMANCE ANALYSIS
+
+  The C version use 25% of the processor or ~250Mips for D1 video rawvideo used as test
+  The ALTIVEC version uses 10% of the processor or ~100Mips for D1 video same sequence
+
+  720*480*30  ~10MPS
+
+  so we have roughly 10clocks per pixel this is too high something has to be wrong.
+
+  OPTIMIZED clip codes to utilize vec_max and vec_packs removing the need for vec_min.
+
+  OPTIMIZED DST OUTPUT cache/dma controls. we are pretty much
+  guaranteed to have the input video frame it was just decompressed so
+  it probably resides in L1 caches.  However we are creating the
+  output video stream this needs to use the DSTST instruction to
+  optimize for the cache.  We couple this with the fact that we are
+  not going to be visiting the input buffer again so we mark it Least
+  Recently Used.  This shaves 25% of the processor cycles off.
+
+  Now MEMCPY is the largest mips consumer in the system, probably due
+  to the inefficient X11 stuff.
+
+  GL libraries seem to be very slow on this machine 1.33Ghz PB running
+  Jaguar, this is not the case for my 1Ghz PB.  I thought it might be
+  a versioning issues, however i have libGL.1.2.dylib for both
+  machines. ((We need to figure this out now))
+
+  GL2 libraries work now with patch for RGB32
+
+  NOTE quartz vo driver ARGB32_to_RGB24 consumes 30% of the processor
+
+  Integrated luma prescaling adjustment for saturation/contrast/brightness adjustment.
+*/
+
+/*
+ * This file is part of FFmpeg.
+ *
+ * FFmpeg is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License as published by
+ * the Free Software Foundation; either version 2 of the License, or
+ * (at your option) any later version.
+ *
+ * FFmpeg is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ * GNU General Public License for more details.
+ *
+ * You should have received a copy of the GNU General Public License
+ * along with FFmpeg; if not, write to the Free Software
+ * Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA
+ */
+#include "config.h"
+#ifdef USE_ALTIVEC
+#include <stdio.h>
+#include <stdlib.h>
+#include <string.h>
+#include <inttypes.h>
+#include <assert.h>
+#include "config.h"
+#ifdef HAVE_MALLOC_H
+#include <malloc.h>
+#endif
+#include "rgb2rgb.h"
+#include "swscale.h"
+#include "swscale_internal.h"
+
+#undef PROFILE_THE_BEAST
+#undef INC_SCALING
+
+typedef unsigned char ubyte;
+typedef signed char   sbyte;
+
+
+/* RGB interleaver, 16 planar pels 8-bit samples per channel in
+   homogeneous vector registers x0,x1,x2 are interleaved with the
+   following technique:
+
+      o0 = vec_mergeh (x0,x1);
+      o1 = vec_perm (o0, x2, perm_rgb_0);
+      o2 = vec_perm (o0, x2, perm_rgb_1);
+      o3 = vec_mergel (x0,x1);
+      o4 = vec_perm (o3,o2,perm_rgb_2);
+      o5 = vec_perm (o3,o2,perm_rgb_3);
+
+  perm_rgb_0:   o0(RG).h v1(B) --> o1*
+              0   1  2   3   4
+             rgbr|gbrg|brgb|rgbr
+             0010 0100 1001 0010
+             0102 3145 2673 894A
+
+  perm_rgb_1:   o0(RG).h v1(B) --> o2
+              0   1  2   3   4
+             gbrg|brgb|bbbb|bbbb
+             0100 1001 1111 1111
+             B5CD 6EF7 89AB CDEF
+
+  perm_rgb_2:   o3(RG).l o2(rgbB.l) --> o4*
+              0   1  2   3   4
+             gbrg|brgb|rgbr|gbrg
+             1111 1111 0010 0100
+             89AB CDEF 0182 3945
+
+  perm_rgb_2:   o3(RG).l o2(rgbB.l) ---> o5*
+              0   1  2   3   4
+             brgb|rgbr|gbrg|brgb
+             1001 0010 0100 1001
+             a67b 89cA BdCD eEFf
+
+*/
+static
+const vector unsigned char
+  perm_rgb_0 = (const vector unsigned char)AVV(0x00,0x01,0x10,0x02,0x03,0x11,0x04,0x05,
+                                               0x12,0x06,0x07,0x13,0x08,0x09,0x14,0x0a),
+  perm_rgb_1 = (const vector unsigned char)AVV(0x0b,0x15,0x0c,0x0d,0x16,0x0e,0x0f,0x17,
+                                               0x18,0x19,0x1a,0x1b,0x1c,0x1d,0x1e,0x1f),
+  perm_rgb_2 = (const vector unsigned char)AVV(0x10,0x11,0x12,0x13,0x14,0x15,0x16,0x17,
+                                               0x00,0x01,0x18,0x02,0x03,0x19,0x04,0x05),
+  perm_rgb_3 = (const vector unsigned char)AVV(0x1a,0x06,0x07,0x1b,0x08,0x09,0x1c,0x0a,
+                                               0x0b,0x1d,0x0c,0x0d,0x1e,0x0e,0x0f,0x1f);
+
+#define vec_merge3(x2,x1,x0,y0,y1,y2)       \
+do {                                        \
+    typeof(x0) o0,o2,o3;                    \
+        o0 = vec_mergeh (x0,x1);            \
+        y0 = vec_perm (o0, x2, perm_rgb_0); \
+        o2 = vec_perm (o0, x2, perm_rgb_1); \
+        o3 = vec_mergel (x0,x1);            \
+        y1 = vec_perm (o3,o2,perm_rgb_2);   \
+        y2 = vec_perm (o3,o2,perm_rgb_3);   \
+} while(0)
+
+#define vec_mstbgr24(x0,x1,x2,ptr)      \
+do {                                    \
+    typeof(x0) _0,_1,_2;                \
+    vec_merge3 (x0,x1,x2,_0,_1,_2);     \
+    vec_st (_0, 0, ptr++);              \
+    vec_st (_1, 0, ptr++);              \
+    vec_st (_2, 0, ptr++);              \
+}  while (0);
+
+#define vec_mstrgb24(x0,x1,x2,ptr)      \
+do {                                    \
+    typeof(x0) _0,_1,_2;                \
+    vec_merge3 (x2,x1,x0,_0,_1,_2);     \
+    vec_st (_0, 0, ptr++);              \
+    vec_st (_1, 0, ptr++);              \
+    vec_st (_2, 0, ptr++);              \
+}  while (0);
+
+/* pack the pixels in rgb0 format
+   msb R
+   lsb 0
+*/
+#define vec_mstrgb32(T,x0,x1,x2,x3,ptr)                                       \
+do {                                                                          \
+    T _0,_1,_2,_3;                                                            \
+    _0 = vec_mergeh (x0,x1);                                                  \
+    _1 = vec_mergeh (x2,x3);                                                  \
+    _2 = (T)vec_mergeh ((vector unsigned short)_0,(vector unsigned short)_1); \
+    _3 = (T)vec_mergel ((vector unsigned short)_0,(vector unsigned short)_1); \
+    vec_st (_2, 0*16, (T *)ptr);                                              \
+    vec_st (_3, 1*16, (T *)ptr);                                              \
+    _0 = vec_mergel (x0,x1);                                                  \
+    _1 = vec_mergel (x2,x3);                                                  \
+    _2 = (T)vec_mergeh ((vector unsigned short)_0,(vector unsigned short)_1); \
+    _3 = (T)vec_mergel ((vector unsigned short)_0,(vector unsigned short)_1); \
+    vec_st (_2, 2*16, (T *)ptr);                                              \
+    vec_st (_3, 3*16, (T *)ptr);                                              \
+    ptr += 4;                                                                 \
+}  while (0);
+
+/*
+
+  | 1     0       1.4021   | | Y |
+  | 1    -0.3441 -0.7142   |x| Cb|
+  | 1     1.7718  0        | | Cr|
+
+
+  Y:      [-128 127]
+  Cb/Cr : [-128 127]
+
+  typical yuv conversion work on Y: 0-255 this version has been optimized for jpeg decode.
+
+*/
+
+
+
+
+#define vec_unh(x) \
+    (vector signed short) \
+        vec_perm(x,(typeof(x))AVV(0),\
+                 (vector unsigned char)AVV(0x10,0x00,0x10,0x01,0x10,0x02,0x10,0x03,\
+                                           0x10,0x04,0x10,0x05,0x10,0x06,0x10,0x07))
+#define vec_unl(x) \
+    (vector signed short) \
+        vec_perm(x,(typeof(x))AVV(0),\
+                 (vector unsigned char)AVV(0x10,0x08,0x10,0x09,0x10,0x0A,0x10,0x0B,\
+                                           0x10,0x0C,0x10,0x0D,0x10,0x0E,0x10,0x0F))
+
+#define vec_clip_s16(x) \
+    vec_max (vec_min (x, (vector signed short)AVV(235,235,235,235,235,235,235,235)),\
+                         (vector signed short)AVV( 16, 16, 16, 16, 16, 16, 16, 16))
+
+#define vec_packclp(x,y) \
+    (vector unsigned char)vec_packs \
+        ((vector unsigned short)vec_max (x,(vector signed short) AVV(0)), \
+         (vector unsigned short)vec_max (y,(vector signed short) AVV(0)))
+
+//#define out_pixels(a,b,c,ptr) vec_mstrgb32(typeof(a),((typeof (a))AVV(0)),a,a,a,ptr)
+
+
+static inline void cvtyuvtoRGB (SwsContext *c,
+                                vector signed short Y, vector signed short U, vector signed short V,
+                                vector signed short *R, vector signed short *G, vector signed short *B)
+{
+    vector signed   short vx,ux,uvx;
+
+    Y = vec_mradds (Y, c->CY, c->OY);
+    U  = vec_sub (U,(vector signed short)
+                    vec_splat((vector signed short)AVV(128),0));
+    V  = vec_sub (V,(vector signed short)
+                    vec_splat((vector signed short)AVV(128),0));
+
+    //   ux  = (CBU*(u<<c->CSHIFT)+0x4000)>>15;
+    ux = vec_sl (U, c->CSHIFT);
+    *B = vec_mradds (ux, c->CBU, Y);
+
+    // vx  = (CRV*(v<<c->CSHIFT)+0x4000)>>15;
+    vx = vec_sl (V, c->CSHIFT);
+    *R = vec_mradds (vx, c->CRV, Y);
+
+    // uvx = ((CGU*u) + (CGV*v))>>15;
+    uvx = vec_mradds (U, c->CGU, Y);
+    *G  = vec_mradds (V, c->CGV, uvx);
+}
+
+
+/*
+  ------------------------------------------------------------------------------
+  CS converters
+  ------------------------------------------------------------------------------
+*/
+
+
+#define DEFCSP420_CVT(name,out_pixels)                                  \
+static int altivec_##name (SwsContext *c,                               \
+                           unsigned char **in, int *instrides,          \
+                           int srcSliceY,        int srcSliceH,         \
+                           unsigned char **oplanes, int *outstrides)    \
+{                                                                       \
+    int w = c->srcW;                                                    \
+    int h = srcSliceH;                                                  \
+    int i,j;                                                            \
+    int instrides_scl[3];                                               \
+    vector unsigned char y0,y1;                                         \
+                                                                        \
+    vector signed char  u,v;                                            \
+                                                                        \
+    vector signed short Y0,Y1,Y2,Y3;                                    \
+    vector signed short U,V;                                            \
+    vector signed short vx,ux,uvx;                                      \
+    vector signed short vx0,ux0,uvx0;                                   \
+    vector signed short vx1,ux1,uvx1;                                   \
+    vector signed short R0,G0,B0;                                       \
+    vector signed short R1,G1,B1;                                       \
+    vector unsigned char R,G,B;                                         \
+                                                                        \
+    vector unsigned char *y1ivP, *y2ivP, *uivP, *vivP;                  \
+    vector unsigned char align_perm;                                    \
+                                                                        \
+    vector signed short                                                 \
+        lCY  = c->CY,                                                   \
+        lOY  = c->OY,                                                   \
+        lCRV = c->CRV,                                                  \
+        lCBU = c->CBU,                                                  \
+        lCGU = c->CGU,                                                  \
+        lCGV = c->CGV;                                                  \
+                                                                        \
+    vector unsigned short lCSHIFT = c->CSHIFT;                          \
+                                                                        \
+    ubyte *y1i   = in[0];                                               \
+    ubyte *y2i   = in[0]+instrides[0];                                  \
+    ubyte *ui    = in[1];                                               \
+    ubyte *vi    = in[2];                                               \
+                                                                        \
+    vector unsigned char *oute                                          \
+        = (vector unsigned char *)                                      \
+            (oplanes[0]+srcSliceY*outstrides[0]);                       \
+    vector unsigned char *outo                                          \
+        = (vector unsigned char *)                                      \
+            (oplanes[0]+srcSliceY*outstrides[0]+outstrides[0]);         \
+                                                                        \
+                                                                        \
+    instrides_scl[0] = instrides[0]*2-w;  /* the loop moves y{1,2}i by w */ \
+    instrides_scl[1] = instrides[1]-w/2;  /* the loop moves ui by w/2 */    \
+    instrides_scl[2] = instrides[2]-w/2;  /* the loop moves vi by w/2 */    \
+                                                                        \
+                                                                        \
+    for (i=0;i<h/2;i++) {                                               \
+        vec_dstst (outo, (0x02000002|(((w*3+32)/32)<<16)), 0);          \
+        vec_dstst (oute, (0x02000002|(((w*3+32)/32)<<16)), 1);          \
+                                                                        \
+        for (j=0;j<w/16;j++) {                                          \
+                                                                        \
+            y1ivP = (vector unsigned char *)y1i;                        \
+            y2ivP = (vector unsigned char *)y2i;                        \
+            uivP  = (vector unsigned char *)ui;                         \
+            vivP  = (vector unsigned char *)vi;                         \
+                                                                        \
+            align_perm = vec_lvsl (0, y1i);                             \
+            y0 = (vector unsigned char)                                 \
+                 vec_perm (y1ivP[0], y1ivP[1], align_perm);             \
+                                                                        \
+            align_perm = vec_lvsl (0, y2i);                             \
+            y1 = (vector unsigned char)                                 \
+                 vec_perm (y2ivP[0], y2ivP[1], align_perm);             \
+                                                                        \
+            align_perm = vec_lvsl (0, ui);                              \
+            u = (vector signed char)                                    \
+                vec_perm (uivP[0], uivP[1], align_perm);                \
+                                                                        \
+            align_perm = vec_lvsl (0, vi);                              \
+            v = (vector signed char)                                    \
+                vec_perm (vivP[0], vivP[1], align_perm);                \
+                                                                        \
+            u  = (vector signed char)                                   \
+                 vec_sub (u,(vector signed char)                        \
+                          vec_splat((vector signed char)AVV(128),0));   \
+            v  = (vector signed char)                                   \
+                 vec_sub (v,(vector signed char)                        \
+                          vec_splat((vector signed char)AVV(128),0));   \
+                                                                        \
+            U  = vec_unpackh (u);                                       \
+            V  = vec_unpackh (v);                                       \
+                                                                        \
+                                                                        \
+            Y0 = vec_unh (y0);                                          \
+            Y1 = vec_unl (y0);                                          \
+            Y2 = vec_unh (y1);                                          \
+            Y3 = vec_unl (y1);                                          \
+                                                                        \
+            Y0 = vec_mradds (Y0, lCY, lOY);                             \
+            Y1 = vec_mradds (Y1, lCY, lOY);                             \
+            Y2 = vec_mradds (Y2, lCY, lOY);                             \
+            Y3 = vec_mradds (Y3, lCY, lOY);                             \
+                                                                        \
+            /*   ux  = (CBU*(u<<CSHIFT)+0x4000)>>15 */                  \
+            ux = vec_sl (U, lCSHIFT);                                   \
+            ux = vec_mradds (ux, lCBU, (vector signed short)AVV(0));    \
+            ux0  = vec_mergeh (ux,ux);                                  \
+            ux1  = vec_mergel (ux,ux);                                  \
+                                                                        \
+            /* vx  = (CRV*(v<<CSHIFT)+0x4000)>>15;        */            \
+            vx = vec_sl (V, lCSHIFT);                                   \
+            vx = vec_mradds (vx, lCRV, (vector signed short)AVV(0));    \
+            vx0  = vec_mergeh (vx,vx);                                  \
+            vx1  = vec_mergel (vx,vx);                                  \
+                                                                        \
+            /* uvx = ((CGU*u) + (CGV*v))>>15 */                         \
+            uvx = vec_mradds (U, lCGU, (vector signed short)AVV(0));    \
+            uvx = vec_mradds (V, lCGV, uvx);                            \
+            uvx0 = vec_mergeh (uvx,uvx);                                \
+            uvx1 = vec_mergel (uvx,uvx);                                \
+                                                                        \
+            R0 = vec_add (Y0,vx0);                                      \
+            G0 = vec_add (Y0,uvx0);                                     \
+            B0 = vec_add (Y0,ux0);                                      \
+            R1 = vec_add (Y1,vx1);                                      \
+            G1 = vec_add (Y1,uvx1);                                     \
+            B1 = vec_add (Y1,ux1);                                      \
+                                                                        \
+            R  = vec_packclp (R0,R1);                                   \
+            G  = vec_packclp (G0,G1);                                   \
+            B  = vec_packclp (B0,B1);                                   \
+                                                                        \
+            out_pixels(R,G,B,oute);                                     \
+                                                                        \
+            R0 = vec_add (Y2,vx0);                                      \
+            G0 = vec_add (Y2,uvx0);                                     \
+            B0 = vec_add (Y2,ux0);                                      \
+            R1 = vec_add (Y3,vx1);                                      \
+            G1 = vec_add (Y3,uvx1);                                     \
+            B1 = vec_add (Y3,ux1);                                      \
+            R  = vec_packclp (R0,R1);                                   \
+            G  = vec_packclp (G0,G1);                                   \
+            B  = vec_packclp (B0,B1);                                   \
+                                                                        \
+                                                                        \
+            out_pixels(R,G,B,outo);                                     \
+                                                                        \
+            y1i  += 16;                                                 \
+            y2i  += 16;                                                 \
+            ui   += 8;                                                  \
+            vi   += 8;                                                  \
+                                                                        \
+        }                                                               \
+                                                                        \
+        outo  += (outstrides[0])>>4;                                    \
+        oute  += (outstrides[0])>>4;                                    \
+                                                                        \
+        ui    += instrides_scl[1];                                      \
+        vi    += instrides_scl[2];                                      \
+        y1i   += instrides_scl[0];                                      \
+        y2i   += instrides_scl[0];                                      \
+    }                                                                   \
+    return srcSliceH;                                                   \
+}
+
+
+#define out_abgr(a,b,c,ptr)  vec_mstrgb32(typeof(a),((typeof (a))AVV(0)),c,b,a,ptr)
+#define out_bgra(a,b,c,ptr)  vec_mstrgb32(typeof(a),c,b,a,((typeof (a))AVV(0)),ptr)
+#define out_rgba(a,b,c,ptr)  vec_mstrgb32(typeof(a),a,b,c,((typeof (a))AVV(0)),ptr)
+#define out_argb(a,b,c,ptr)  vec_mstrgb32(typeof(a),((typeof (a))AVV(0)),a,b,c,ptr)
+#define out_rgb24(a,b,c,ptr) vec_mstrgb24(a,b,c,ptr)
+#define out_bgr24(a,b,c,ptr) vec_mstbgr24(a,b,c,ptr)
+
+DEFCSP420_CVT (yuv2_abgr, out_abgr)
+#if 1
+DEFCSP420_CVT (yuv2_bgra, out_bgra)
+#else
+static int altivec_yuv2_bgra32 (SwsContext *c,
+                                unsigned char **in, int *instrides,
+                                int srcSliceY,        int srcSliceH,
+                                unsigned char **oplanes, int *outstrides)
+{
+    int w = c->srcW;
+    int h = srcSliceH;
+    int i,j;
+    int instrides_scl[3];
+    vector unsigned char y0,y1;
+
+    vector signed char  u,v;
+
+    vector signed short Y0,Y1,Y2,Y3;
+    vector signed short U,V;
+    vector signed short vx,ux,uvx;
+    vector signed short vx0,ux0,uvx0;
+    vector signed short vx1,ux1,uvx1;
+    vector signed short R0,G0,B0;
+    vector signed short R1,G1,B1;
+    vector unsigned char R,G,B;
+
+    vector unsigned char *uivP, *vivP;
+    vector unsigned char align_perm;
+
+    vector signed short
+        lCY  = c->CY,
+        lOY  = c->OY,
+        lCRV = c->CRV,
+        lCBU = c->CBU,
+        lCGU = c->CGU,
+        lCGV = c->CGV;
+
+    vector unsigned short lCSHIFT = c->CSHIFT;
+
+    ubyte *y1i   = in[0];
+    ubyte *y2i   = in[0]+w;
+    ubyte *ui    = in[1];
+    ubyte *vi    = in[2];
+
+    vector unsigned char *oute
+        = (vector unsigned char *)
+          (oplanes[0]+srcSliceY*outstrides[0]);
+    vector unsigned char *outo
+        = (vector unsigned char *)
+          (oplanes[0]+srcSliceY*outstrides[0]+outstrides[0]);
+
+
+    instrides_scl[0] = instrides[0];
+    instrides_scl[1] = instrides[1]-w/2;  /* the loop moves ui by w/2 */
+    instrides_scl[2] = instrides[2]-w/2;  /* the loop moves vi by w/2 */
+
+
+    for (i=0;i<h/2;i++) {
+        vec_dstst (outo, (0x02000002|(((w*3+32)/32)<<16)), 0);
+        vec_dstst (oute, (0x02000002|(((w*3+32)/32)<<16)), 1);
+
+        for (j=0;j<w/16;j++) {
+
+            y0 = vec_ldl (0,y1i);
+            y1 = vec_ldl (0,y2i);
+            uivP = (vector unsigned char *)ui;
+            vivP = (vector unsigned char *)vi;
+
+            align_perm = vec_lvsl (0, ui);
+            u  = (vector signed char)vec_perm (uivP[0], uivP[1], align_perm);
+
+            align_perm = vec_lvsl (0, vi);
+            v  = (vector signed char)vec_perm (vivP[0], vivP[1], align_perm);
+            u  = (vector signed char)
+                 vec_sub (u,(vector signed char)
+                          vec_splat((vector signed char)AVV(128),0));
+
+            v  = (vector signed char)
+                 vec_sub (v, (vector signed char)
+                          vec_splat((vector signed char)AVV(128),0));
+
+            U  = vec_unpackh (u);
+            V  = vec_unpackh (v);
+
+
+            Y0 = vec_unh (y0);
+            Y1 = vec_unl (y0);
+            Y2 = vec_unh (y1);
+            Y3 = vec_unl (y1);
+
+            Y0 = vec_mradds (Y0, lCY, lOY);
+            Y1 = vec_mradds (Y1, lCY, lOY);
+            Y2 = vec_mradds (Y2, lCY, lOY);
+            Y3 = vec_mradds (Y3, lCY, lOY);
+
+            /*   ux  = (CBU*(u<<CSHIFT)+0x4000)>>15 */
+            ux = vec_sl (U, lCSHIFT);
+            ux = vec_mradds (ux, lCBU, (vector signed short)AVV(0));
+            ux0  = vec_mergeh (ux,ux);
+            ux1  = vec_mergel (ux,ux);
+
+            /* vx  = (CRV*(v<<CSHIFT)+0x4000)>>15;        */
+            vx = vec_sl (V, lCSHIFT);
+            vx = vec_mradds (vx, lCRV, (vector signed short)AVV(0));
+            vx0  = vec_mergeh (vx,vx);
+            vx1  = vec_mergel (vx,vx);
+            /* uvx = ((CGU*u) + (CGV*v))>>15 */
+            uvx = vec_mradds (U, lCGU, (vector signed short)AVV(0));
+            uvx = vec_mradds (V, lCGV, uvx);
+            uvx0 = vec_mergeh (uvx,uvx);
+            uvx1 = vec_mergel (uvx,uvx);
+            R0 = vec_add (Y0,vx0);
+            G0 = vec_add (Y0,uvx0);
+            B0 = vec_add (Y0,ux0);
+            R1 = vec_add (Y1,vx1);
+            G1 = vec_add (Y1,uvx1);
+            B1 = vec_add (Y1,ux1);
+            R  = vec_packclp (R0,R1);
+            G  = vec_packclp (G0,G1);
+            B  = vec_packclp (B0,B1);
+
+            out_argb(R,G,B,oute);
+            R0 = vec_add (Y2,vx0);
+            G0 = vec_add (Y2,uvx0);
+            B0 = vec_add (Y2,ux0);
+            R1 = vec_add (Y3,vx1);
+            G1 = vec_add (Y3,uvx1);
+            B1 = vec_add (Y3,ux1);
+            R  = vec_packclp (R0,R1);
+            G  = vec_packclp (G0,G1);
+            B  = vec_packclp (B0,B1);
+
+            out_argb(R,G,B,outo);
+            y1i  += 16;
+            y2i  += 16;
+            ui   += 8;
+            vi   += 8;
+
+        }
+
+        outo  += (outstrides[0])>>4;
+        oute  += (outstrides[0])>>4;
+
+        ui    += instrides_scl[1];
+        vi    += instrides_scl[2];
+        y1i   += instrides_scl[0];
+        y2i   += instrides_scl[0];
+    }
+    return srcSliceH;
+}
+
+#endif
+
+
+DEFCSP420_CVT (yuv2_rgba, out_rgba)
+DEFCSP420_CVT (yuv2_argb, out_argb)
+DEFCSP420_CVT (yuv2_rgb24,  out_rgb24)
+DEFCSP420_CVT (yuv2_bgr24,  out_bgr24)
+
+
+// uyvy|uyvy|uyvy|uyvy
+// 0123 4567 89ab cdef
+static
+const vector unsigned char
+    demux_u = (const vector unsigned char)AVV(0x10,0x00,0x10,0x00,
+                                              0x10,0x04,0x10,0x04,
+                                              0x10,0x08,0x10,0x08,
+                                              0x10,0x0c,0x10,0x0c),
+    demux_v = (const vector unsigned char)AVV(0x10,0x02,0x10,0x02,
+                                              0x10,0x06,0x10,0x06,
+                                              0x10,0x0A,0x10,0x0A,
+                                              0x10,0x0E,0x10,0x0E),
+    demux_y = (const vector unsigned char)AVV(0x10,0x01,0x10,0x03,
+                                              0x10,0x05,0x10,0x07,
+                                              0x10,0x09,0x10,0x0B,
+                                              0x10,0x0D,0x10,0x0F);
+
+/*
+  this is so I can play live CCIR raw video
+*/
+static int altivec_uyvy_rgb32 (SwsContext *c,
+                               unsigned char **in, int *instrides,
+                               int srcSliceY,        int srcSliceH,
+                               unsigned char **oplanes, int *outstrides)
+{
+    int w = c->srcW;
+    int h = srcSliceH;
+    int i,j;
+    vector unsigned char uyvy;
+    vector signed   short Y,U,V;
+    vector signed   short R0,G0,B0,R1,G1,B1;
+    vector unsigned char  R,G,B;
+    vector unsigned char *out;
+    ubyte *img;
+
+    img = in[0];
+    out = (vector unsigned char *)(oplanes[0]+srcSliceY*outstrides[0]);
+
+    for (i=0;i<h;i++) {
+        for (j=0;j<w/16;j++) {
+            uyvy = vec_ld (0, img);
+            U = (vector signed short)
+                vec_perm (uyvy, (vector unsigned char)AVV(0), demux_u);
+
+            V = (vector signed short)
+                vec_perm (uyvy, (vector unsigned char)AVV(0), demux_v);
+
+            Y = (vector signed short)
+                vec_perm (uyvy, (vector unsigned char)AVV(0), demux_y);
+
+            cvtyuvtoRGB (c, Y,U,V,&R0,&G0,&B0);
+
+            uyvy = vec_ld (16, img);
+            U = (vector signed short)
+                vec_perm (uyvy, (vector unsigned char)AVV(0), demux_u);
+
+            V = (vector signed short)
+                vec_perm (uyvy, (vector unsigned char)AVV(0), demux_v);
+
+            Y = (vector signed short)
+                vec_perm (uyvy, (vector unsigned char)AVV(0), demux_y);
+
+            cvtyuvtoRGB (c, Y,U,V,&R1,&G1,&B1);
+
+            R  = vec_packclp (R0,R1);
+            G  = vec_packclp (G0,G1);
+            B  = vec_packclp (B0,B1);
+
+            //      vec_mstbgr24 (R,G,B, out);
+            out_rgba (R,G,B,out);
+
+            img += 32;
+        }
+    }
+    return srcSliceH;
+}
+
+
+
+/* Ok currently the acceleration routine only supports
+   inputs of widths a multiple of 16
+   and heights a multiple 2
+
+   So we just fall back to the C codes for this.
+*/
+SwsFunc yuv2rgb_init_altivec (SwsContext *c)
+{
+    if (!(c->flags & SWS_CPU_CAPS_ALTIVEC))
+        return NULL;
+
+    /*
+      and this seems not to matter too much I tried a bunch of
+      videos with abnormal widths and mplayer crashes else where.
+      mplayer -vo x11 -rawvideo on:w=350:h=240 raw-350x240.eyuv
+      boom with X11 bad match.
+
+    */
+    if ((c->srcW & 0xf) != 0)    return NULL;
+
+    switch (c->srcFormat) {
+    case PIX_FMT_YUV410P:
+    case PIX_FMT_YUV420P:
+    /*case IMGFMT_CLPL:        ??? */
+    case PIX_FMT_GRAY8:
+    case PIX_FMT_NV12:
+    case PIX_FMT_NV21:
+        if ((c->srcH & 0x1) != 0)
+            return NULL;
+
+        switch(c->dstFormat){
+        case PIX_FMT_RGB24:
+            av_log(c, AV_LOG_WARNING, "ALTIVEC: Color Space RGB24\n");
+            return altivec_yuv2_rgb24;
+        case PIX_FMT_BGR24:
+            av_log(c, AV_LOG_WARNING, "ALTIVEC: Color Space BGR24\n");
+            return altivec_yuv2_bgr24;
+        case PIX_FMT_ARGB:
+            av_log(c, AV_LOG_WARNING, "ALTIVEC: Color Space ARGB\n");
+            return altivec_yuv2_argb;
+        case PIX_FMT_ABGR:
+            av_log(c, AV_LOG_WARNING, "ALTIVEC: Color Space ABGR\n");
+            return altivec_yuv2_abgr;
+        case PIX_FMT_RGBA:
+            av_log(c, AV_LOG_WARNING, "ALTIVEC: Color Space RGBA\n");
+            return altivec_yuv2_rgba;
+        case PIX_FMT_BGRA:
+            av_log(c, AV_LOG_WARNING, "ALTIVEC: Color Space BGRA\n");
+            return altivec_yuv2_bgra;
+        default: return NULL;
+        }
+        break;
+
+    case PIX_FMT_UYVY422:
+        switch(c->dstFormat){
+        case PIX_FMT_BGR32:
+            av_log(c, AV_LOG_WARNING, "ALTIVEC: Color Space UYVY -> RGB32\n");
+            return altivec_uyvy_rgb32;
+        default: return NULL;
+        }
+        break;
+
+    }
+    return NULL;
+}
+
+static uint16_t roundToInt16(int64_t f){
+    int r= (f + (1<<15))>>16;
+         if (r<-0x7FFF) return 0x8000;
+    else if (r> 0x7FFF) return 0x7FFF;
+    else                return r;
+}
+
+void yuv2rgb_altivec_init_tables (SwsContext *c, const int inv_table[4],int brightness,int contrast, int saturation)
+{
+    union {
+        signed short tmp[8] __attribute__ ((aligned(16)));
+        vector signed short vec;
+    } buf;
+
+    buf.tmp[0] =  ( (0xffffLL) * contrast>>8 )>>9;                      //cy
+    buf.tmp[1] =  -256*brightness;                                      //oy
+    buf.tmp[2] =  (inv_table[0]>>3) *(contrast>>16)*(saturation>>16);   //crv
+    buf.tmp[3] =  (inv_table[1]>>3) *(contrast>>16)*(saturation>>16);   //cbu
+    buf.tmp[4] = -((inv_table[2]>>1)*(contrast>>16)*(saturation>>16));  //cgu
+    buf.tmp[5] = -((inv_table[3]>>1)*(contrast>>16)*(saturation>>16));  //cgv
+
+
+    c->CSHIFT = (vector unsigned short)vec_splat_u16(2);
+    c->CY   = vec_splat ((vector signed short)buf.vec, 0);
+    c->OY   = vec_splat ((vector signed short)buf.vec, 1);
+    c->CRV  = vec_splat ((vector signed short)buf.vec, 2);
+    c->CBU  = vec_splat ((vector signed short)buf.vec, 3);
+    c->CGU  = vec_splat ((vector signed short)buf.vec, 4);
+    c->CGV  = vec_splat ((vector signed short)buf.vec, 5);
+#if 0
+    {
+    int i;
+    char *v[6]={"cy","oy","crv","cbu","cgu","cgv"};
+    for (i=0; i<6; i++)
+        printf("%s %d ", v[i],buf.tmp[i] );
+        printf("\n");
+    }
+#endif
+    return;
+}
+
+
+void
+altivec_yuv2packedX (SwsContext *c,
+                     int16_t *lumFilter, int16_t **lumSrc, int lumFilterSize,
+                     int16_t *chrFilter, int16_t **chrSrc, int chrFilterSize,
+                     uint8_t *dest, int dstW, int dstY)
+{
+    int i,j;
+    vector signed short X,X0,X1,Y0,U0,V0,Y1,U1,V1,U,V;
+    vector signed short R0,G0,B0,R1,G1,B1;
+
+    vector unsigned char R,G,B;
+    vector unsigned char *out,*nout;
+
+    vector signed short   RND = vec_splat_s16(1<<3);
+    vector unsigned short SCL = vec_splat_u16(4);
+    unsigned long scratch[16] __attribute__ ((aligned (16)));
+
+    vector signed short *YCoeffs, *CCoeffs;
+
+    YCoeffs = c->vYCoeffsBank+dstY*lumFilterSize;
+    CCoeffs = c->vCCoeffsBank+dstY*chrFilterSize;
+
+    out = (vector unsigned char *)dest;
+
+    for (i=0; i<dstW; i+=16){
+        Y0 = RND;
+        Y1 = RND;
+        /* extract 16 coeffs from lumSrc */
+        for (j=0; j<lumFilterSize; j++) {
+            X0 = vec_ld (0,  &lumSrc[j][i]);
+            X1 = vec_ld (16, &lumSrc[j][i]);
+            Y0 = vec_mradds (X0, YCoeffs[j], Y0);
+            Y1 = vec_mradds (X1, YCoeffs[j], Y1);
+        }
+
+        U = RND;
+        V = RND;
+        /* extract 8 coeffs from U,V */
+        for (j=0; j<chrFilterSize; j++) {
+            X  = vec_ld (0, &chrSrc[j][i/2]);
+            U  = vec_mradds (X, CCoeffs[j], U);
+            X  = vec_ld (0, &chrSrc[j][i/2+2048]);
+            V  = vec_mradds (X, CCoeffs[j], V);
+        }
+
+        /* scale and clip signals */
+        Y0 = vec_sra (Y0, SCL);
+        Y1 = vec_sra (Y1, SCL);
+        U  = vec_sra (U,  SCL);
+        V  = vec_sra (V,  SCL);
+
+        Y0 = vec_clip_s16 (Y0);
+        Y1 = vec_clip_s16 (Y1);
+        U  = vec_clip_s16 (U);
+        V  = vec_clip_s16 (V);
+
+        /* now we have
+          Y0= y0 y1 y2 y3 y4 y5 y6 y7     Y1= y8 y9 y10 y11 y12 y13 y14 y15
+          U= u0 u1 u2 u3 u4 u5 u6 u7      V= v0 v1 v2 v3 v4 v5 v6 v7
+
+          Y0= y0 y1 y2 y3 y4 y5 y6 y7    Y1= y8 y9 y10 y11 y12 y13 y14 y15
+          U0= u0 u0 u1 u1 u2 u2 u3 u3    U1= u4 u4 u5 u5 u6 u6 u7 u7
+          V0= v0 v0 v1 v1 v2 v2 v3 v3    V1= v4 v4 v5 v5 v6 v6 v7 v7
+        */
+
+        U0 = vec_mergeh (U,U);
+        V0 = vec_mergeh (V,V);
+
+        U1 = vec_mergel (U,U);
+        V1 = vec_mergel (V,V);
+
+        cvtyuvtoRGB (c, Y0,U0,V0,&R0,&G0,&B0);
+        cvtyuvtoRGB (c, Y1,U1,V1,&R1,&G1,&B1);
+
+        R  = vec_packclp (R0,R1);
+        G  = vec_packclp (G0,G1);
+        B  = vec_packclp (B0,B1);
+
+        switch(c->dstFormat) {
+            case PIX_FMT_ABGR:  out_abgr  (R,G,B,out); break;
+            case PIX_FMT_BGRA:  out_bgra  (R,G,B,out); break;
+            case PIX_FMT_RGBA:  out_rgba  (R,G,B,out); break;
+            case PIX_FMT_ARGB:  out_argb  (R,G,B,out); break;
+            case PIX_FMT_RGB24: out_rgb24 (R,G,B,out); break;
+            case PIX_FMT_BGR24: out_bgr24 (R,G,B,out); break;
+            default:
+            {
+                /* If this is reached, the caller should have called yuv2packedXinC
+                   instead. */
+                static int printed_error_message;
+                if (!printed_error_message) {
+                    av_log(c, AV_LOG_ERROR, "altivec_yuv2packedX doesn't support %s output\n",
+                           sws_format_name(c->dstFormat));
+                    printed_error_message=1;
+                }
+                return;
+            }
+        }
+    }
+
+    if (i < dstW) {
+        i -= 16;
+
+        Y0 = RND;
+        Y1 = RND;
+        /* extract 16 coeffs from lumSrc */
+        for (j=0; j<lumFilterSize; j++) {
+            X0 = vec_ld (0,  &lumSrc[j][i]);
+            X1 = vec_ld (16, &lumSrc[j][i]);
+            Y0 = vec_mradds (X0, YCoeffs[j], Y0);
+            Y1 = vec_mradds (X1, YCoeffs[j], Y1);
+        }
+
+        U = RND;
+        V = RND;
+        /* extract 8 coeffs from U,V */
+        for (j=0; j<chrFilterSize; j++) {
+            X  = vec_ld (0, &chrSrc[j][i/2]);
+            U  = vec_mradds (X, CCoeffs[j], U);
+            X  = vec_ld (0, &chrSrc[j][i/2+2048]);
+            V  = vec_mradds (X, CCoeffs[j], V);
+        }
+
+        /* scale and clip signals */
+        Y0 = vec_sra (Y0, SCL);
+        Y1 = vec_sra (Y1, SCL);
+        U  = vec_sra (U,  SCL);
+        V  = vec_sra (V,  SCL);
+
+        Y0 = vec_clip_s16 (Y0);
+        Y1 = vec_clip_s16 (Y1);
+        U  = vec_clip_s16 (U);
+        V  = vec_clip_s16 (V);
+
+        /* now we have
+           Y0= y0 y1 y2 y3 y4 y5 y6 y7     Y1= y8 y9 y10 y11 y12 y13 y14 y15
+           U = u0 u1 u2 u3 u4 u5 u6 u7     V = v0 v1 v2 v3 v4 v5 v6 v7
+
+           Y0= y0 y1 y2 y3 y4 y5 y6 y7    Y1= y8 y9 y10 y11 y12 y13 y14 y15
+           U0= u0 u0 u1 u1 u2 u2 u3 u3    U1= u4 u4 u5 u5 u6 u6 u7 u7
+           V0= v0 v0 v1 v1 v2 v2 v3 v3    V1= v4 v4 v5 v5 v6 v6 v7 v7
+        */
+
+        U0 = vec_mergeh (U,U);
+        V0 = vec_mergeh (V,V);
+
+        U1 = vec_mergel (U,U);
+        V1 = vec_mergel (V,V);
+
+        cvtyuvtoRGB (c, Y0,U0,V0,&R0,&G0,&B0);
+        cvtyuvtoRGB (c, Y1,U1,V1,&R1,&G1,&B1);
+
+        R  = vec_packclp (R0,R1);
+        G  = vec_packclp (G0,G1);
+        B  = vec_packclp (B0,B1);
+
+        nout = (vector unsigned char *)scratch;
+        switch(c->dstFormat) {
+            case PIX_FMT_ABGR:  out_abgr  (R,G,B,nout); break;
+            case PIX_FMT_BGRA:  out_bgra  (R,G,B,nout); break;
+            case PIX_FMT_RGBA:  out_rgba  (R,G,B,nout); break;
+            case PIX_FMT_ARGB:  out_argb  (R,G,B,nout); break;
+            case PIX_FMT_RGB24: out_rgb24 (R,G,B,nout); break;
+            case PIX_FMT_BGR24: out_bgr24 (R,G,B,nout); break;
+            default:
+                /* Unreachable, I think. */
+                av_log(c, AV_LOG_ERROR, "altivec_yuv2packedX doesn't support %s output\n",
+                       sws_format_name(c->dstFormat));
+                return;
+        }
+
+        memcpy (&((uint32_t*)dest)[i], scratch, (dstW-i)/4);
+    }
+
+}
+#endif //MEANX

Modified: branches/avidemux_2.4_branch/avidemux/ADM_libraries/ADM_mplex/CMakeLists.txt
===================================================================
--- branches/avidemux_2.4_branch/avidemux/ADM_libraries/ADM_mplex/CMakeLists.txt	2007-12-04 00:19:48 UTC (rev 3720)
+++ branches/avidemux_2.4_branch/avidemux/ADM_libraries/ADM_mplex/CMakeLists.txt	2007-12-04 13:43:28 UTC (rev 3721)
@@ -14,6 +14,7 @@
 ADD_ADM_LIB(${ADM_LIB} ADM_libraries)
 ################################
 include_directories("${CMAKE_CURRENT_SOURCE_DIR}")
+add_definitions(-DHAVE_FMAX)
 
 IF (NOT ADM_OS_APPLE)
     add_definitions(-DHAVE_MEMALIGN)



From gruntster at mail.berlios.de  Tue Dec  4 14:52:28 2007
From: gruntster at mail.berlios.de (gruntster at mail.berlios.de)
Date: Tue, 4 Dec 2007 14:52:28 +0100
Subject: [Avidemux-svn-commit] r3722 - branches/avidemux_2.4_branch
Message-ID: <200712041352.lB4DqS6S032547@sheep.berlios.de>

Author: gruntster
Date: 2007-12-04 14:52:16 +0100 (Tue, 04 Dec 2007)
New Revision: 3722

Modified:
   branches/avidemux_2.4_branch/ConfigureChecks.cmake
Log:
[CMake] assist cross-compilation

Modified: branches/avidemux_2.4_branch/ConfigureChecks.cmake
===================================================================
--- branches/avidemux_2.4_branch/ConfigureChecks.cmake	2007-12-04 13:43:28 UTC (rev 3721)
+++ branches/avidemux_2.4_branch/ConfigureChecks.cmake	2007-12-04 13:52:16 UTC (rev 3722)
@@ -4,6 +4,11 @@
 MESSAGE(STATUS "Checking CPU and OS")
 INCLUDE(CMakeDetermineSystem)
 
+# override detected processor for cross-compilation purposes
+IF (CROSS_SYSTEM_PROCESSOR)
+	SET(CMAKE_SYSTEM_PROCESSOR ${CROSS_SYSTEM_PROCESSOR})
+ENDIF (CROSS_SYSTEM_PROCESSOR)
+
 IF (WIN32)
 	SET(ADM_OS_WINDOWS 1)
 



From gruntster at mail.berlios.de  Thu Dec  6 20:39:42 2007
From: gruntster at mail.berlios.de (gruntster at mail.berlios.de)
Date: Thu, 6 Dec 2007 20:39:42 +0100
Subject: [Avidemux-svn-commit] r3723 -
	branches/avidemux_2.4_branch/avidemux/ADM_userInterfaces/ADM_QT4/ADM_gui
Message-ID: <200712061939.lB6JdgEG016760@sheep.berlios.de>

Author: gruntster
Date: 2007-12-06 20:39:29 +0100 (Thu, 06 Dec 2007)
New Revision: 3723

Modified:
   branches/avidemux_2.4_branch/avidemux/ADM_userInterfaces/ADM_QT4/ADM_gui/Q_gui2.cpp
Log:
[Mac] prevent menu items from moving to application menu due to conflict with SDL

Modified: branches/avidemux_2.4_branch/avidemux/ADM_userInterfaces/ADM_QT4/ADM_gui/Q_gui2.cpp
===================================================================
--- branches/avidemux_2.4_branch/avidemux/ADM_userInterfaces/ADM_QT4/ADM_gui/Q_gui2.cpp	2007-12-04 13:52:16 UTC (rev 3722)
+++ branches/avidemux_2.4_branch/avidemux/ADM_userInterfaces/ADM_QT4/ADM_gui/Q_gui2.cpp	2007-12-06 19:39:29 UTC (rev 3723)
@@ -239,6 +239,12 @@
 	this->setStatusBar(0);
 	this->adjustSize();
 
+#if defined(__APPLE__) && defined(USE_SDL)
+	ui.actionAbout_avidemux->setMenuRole(QAction::NoRole);
+	ui.actionPreferences->setMenuRole(QAction::NoRole);
+	ui.actionQuit->setMenuRole(QAction::NoRole);
+#endif
+
 	// Preview modes
 	QActionGroup *groupPreviewModes = new QActionGroup(this);
 



From gruntster at mail.berlios.de  Fri Dec  7 17:52:24 2007
From: gruntster at mail.berlios.de (gruntster at mail.berlios.de)
Date: Fri, 7 Dec 2007 17:52:24 +0100
Subject: [Avidemux-svn-commit] r3724 - branches/avidemux_2.4_branch/avidemux
Message-ID: <200712071652.lB7GqO4l006832@sheep.berlios.de>

Author: gruntster
Date: 2007-12-07 17:52:14 +0100 (Fri, 07 Dec 2007)
New Revision: 3724

Modified:
   branches/avidemux_2.4_branch/avidemux/CMakeLists.txt
Log:
[CMake] include OpenGL lib for Mac Leopard build

Modified: branches/avidemux_2.4_branch/avidemux/CMakeLists.txt
===================================================================
--- branches/avidemux_2.4_branch/avidemux/CMakeLists.txt	2007-12-06 19:39:29 UTC (rev 3723)
+++ branches/avidemux_2.4_branch/avidemux/CMakeLists.txt	2007-12-07 16:52:14 UTC (rev 3724)
@@ -414,6 +414,9 @@
 if (ADM_OS_APPLE)
 	add_libs_all_targets(z)
 	add_libs_all_targets("-framework CoreServices -framework CoreAudio -framework AudioUnit")
+
+	# for Leopard but it doesn't hurt Tiger
+	add_libs_all_targets("-Wl,-dylib_file,/System/Library/Frameworks/OpenGL.framework/Versions/A/Libraries/libGL.dylib:/System/Library/Frameworks/OpenGL.framework/Versions/A/Libraries/libGL.dylib")
 endif (ADM_OS_APPLE)
 
 ########################



From mean at mail.berlios.de  Sun Dec  9 16:48:41 2007
From: mean at mail.berlios.de (mean at BerliOS)
Date: Sun, 9 Dec 2007 16:48:41 +0100
Subject: [Avidemux-svn-commit] r3725 - branches
Message-ID: <200712091548.lB9Fmffa031782@sheep.berlios.de>

Author: mean
Date: 2007-12-09 16:48:41 +0100 (Sun, 09 Dec 2007)
New Revision: 3725

Removed:
   branches/avidemux_2.5_branch_mean/
Log:
Removed obsolete 2.5 branch



From mean at mail.berlios.de  Sun Dec  9 16:49:50 2007
From: mean at mail.berlios.de (mean at BerliOS)
Date: Sun, 9 Dec 2007 16:49:50 +0100
Subject: [Avidemux-svn-commit] r3726 - branches
Message-ID: <200712091549.lB9Fnojl031885@sheep.berlios.de>

Author: mean
Date: 2007-12-09 16:49:50 +0100 (Sun, 09 Dec 2007)
New Revision: 3726

Added:
   branches/avidemux_2.5_branch_mean/
Log:
copy



Copied: branches/avidemux_2.5_branch_mean (from rev 3725, branches/avidemux_2.4_branch)



From gruntster at mail.berlios.de  Mon Dec 10 17:24:52 2007
From: gruntster at mail.berlios.de (gruntster at mail.berlios.de)
Date: Mon, 10 Dec 2007 17:24:52 +0100
Subject: [Avidemux-svn-commit] r3727 -
	branches/avidemux_2.4_branch/avidemux/ADM_userInterfaces/ADM_QT4/ADM_gui
Message-ID: <200712101624.lBAGOqVr017691@sheep.berlios.de>

Author: gruntster
Date: 2007-12-10 17:24:43 +0100 (Mon, 10 Dec 2007)
New Revision: 3727

Modified:
   branches/avidemux_2.4_branch/avidemux/ADM_userInterfaces/ADM_QT4/ADM_gui/file_qt4.cpp
Log:
[Mac] use Qt4's file dialog instead of native

Modified: branches/avidemux_2.4_branch/avidemux/ADM_userInterfaces/ADM_QT4/ADM_gui/file_qt4.cpp
===================================================================
--- branches/avidemux_2.4_branch/avidemux/ADM_userInterfaces/ADM_QT4/ADM_gui/file_qt4.cpp	2007-12-09 15:49:50 UTC (rev 3726)
+++ branches/avidemux_2.4_branch/avidemux/ADM_userInterfaces/ADM_QT4/ADM_gui/file_qt4.cpp	2007-12-10 16:24:43 UTC (rev 3727)
@@ -22,7 +22,6 @@
 #include "prefs.h"
 #include "ADM_assert.h"
     
-extern QWidget *QuiMainWindows;
 static void GUI_FileSelSelect(const char *label, char **name, uint32_t access) ;
 
 //****************************************************************************************************
@@ -88,9 +87,15 @@
         }
   }
   QString fileName;
-    if(access)  fileName=QFileDialog::getSaveFileName(QuiMainWindows,label,str);
-        else    fileName=QFileDialog::getOpenFileName(QuiMainWindows,label,str);
-                
+  QFileDialog::Options options = 0;
+
+#if defined(__APPLE__)
+   options |= QFileDialog::DontUseNativeDialog;
+#endif
+
+   if(access)  fileName=QFileDialog::getSaveFileName(NULL, label, str, NULL, NULL, options);
+       else    fileName=QFileDialog::getOpenFileName(NULL, label, str, NULL, NULL, options);
+
   if(!fileName.isNull() )
   {
     const char *s=fileName.toUtf8().constData();
@@ -113,10 +118,15 @@
 uint8_t FileSel_SelectWrite(const char *title,char *target,uint32_t max, const char *source)
 {
   char *dir=NULL,*tmpname=NULL;
-   QString fileName;
- 
-  fileName=QFileDialog::getSaveFileName(QuiMainWindows,title,source);
-                
+  QString fileName;
+  QFileDialog::Options options = 0;
+
+#if defined(__APPLE__)
+  options |= QFileDialog::DontUseNativeDialog;
+#endif
+
+  fileName=QFileDialog::getSaveFileName(NULL, title, source, NULL, NULL, options);
+
   if(!fileName.isNull() )
   {
     const char *s=fileName.toUtf8().constData();
@@ -138,10 +148,15 @@
 uint8_t FileSel_SelectRead(const char *title,char *target,uint32_t max, const char *source)
 {
   char *dir=NULL,*tmpname=NULL;
-   QString fileName;
- 
-  fileName=QFileDialog::getOpenFileName(QuiMainWindows,title,source);
-                
+  QString fileName;
+  QFileDialog::Options options = 0;
+
+#if defined(__APPLE__)
+  options |= QFileDialog::DontUseNativeDialog;
+#endif
+
+  fileName = QFileDialog::getOpenFileName(NULL, title, source, NULL, NULL, options);
+
   if(!fileName.isNull() )
   {
     const char *s=fileName.toUtf8().constData();
@@ -168,9 +183,14 @@
    char *dir=NULL,*tmpname=NULL;
   
    QString fileName;
- 
-  fileName=QFileDialog::getExistingDirectory(QuiMainWindows,title,source);
-                
+   QFileDialog::Options options = QFileDialog::ShowDirsOnly;
+
+#if defined(__APPLE__)
+   options |= QFileDialog::DontUseNativeDialog;
+#endif
+
+   fileName=QFileDialog::getExistingDirectory(NULL, title, source, options);
+
   if(!fileName.isNull() )
   {
     const char *s=fileName.toUtf8().constData();



From gruntster at mail.berlios.de  Wed Dec 12 19:35:38 2007
From: gruntster at mail.berlios.de (gruntster at mail.berlios.de)
Date: Wed, 12 Dec 2007 19:35:38 +0100
Subject: [Avidemux-svn-commit] r3728 -
	branches/avidemux_2.4_branch/avidemux/winInstaller
Message-ID: <200712121835.lBCIZcaq020380@sheep.berlios.de>

Author: gruntster
Date: 2007-12-12 19:35:22 +0100 (Wed, 12 Dec 2007)
New Revision: 3728

Modified:
   branches/avidemux_2.4_branch/avidemux/winInstaller/avidemux.nsi
Log:
[Win32] add fonts from FontConfig to Win32 installer

Modified: branches/avidemux_2.4_branch/avidemux/winInstaller/avidemux.nsi
===================================================================
--- branches/avidemux_2.4_branch/avidemux/winInstaller/avidemux.nsi	2007-12-10 16:24:43 UTC (rev 3727)
+++ branches/avidemux_2.4_branch/avidemux/winInstaller/avidemux.nsi	2007-12-12 18:35:22 UTC (rev 3728)
@@ -187,6 +187,8 @@
     File ..\..\..\avidemux_2.4_build\vorbisenc.dll
     File ..\..\..\avidemux_2.4_build\xmltok.dll
     File ..\..\..\avidemux_2.4_build\xvidcore.dll
+    SetOutPath $INSTDIR\etc\fonts
+    File /r ..\..\..\avidemux_2.4_build\etc\fonts\*
     WriteRegStr HKLM "${REGKEY}\Components" "Core files (required)" 1
 SectionEnd
 
@@ -201,9 +203,11 @@
 
     Section GTK+ SecUiGtk
         SectionIn 1 2
-        SetOutPath $INSTDIR\etc
         SetOverwrite on
-        File /r ..\..\..\avidemux_2.4_build\etc\*
+        SetOutPath $INSTDIR\etc\gtk-2.0
+        File /r ..\..\..\avidemux_2.4_build\etc\gtk-2.0\*
+        SetOutPath $INSTDIR\etc\pango
+        File /r ..\..\..\avidemux_2.4_build\etc\pango\*
         SetOutPath $INSTDIR\lib\gtk-2.0
         File /r ..\..\..\avidemux_2.4_build\lib\gtk-2.0\*
         SetOutPath $INSTDIR\share\themes
@@ -468,7 +472,8 @@
     Delete /REBOOTOK $INSTDIR\avidemux2_gtk.exe
     RmDir /r /REBOOTOK $INSTDIR\share\themes
     RmDir /r /REBOOTOK $INSTDIR\lib\gtk-2.0
-    RmDir /r /REBOOTOK $INSTDIR\etc
+    RmDir /r /REBOOTOK $INSTDIR\etc\gtk-2.0
+    RmDir /r /REBOOTOK $INSTDIR\etc\pango
     DeleteRegValue HKLM "${REGKEY}\Components" GTK+
 SectionEnd
 
@@ -511,6 +516,7 @@
     Delete /REBOOTOK "$INSTDIR\Change Log.html"
     Delete /REBOOTOK "$INSTDIR\stdout.txt"
     Delete /REBOOTOK "$INSTDIR\stderr.txt"
+    RmDir /r /REBOOTOK $INSTDIR\etc\fonts
     DeleteRegValue HKLM "${REGKEY}\Components" Avidemux
     
     RmDir /REBOOTOK $INSTDIR\share\gettext
@@ -535,7 +541,8 @@
 SectionEnd
 
 Section un.post UnSecUninstaller
-	RmDir /REBOOTOK $INSTDIR\i18n
+    RmDir /REBOOTOK $INSTDIR\etc
+    RmDir /REBOOTOK $INSTDIR\i18n
     RmDir /REBOOTOK $INSTDIR\lib\locale
     RmDir /REBOOTOK $INSTDIR\lib
     RmDir /REBOOTOK $INSTDIR\share\locale



From gruntster at mail.berlios.de  Thu Dec 13 22:15:21 2007
From: gruntster at mail.berlios.de (gruntster at mail.berlios.de)
Date: Thu, 13 Dec 2007 22:15:21 +0100
Subject: [Avidemux-svn-commit] r3729 - branches/avidemux_2.5_branch_gruntster
Message-ID: <200712132115.lBDLFL0G028904@sheep.berlios.de>

Author: gruntster
Date: 2007-12-13 22:15:16 +0100 (Thu, 13 Dec 2007)
New Revision: 3729

Modified:
   branches/avidemux_2.5_branch_gruntster/CMakeLists.txt
Log:
[CMake] don't stop because Subversion can't be found

Modified: branches/avidemux_2.5_branch_gruntster/CMakeLists.txt
===================================================================
--- branches/avidemux_2.5_branch_gruntster/CMakeLists.txt	2007-12-12 18:35:22 UTC (rev 3728)
+++ branches/avidemux_2.5_branch_gruntster/CMakeLists.txt	2007-12-13 21:15:16 UTC (rev 3729)
@@ -104,6 +104,7 @@
 	MESSAGE(STATUS "Checking for Subversion")
 	MESSAGE(STATUS "***********************")
 
+	SET(Subversion_FIND_REQUIRED OFF)
 	FIND_PACKAGE(Subversion)
 
 	IF (Subversion_FOUND)
@@ -189,4 +190,4 @@
 # Directories to build
 ########################################
 ADD_SUBDIRECTORY(avidemux)
-ADD_SUBDIRECTORY(po)
\ No newline at end of file
+ADD_SUBDIRECTORY(po)



From gruntster at mail.berlios.de  Thu Dec 13 22:25:10 2007
From: gruntster at mail.berlios.de (gruntster at mail.berlios.de)
Date: Thu, 13 Dec 2007 22:25:10 +0100
Subject: [Avidemux-svn-commit] r3730 -
	branches/avidemux_2.5_branch_gruntster/avidemux/ADM_osSupport
Message-ID: <200712132125.lBDLPAPo029351@sheep.berlios.de>

Author: gruntster
Date: 2007-12-13 22:24:55 +0100 (Thu, 13 Dec 2007)
New Revision: 3730

Modified:
   branches/avidemux_2.5_branch_gruntster/avidemux/ADM_osSupport/ADM_cpuCap.cpp
Log:
[Mac] autodetect number of processors for libavcodec threading

Modified: branches/avidemux_2.5_branch_gruntster/avidemux/ADM_osSupport/ADM_cpuCap.cpp
===================================================================
--- branches/avidemux_2.5_branch_gruntster/avidemux/ADM_osSupport/ADM_cpuCap.cpp	2007-12-13 21:15:16 UTC (rev 3729)
+++ branches/avidemux_2.5_branch_gruntster/avidemux/ADM_osSupport/ADM_cpuCap.cpp	2007-12-13 21:24:55 UTC (rev 3730)
@@ -14,7 +14,10 @@
 
 #if defined(ADM_WIN32)
 #include <pthread.h>
-#elif !defined(__APPLE__)
+#elif defined(__APPLE__)
+#include <sys/types.h>
+#include <sys/sysctl.h>
+#else
 #include <string.h>
 #include <sched.h>
 #endif
@@ -182,7 +185,16 @@
 {
 #if defined(ADM_WIN32)
     return pthread_num_processors_np();
-#elif !defined(__APPLE__)
+#elif defined(__APPLE__)
+    int np;
+
+    size_t length = sizeof(np);
+
+    if (sysctlbyname("hw.ncpu", &np, &length, NULL, 0))
+        np = 1;
+
+    return np;
+#else
     unsigned int bit;
     int np;
 
@@ -194,7 +206,5 @@
         np += (((uint8_t *)&p_aff)[bit / 8] >> (bit % 8)) & 1;
 
     return np;
-#else
-    return 1;
 #endif
 }



From mean at mail.berlios.de  Fri Dec 14 19:43:41 2007
From: mean at mail.berlios.de (mean at BerliOS)
Date: Fri, 14 Dec 2007 19:43:41 +0100
Subject: [Avidemux-svn-commit] r3731 -
	branches/avidemux_2.4_branch/avidemux/ADM_audiocodec
Message-ID: <200712141843.lBEIhfD3018101@sheep.berlios.de>

Author: mean
Date: 2007-12-14 19:43:41 +0100 (Fri, 14 Dec 2007)
New Revision: 3731

Modified:
   branches/avidemux_2.4_branch/avidemux/ADM_audiocodec/ADM_audiocodec.cpp
Log:
[Mixer] Fix mono to stereo

Modified: branches/avidemux_2.4_branch/avidemux/ADM_audiocodec/ADM_audiocodec.cpp
===================================================================
--- branches/avidemux_2.4_branch/avidemux/ADM_audiocodec/ADM_audiocodec.cpp	2007-12-13 21:24:55 UTC (rev 3730)
+++ branches/avidemux_2.4_branch/avidemux/ADM_audiocodec/ADM_audiocodec.cpp	2007-12-14 18:43:41 UTC (rev 3731)
@@ -161,10 +161,15 @@
 		printf("\n Unknown codec : %lu",fourcc);
 		out = (ADM_Audiocodec *) new ADM_AudiocodecUnknown(fourcc);
 	}
+        if(info->channels==1)
+        {
+            ch_route.input_type[0] = CH_MONO;
+        }else
+        {
+	   ch_route.input_type[0] = CH_FRONT_LEFT;
+	   ch_route.input_type[1] = CH_FRONT_RIGHT;
+        }
 
-	ch_route.input_type[0] = CH_FRONT_LEFT;
-	ch_route.input_type[1] = CH_FRONT_RIGHT;
-
 	return out;
 }
 



