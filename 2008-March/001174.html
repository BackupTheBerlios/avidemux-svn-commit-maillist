<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<HTML>
 <HEAD>
   <TITLE> [Avidemux-svn-commit] r3912 - in	branches/avidemux_2.5_branch_gruntster/avidemux/ADM_libraries/ADM_ffmpeg:	ADM_libpostproc ADM_libswscale
   </TITLE>
   <LINK REL="Index" HREF="http://lists.berlios.de/pipermail/avidemux-svn-commit/2008-March/index.html" >
   <LINK REL="made" HREF="mailto:avidemux-svn-commit%40lists.berlios.de?Subject=Re%3A%20%5BAvidemux-svn-commit%5D%20r3912%20-%20in%0A%09branches/avidemux_2.5_branch_gruntster/avidemux/ADM_libraries/ADM_ffmpeg%3A%0A%09ADM_libpostproc%20ADM_libswscale&In-Reply-To=%3C200803210651.m2L6pe0Z005037%40sheep.berlios.de%3E">
   <META NAME="robots" CONTENT="index,nofollow">
   <style type="text/css">
       pre {
           white-space: pre-wrap;       /* css-2.1, curent FF, Opera, Safari */
           }
   </style>
   <META http-equiv="Content-Type" content="text/html; charset=us-ascii">
   <LINK REL="Previous"  HREF="001173.html">
   <LINK REL="Next"  HREF="001175.html">
 </HEAD>
 <BODY BGCOLOR="#ffffff">
   <H1>[Avidemux-svn-commit] r3912 - in	branches/avidemux_2.5_branch_gruntster/avidemux/ADM_libraries/ADM_ffmpeg:	ADM_libpostproc ADM_libswscale</H1>
    <B>mean at BerliOS</B> 
    <A HREF="mailto:avidemux-svn-commit%40lists.berlios.de?Subject=Re%3A%20%5BAvidemux-svn-commit%5D%20r3912%20-%20in%0A%09branches/avidemux_2.5_branch_gruntster/avidemux/ADM_libraries/ADM_ffmpeg%3A%0A%09ADM_libpostproc%20ADM_libswscale&In-Reply-To=%3C200803210651.m2L6pe0Z005037%40sheep.berlios.de%3E"
       TITLE="[Avidemux-svn-commit] r3912 - in	branches/avidemux_2.5_branch_gruntster/avidemux/ADM_libraries/ADM_ffmpeg:	ADM_libpostproc ADM_libswscale">mean at mail.berlios.de
       </A><BR>
    <I>Fri Mar 21 07:51:40 CET 2008</I>
    <P><UL>
        <LI>Previous message: <A HREF="001173.html">[Avidemux-svn-commit] r3910 - in	branches/avidemux_2.5_branch_gruntster:	avidemux/plugins/ADM_audiodecoder/ADM_ad_dca cmake
</A></li>
        <LI>Next message: <A HREF="001175.html">[Avidemux-svn-commit] r3913 -	branches/avidemux_2.5_branch_gruntster/avidemux/ADM_libraries/ADM_ffmpeg/ADM_lavformat
</A></li>
         <LI> <B>Messages sorted by:</B> 
              <a href="date.html#1174">[ date ]</a>
              <a href="thread.html#1174">[ thread ]</a>
              <a href="subject.html#1174">[ subject ]</a>
              <a href="author.html#1174">[ author ]</a>
         </LI>
       </UL>
    <HR>  
<!--beginarticle-->
<PRE>Author: mean
Date: 2008-03-21 07:51:32 +0100 (Fri, 21 Mar 2008)
New Revision: 3912

Modified:
   branches/avidemux_2.5_branch_gruntster/avidemux/ADM_libraries/ADM_ffmpeg/ADM_libpostproc/postprocess.c
   branches/avidemux_2.5_branch_gruntster/avidemux/ADM_libraries/ADM_ffmpeg/ADM_libpostproc/postprocess.h
   branches/avidemux_2.5_branch_gruntster/avidemux/ADM_libraries/ADM_ffmpeg/ADM_libpostproc/postprocess_altivec_template.c
   branches/avidemux_2.5_branch_gruntster/avidemux/ADM_libraries/ADM_ffmpeg/ADM_libpostproc/postprocess_internal.h
   branches/avidemux_2.5_branch_gruntster/avidemux/ADM_libraries/ADM_ffmpeg/ADM_libpostproc/postprocess_template.c
   branches/avidemux_2.5_branch_gruntster/avidemux/ADM_libraries/ADM_ffmpeg/ADM_libswscale/rgb2rgb.c
   branches/avidemux_2.5_branch_gruntster/avidemux/ADM_libraries/ADM_ffmpeg/ADM_libswscale/rgb2rgb.h
   branches/avidemux_2.5_branch_gruntster/avidemux/ADM_libraries/ADM_ffmpeg/ADM_libswscale/rgb2rgb_template.c
   branches/avidemux_2.5_branch_gruntster/avidemux/ADM_libraries/ADM_ffmpeg/ADM_libswscale/swscale.c
   branches/avidemux_2.5_branch_gruntster/avidemux/ADM_libraries/ADM_ffmpeg/ADM_libswscale/swscale.h
   branches/avidemux_2.5_branch_gruntster/avidemux/ADM_libraries/ADM_ffmpeg/ADM_libswscale/swscale_altivec_template.c
   branches/avidemux_2.5_branch_gruntster/avidemux/ADM_libraries/ADM_ffmpeg/ADM_libswscale/swscale_internal.h
   branches/avidemux_2.5_branch_gruntster/avidemux/ADM_libraries/ADM_ffmpeg/ADM_libswscale/swscale_template.c
   branches/avidemux_2.5_branch_gruntster/avidemux/ADM_libraries/ADM_ffmpeg/ADM_libswscale/yuv2rgb.c
   branches/avidemux_2.5_branch_gruntster/avidemux/ADM_libraries/ADM_ffmpeg/ADM_libswscale/yuv2rgb_altivec.c
   branches/avidemux_2.5_branch_gruntster/avidemux/ADM_libraries/ADM_ffmpeg/ADM_libswscale/yuv2rgb_template.c
Log:
[Lav*] import


Modified: branches/avidemux_2.5_branch_gruntster/avidemux/ADM_libraries/ADM_ffmpeg/ADM_libpostproc/postprocess.c
===================================================================
--- branches/avidemux_2.5_branch_gruntster/avidemux/ADM_libraries/ADM_ffmpeg/ADM_libpostproc/postprocess.c	2008-03-21 06:48:49 UTC (rev 3911)
+++ branches/avidemux_2.5_branch_gruntster/avidemux/ADM_libraries/ADM_ffmpeg/ADM_libpostproc/postprocess.c	2008-03-21 06:51:32 UTC (rev 3912)
@@ -47,8 +47,8 @@
 MedianDeinterlace#      E       Ec      Ec
 TempDeNoiser#           E               e       e       Ec
 
-* i dont have a 3dnow CPU -&gt; its untested, but noone said it doesnt work so it seems to work
-# more or less selfinvented filters so the exactness isnt too meaningfull
+* I do not have a 3DNow! CPU -&gt; it is untested, but no one said it does not work so it seems to work
+# more or less selfinvented filters so the exactness is not too meaningful
 E = Exact implementation
 e = allmost exact implementation (slightly different rounding,...)
 a = alternative / approximate impl
@@ -74,7 +74,7 @@
 //Changelog: use the Subversion log
 
 #include &quot;config.h&quot;
-#include &quot;../ADM_lavutil/avutil.h&quot;
+#include &quot;avutil.h&quot;
 #include &lt;inttypes.h&gt;
 #include &lt;stdio.h&gt;
 #include &lt;stdlib.h&gt;
@@ -87,14 +87,9 @@
 //#undef HAVE_MMX
 //#undef ARCH_X86
 //#define DEBUG_BRIGHTNESS
-#ifdef USE_FASTMEMCPY
-#include &quot;libvo/fastmemcpy.h&quot;
-#endif
 #include &quot;postprocess.h&quot;
 #include &quot;postprocess_internal.h&quot;
 
-#include &quot;mangle.h&quot; //FIXME should be supressed
-
 #ifdef HAVE_ALTIVEC_H
 #include &lt;altivec.h&gt;
 #endif
@@ -106,22 +101,19 @@
 //#define NUM_BLOCKS_AT_ONCE 16 //not used yet
 
 #if defined(ARCH_X86)
-static uint64_t __attribute__((aligned(8))) attribute_used w05= 0x0005000500050005LL;
-static uint64_t __attribute__((aligned(8))) attribute_used w04= 0x0004000400040004LL;
-static uint64_t __attribute__((aligned(8))) attribute_used w20= 0x0020002000200020LL;
-static uint64_t __attribute__((aligned(8))) attribute_used b00= 0x0000000000000000LL;
-static uint64_t __attribute__((aligned(8))) attribute_used b01= 0x0101010101010101LL;
-static uint64_t __attribute__((aligned(8))) attribute_used b02= 0x0202020202020202LL;
-static uint64_t __attribute__((aligned(8))) attribute_used b08= 0x0808080808080808LL;
-static uint64_t __attribute__((aligned(8))) attribute_used b80= 0x8080808080808080LL;
+DECLARE_ASM_CONST(8, uint64_t, w05)= 0x0005000500050005LL;
+DECLARE_ASM_CONST(8, uint64_t, w04)= 0x0004000400040004LL;
+DECLARE_ASM_CONST(8, uint64_t, w20)= 0x0020002000200020LL;
+DECLARE_ASM_CONST(8, uint64_t, b00)= 0x0000000000000000LL;
+DECLARE_ASM_CONST(8, uint64_t, b01)= 0x0101010101010101LL;
+DECLARE_ASM_CONST(8, uint64_t, b02)= 0x0202020202020202LL;
+DECLARE_ASM_CONST(8, uint64_t, b08)= 0x0808080808080808LL;
+DECLARE_ASM_CONST(8, uint64_t, b80)= 0x8080808080808080LL;
 #endif
 
-static uint8_t clip_table[3*256];
-static uint8_t * const clip_tab= clip_table + 256;
+DECLARE_ASM_CONST(8, int, deringThreshold)= 20;
 
-static const int attribute_used deringThreshold= 20;
 
-
 static struct PPFilter filters[]=
 {
         {&quot;hb&quot;, &quot;hdeblock&quot;,              1, 1, 3, H_DEBLOCK},
@@ -147,11 +139,11 @@
 
 static const char *replaceTable[]=
 {
-        &quot;default&quot;,      &quot;hdeblock:a,vdeblock:a,dering:a&quot;,
-        &quot;de&quot;,           &quot;hdeblock:a,vdeblock:a,dering:a&quot;,
-        &quot;fast&quot;,         &quot;x1hdeblock:a,x1vdeblock:a,dering:a&quot;,
-        &quot;fa&quot;,           &quot;x1hdeblock:a,x1vdeblock:a,dering:a&quot;,
-        &quot;ac&quot;,           &quot;ha:a:128:7,va:a,dering:a&quot;,
+        &quot;default&quot;,      &quot;hb:a,vb:a,dr:a&quot;,
+        &quot;de&quot;,           &quot;hb:a,vb:a,dr:a&quot;,
+        &quot;fast&quot;,         &quot;h1:a,v1:a,dr:a&quot;,
+        &quot;fa&quot;,           &quot;h1:a,v1:a,dr:a&quot;,
+        &quot;ac&quot;,           &quot;ha:a:128:7,va:a,dr:a&quot;,
         NULL //End Marker
 };
 
@@ -400,9 +392,9 @@
 /**
  * Experimental Filter 1 (Horizontal)
  * will not damage linear gradients
- * Flat blocks should look like they where passed through the (1,1,2,2,4,2,2,1,1) 9-Tap filter
- * can only smooth blocks at the expected locations (it cant smooth them if they did move)
- * MMX2 version does correct clipping C version doesnt
+ * Flat blocks should look like they were passed through the (1,1,2,2,4,2,2,1,1) 9-Tap filter
+ * can only smooth blocks at the expected locations (it cannot smooth them if they did move)
+ * MMX2 version does correct clipping C version does not
  * not identical with the vertical one
  */
 static inline void horizX1Filter(uint8_t *src, int stride, int QP)
@@ -649,21 +641,21 @@
 #include &quot;postprocess_template.c&quot;
 #endif
 
-// minor note: the HAVE_xyz is messed up after that line so dont use it
+// minor note: the HAVE_xyz is messed up after that line so do not use it.
 
-static inline void postProcess(uint8_t src[], int srcStride, uint8_t dst[], int dstStride, int width, int height,
-        QP_STORE_T QPs[], int QPStride, int isColor, pp_mode_t *vm, pp_context_t *vc)
+static inline void postProcess(const uint8_t src[], int srcStride, uint8_t dst[], int dstStride, int width, int height,
+        const QP_STORE_T QPs[], int QPStride, int isColor, pp_mode_t *vm, pp_context_t *vc)
 {
         PPContext *c= (PPContext *)vc;
         PPMode *ppMode= (PPMode *)vm;
         c-&gt;ppMode= *ppMode; //FIXME
 
-        // useing ifs here as they are faster than function pointers allthough the
-        // difference wouldnt be messureable here but its much better because
-        // someone might exchange the cpu whithout restarting mplayer ;)
+        // Using ifs here as they are faster than function pointers although the
+        // difference would not be measurable here but it is much better because
+        // someone might exchange the CPU whithout restarting MPlayer ;)
 #ifdef RUNTIME_CPUDETECT
 #if defined(ARCH_X86)
-        // ordered per speed fasterst first
+        // ordered per speed fastest first
         if(c-&gt;cpuCaps &amp; PP_CPU_CAPS_MMX2)
                 postProcess_MMX2(src, srcStride, dst, dstStride, width, height, QPs, QPStride, isColor, c);
         else if(c-&gt;cpuCaps &amp; PP_CPU_CAPS_3DNOW)
@@ -702,7 +694,11 @@
 
 /* -pp Command line Help
 */
-char *pp_help=
+#if LIBPOSTPROC_VERSION_INT &lt; (52&lt;&lt;16)
+const char *const pp_help=
+#else
+const char pp_help[] =
+#endif
 &quot;Available postprocessing filters:\n&quot;
 &quot;Filters                        Options\n&quot;
 &quot;short  long name       short   long option     Description\n&quot;
@@ -746,12 +742,12 @@
 &quot;\n&quot;
 ;
 
-pp_mode_t *pp_get_mode_by_name_and_quality(char *name, int quality)
+pp_mode_t *pp_get_mode_by_name_and_quality(const char *name, int quality)
 {
         char temp[GET_MODE_BUFFER_SIZE];
         char *p= temp;
-        const char *filterDelimiters= &quot;,/&quot;;
-        const char *optionDelimiters= &quot;:&quot;;
+        static const char filterDelimiters[] = &quot;,/&quot;;
+        static const char optionDelimiters[] = &quot;:&quot;;
         struct PPMode *ppMode;
         char *filterToken;
 
@@ -964,7 +960,7 @@
 
         for(i=0; i&lt;3; i++)
         {
-                //Note:the +17*1024 is just there so i dont have to worry about r/w over te end
+                //Note: The +17*1024 is just there so i do not have to worry about r/w over the end.
                 reallocAlign((void **)&amp;c-&gt;tempBlured[i], 8, stride*mbHeight*16 + 17*1024);
                 reallocAlign((void **)&amp;c-&gt;tempBluredPast[i], 8, 256*((height+7)&amp;(~7))/2 + 17*1024);//FIXME size
         }
@@ -975,27 +971,17 @@
         reallocAlign((void **)&amp;c-&gt;forcedQPTable, 8, mbWidth*sizeof(QP_STORE_T));
 }
 
-static void global_init(void){
-        int i;
-        memset(clip_table, 0, 256);
-        for(i=256; i&lt;512; i++)
-                clip_table[i]= i;
-        memset(clip_table+512, 0, 256);
-}
-
 static const char * context_to_name(void * ptr) {
     return &quot;postproc&quot;;
 }
 
-static AVClass av_codec_context_class = { &quot;Postproc&quot;, context_to_name, NULL };
+static const AVClass av_codec_context_class = { &quot;Postproc&quot;, context_to_name, NULL };
 
 pp_context_t *pp_get_context(int width, int height, int cpuCaps){
         PPContext *c= av_malloc(sizeof(PPContext));
         int stride= (width+15)&amp;(~15);    //assumed / will realloc if needed
         int qpStride= (width+15)/16 + 2; //assumed / will realloc if needed
 
-        global_init();
-
         memset(c, 0, sizeof(PPContext));
         c-&gt;av_class = &amp;av_codec_context_class;
         c-&gt;cpuCaps= cpuCaps;
@@ -1035,10 +1021,10 @@
         av_free(c);
 }
 
-void  pp_postprocess(uint8_t * src[3], int srcStride[3],
-                 uint8_t * dst[3], int dstStride[3],
+void  pp_postprocess(const uint8_t * src[3], const int srcStride[3],
+                 uint8_t * dst[3], const int dstStride[3],
                  int width, int height,
-                 QP_STORE_T *QP_store,  int QPStride,
+                 const QP_STORE_T *QP_store,  int QPStride,
                  pp_mode_t *vm,  void *vc, int pict_type)
 {
         int mbWidth = (width+15)&gt;&gt;4;
@@ -1060,16 +1046,16 @@
                 QP_store= c-&gt;forcedQPTable;
                 absQPStride = QPStride = 0;
                 if(mode-&gt;lumMode &amp; FORCE_QUANT)
-                        for(i=0; i&lt;mbWidth; i++) QP_store[i]= mode-&gt;forcedQuant;
+                        for(i=0; i&lt;mbWidth; i++) c-&gt;forcedQPTable[i]= mode-&gt;forcedQuant;
                 else
-                        for(i=0; i&lt;mbWidth; i++) QP_store[i]= 1;
+                        for(i=0; i&lt;mbWidth; i++) c-&gt;forcedQPTable[i]= 1;
         }
 
         if(pict_type &amp; PP_PICT_TYPE_QP2){
                 int i;
                 const int count= mbHeight * absQPStride;
                 for(i=0; i&lt;(count&gt;&gt;2); i++){
-                        ((uint32_t*)c-&gt;stdQPTable)[i] = (((uint32_t*)QP_store)[i]&gt;&gt;1) &amp; 0x7F7F7F7F;
+                        ((uint32_t*)c-&gt;stdQPTable)[i] = (((const uint32_t*)QP_store)[i]&gt;&gt;1) &amp; 0x7F7F7F7F;
                 }
                 for(i&lt;&lt;=2; i&lt;count; i++){
                         c-&gt;stdQPTable[i] = QP_store[i]&gt;&gt;1;
@@ -1095,7 +1081,7 @@
                         int i;
                         const int count= mbHeight * QPStride;
                         for(i=0; i&lt;(count&gt;&gt;2); i++){
-                                ((uint32_t*)c-&gt;nonBQPTable)[i] = ((uint32_t*)QP_store)[i] &amp; 0x3F3F3F3F;
+                                ((uint32_t*)c-&gt;nonBQPTable)[i] = ((const uint32_t*)QP_store)[i] &amp; 0x3F3F3F3F;
                         }
                         for(i&lt;&lt;=2; i&lt;count; i++){
                                 c-&gt;nonBQPTable[i] = QP_store[i] &amp; 0x3F;

Modified: branches/avidemux_2.5_branch_gruntster/avidemux/ADM_libraries/ADM_ffmpeg/ADM_libpostproc/postprocess.h
===================================================================
--- branches/avidemux_2.5_branch_gruntster/avidemux/ADM_libraries/ADM_ffmpeg/ADM_libpostproc/postprocess.h	2008-03-21 06:48:49 UTC (rev 3911)
+++ branches/avidemux_2.5_branch_gruntster/avidemux/ADM_libraries/ADM_ffmpeg/ADM_libpostproc/postprocess.h	2008-03-21 06:51:32 UTC (rev 3912)
@@ -18,21 +18,27 @@
  * Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA
  */
 
-#ifndef NEWPOSTPROCESS_H
-#define NEWPOSTPROCESS_H
+#ifndef FFMPEG_POSTPROCESS_H
+#define FFMPEG_POSTPROCESS_H
 
 /**
  * @file postprocess.h
  * @brief
- *     external api for the pp stuff
+ *     external postprocessing API
  */
 
-#ifdef __cplusplus
-extern &quot;C&quot; {
-#endif
+#include &quot;libavutil/avutil.h&quot;
 
-#define LIBPOSTPROC_VERSION_INT ((51&lt;&lt;16)+(1&lt;&lt;8)+0)
-#define LIBPOSTPROC_VERSION     51.1.0
+#define LIBPOSTPROC_VERSION_MAJOR 51
+#define LIBPOSTPROC_VERSION_MINOR  1
+#define LIBPOSTPROC_VERSION_MICRO  0
+
+#define LIBPOSTPROC_VERSION_INT AV_VERSION_INT(LIBPOSTPROC_VERSION_MAJOR, \
+                                               LIBPOSTPROC_VERSION_MINOR, \
+                                               LIBPOSTPROC_VERSION_MICRO)
+#define LIBPOSTPROC_VERSION     AV_VERSION(LIBPOSTPROC_VERSION_MAJOR, \
+                                           LIBPOSTPROC_VERSION_MINOR, \
+                                           LIBPOSTPROC_VERSION_MICRO)
 #define LIBPOSTPROC_BUILD       LIBPOSTPROC_VERSION_INT
 
 #define LIBPOSTPROC_IDENT       &quot;postproc&quot; AV_STRINGIFY(LIBPOSTPROC_VERSION)
@@ -41,15 +47,21 @@
 
 #define QP_STORE_T int8_t
 
+#include &lt;inttypes.h&gt;
+
 typedef void pp_context_t;
 typedef void pp_mode_t;
 
-extern char *pp_help; ///&lt; a simple help text
+#if LIBPOSTPROC_VERSION_INT &lt; (52&lt;&lt;16)
+extern const char *const pp_help; ///&lt; a simple help text
+#else
+extern const char pp_help[]; ///&lt; a simple help text
+#endif
 
-void  pp_postprocess(uint8_t * src[3], int srcStride[3],
-                 uint8_t * dst[3], int dstStride[3],
+void  pp_postprocess(const uint8_t * src[3], const int srcStride[3],
+                 uint8_t * dst[3], const int dstStride[3],
                  int horizontalSize, int verticalSize,
-                 QP_STORE_T *QP_store,  int QP_stride,
+                 const QP_STORE_T *QP_store,  int QP_stride,
                  pp_mode_t *mode, pp_context_t *ppContext, int pict_type);
 
 
@@ -58,7 +70,7 @@
  * name is the string after &quot;-pp&quot; on the command line
  * quality is a number from 0 to PP_QUALITY_MAX
  */
-pp_mode_t *pp_get_mode_by_name_and_quality(char *name, int quality);
+pp_mode_t *pp_get_mode_by_name_and_quality(const char *name, int quality);
 void pp_free_mode(pp_mode_t *mode);
 
 pp_context_t *pp_get_context(int width, int height, int flags);
@@ -77,8 +89,4 @@
 
 #define PP_PICT_TYPE_QP2  0x00000010 ///&lt; MPEG2 style QScale
 
-#ifdef __cplusplus
-}
-#endif
-
-#endif
+#endif /* FFMPEG_POSTPROCESS_H */

Modified: branches/avidemux_2.5_branch_gruntster/avidemux/ADM_libraries/ADM_ffmpeg/ADM_libpostproc/postprocess_altivec_template.c
===================================================================
--- branches/avidemux_2.5_branch_gruntster/avidemux/ADM_libraries/ADM_ffmpeg/ADM_libpostproc/postprocess_altivec_template.c	2008-03-21 06:48:49 UTC (rev 3911)
+++ branches/avidemux_2.5_branch_gruntster/avidemux/ADM_libraries/ADM_ffmpeg/ADM_libpostproc/postprocess_altivec_template.c	2008-03-21 06:51:32 UTC (rev 3912)
@@ -20,13 +20,8 @@
  * Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA
  */
 
+#include &quot;avutil.h&quot;
 
-#ifdef CONFIG_DARWIN
-#define AVV(x...) (x)
-#else
-#define AVV(x...) {x}
-#endif
-
 #define ALTIVEC_TRANSPOSE_8x8_SHORT(src_a,src_b,src_c,src_d,src_e,src_f,src_g,src_h) \
   do {                                                                  \
     __typeof__(src_a) tempA1, tempB1, tempC1, tempD1;                   \
@@ -67,7 +62,7 @@
     vector by assuming (stride % 16) == 0, unfortunately
     this is not always true.
   */
-  short __attribute__ ((aligned(16))) data[8];
+  DECLARE_ALIGNED(16, short, data[8]);
   int numEq;
   uint8_t *src2 = src;
   vector signed short v_dcOffset;
@@ -115,9 +110,9 @@
     (vector signed short)vec_mergeh((vector signed char)zero,           \
                                     (vector signed char)v_srcA##i)
 
-    // special casing the aligned case is worthwhile, as all call from
-    // the (transposed) horizontable deblocks will be aligned, i naddition
-    // to the naturraly aligned vertical deblocks.
+    /* Special-casing the aligned case is worthwhile, as all calls from
+     * the (transposed) horizontable deblocks will be aligned, in addition
+     * to the naturally aligned vertical deblocks. */
     if (properStride &amp;&amp; srcAlign) {
       LOAD_LINE_ALIGNED(0);
       LOAD_LINE_ALIGNED(1);
@@ -199,14 +194,14 @@
     One could remove the recomputation of the perm
     vector by assuming (stride % 16) == 0, unfortunately
     this is not always true. Quite a lot of load/stores
-    can be removed by assuming proper alignement of
+    can be removed by assuming proper alignment of
     src &amp; stride :-(
   */
   uint8_t *src2 = src;
   const vector signed int zero = vec_splat_s32(0);
   const int properStride = (stride % 16);
   const int srcAlign = ((unsigned long)src2 % 16);
-  short __attribute__ ((aligned(16))) qp[8];
+  DECLARE_ALIGNED(16, short, qp[8]);
   qp[0] = c-&gt;QP;
   vector signed short vqp = vec_ld(0, qp);
   vqp = vec_splat(vqp, 0);
@@ -235,9 +230,9 @@
     (vector signed short)vec_mergeh((vector signed char)zero,           \
                                     (vector signed char)vbT##i)
 
-    // special casing the aligned case is worthwhile, as all call from
-    // the (transposed) horizontable deblocks will be aligned, in addition
-    // to the naturraly aligned vertical deblocks.
+    /* Special-casing the aligned case is worthwhile, as all calls from
+     * the (transposed) horizontable deblocks will be aligned, in addition
+     * to the naturally aligned vertical deblocks. */
     if (properStride &amp;&amp; srcAlign) {
       LOAD_LINE_ALIGNED(0);
       LOAD_LINE_ALIGNED(1);
@@ -353,9 +348,9 @@
     vec_perm(vf##i, vbT##i, permHH);                            \
   vec_st(vg##i, i * stride, src2)
 
-  // special casing the aligned case is worthwhile, as all call from
-  // the (transposed) horizontable deblocks will be aligned, in addition
-  // to the naturraly aligned vertical deblocks.
+  /* Special-casing the aligned case is worthwhile, as all calls from
+   * the (transposed) horizontable deblocks will be aligned, in addition
+   * to the naturally aligned vertical deblocks. */
   if (properStride &amp;&amp; srcAlign) {
     PACK_AND_STORE_ALIGNED(1);
     PACK_AND_STORE_ALIGNED(2);
@@ -387,12 +382,12 @@
     One could remove the recomputation of the perm
     vector by assuming (stride % 16) == 0, unfortunately
     this is not always true. Quite a lot of load/stores
-    can be removed by assuming proper alignement of
+    can be removed by assuming proper alignment of
     src &amp; stride :-(
   */
   uint8_t *src2 = src;
   const vector signed int zero = vec_splat_s32(0);
-  short __attribute__ ((aligned(16))) qp[8];
+  DECLARE_ALIGNED(16, short, qp[8]);
   qp[0] = 8*c-&gt;QP;
   vector signed short vqp = vec_ld(0, qp);
   vqp = vec_splat(vqp, 0);
@@ -474,7 +469,7 @@
   const vector signed short dornotd = vec_sel((vector signed short)zero,
                                               dclampedfinal,
                                               vec_cmplt(absmE, vqp));
-  /* add/substract to l4 and l5 */
+  /* add/subtract to l4 and l5 */
   const vector signed short vb4minusd = vec_sub(vb4, dornotd);
   const vector signed short vb5plusd = vec_add(vb5, dornotd);
   /* finally, stores */
@@ -511,11 +506,11 @@
     One could remove the recomputation of the perm
     vector by assuming (stride % 16) == 0, unfortunately
     this is not always true. Quite a lot of load/stores
-    can be removed by assuming proper alignement of
+    can be removed by assuming proper alignment of
     src &amp; stride :-(
   */
   uint8_t *srcCopy = src;
-  uint8_t __attribute__((aligned(16))) dt[16];
+  DECLARE_ALIGNED(16, uint8_t, dt[16]);
   const vector signed int zero = vec_splat_s32(0);
   vector unsigned char v_dt;
   dt[0] = deringThreshold;
@@ -579,7 +574,7 @@
     v_avg = vec_avg(v_min, v_max);
   }
 
-  signed int __attribute__((aligned(16))) S[8];
+  DECLARE_ALIGNED(16, signed int, S[8]);
   {
     const vector unsigned short mask1 = (vector unsigned short)
       AVV(0x0001, 0x0002, 0x0004, 0x0008,
@@ -675,7 +670,7 @@
   /* I'm not sure the following is actually faster
      than straight, unvectorized C code :-( */
 
-  int __attribute__((aligned(16))) tQP2[4];
+  DECLARE_ALIGNED(16, int, tQP2[4]);
   tQP2[0]= c-&gt;QP/2 + 1;
   vector signed int vQP2 = vec_ld(0, tQP2);
   vQP2 = vec_splat(vQP2, 0);

Modified: branches/avidemux_2.5_branch_gruntster/avidemux/ADM_libraries/ADM_ffmpeg/ADM_libpostproc/postprocess_internal.h
===================================================================
--- branches/avidemux_2.5_branch_gruntster/avidemux/ADM_libraries/ADM_ffmpeg/ADM_libpostproc/postprocess_internal.h	2008-03-21 06:48:49 UTC (rev 3911)
+++ branches/avidemux_2.5_branch_gruntster/avidemux/ADM_libraries/ADM_ffmpeg/ADM_libpostproc/postprocess_internal.h	2008-03-21 06:51:32 UTC (rev 3912)
@@ -23,8 +23,12 @@
  * internal api header.
  */
 
-#include &quot;../ADM_lavutil/avutil.h&quot;
+#ifndef FFMPEG_POSTPROCESS_INTERNAL_H
+#define FFMPEG_POSTPROCESS_INTERNAL_H
 
+#include &quot;avutil.h&quot;
+#include &quot;postprocess.h&quot;
+
 #define V_DEBLOCK       0x01
 #define H_DEBLOCK       0x02
 #define DERING          0x04
@@ -62,27 +66,16 @@
 #define TEMP_NOISE_FILTER               0x100000
 #define FORCE_QUANT                     0x200000
 
-#if ( defined(__PIC__) || defined(__pic__) ) &amp;&amp; ! defined(PIC)
-#    define PIC
-#endif
-
-//use if u want a faster postprocessing code
-//cant differentiate between chroma &amp; luma filters (both on or both off)
-//obviosly the -pp option at the commandline has no effect except turning the here selected
+//use if you want a faster postprocessing code
+//cannot differentiate between chroma &amp; luma filters (both on or both off)
+//obviously the -pp option on the command line has no effect except turning the here selected
 //filters on
 //#define COMPILE_TIME_MODE 0x77
 
-#if 1
 static inline int CLIP(int a){
         if(a&amp;256) return ((a)&gt;&gt;31)^(-1);
         else      return a;
 }
-//#define CLIP(a) (((a)&amp;256) ? ((a)&gt;&gt;31)^(-1) : (a))
-#elif 0
-#define CLIP(a) clip_tab[a]
-#else
-#define CLIP(a) (a)
-#endif
 /**
  * Postprocessng filter.
  */
@@ -122,7 +115,7 @@
         /**
          * info on struct for av_log
          */
-        AVClass *av_class;
+        const AVClass *av_class;
 
         uint8_t *tempBlocks; ///&lt;used for the horizontal code
 
@@ -173,11 +166,13 @@
 } PPContext;
 
 
-static inline void linecpy(void *dest, void *src, int lines, int stride)
+static inline void linecpy(void *dest, const void *src, int lines, int stride)
 {
         if (stride &gt; 0) {
                 memcpy(dest, src, lines*stride);
         } else {
-                memcpy(dest+(lines-1)*stride, src+(lines-1)*stride, -lines*stride);
+                memcpy((uint8_t*)dest+(lines-1)*stride, (const uint8_t*)src+(lines-1)*stride, -lines*stride);
         }
 }
+
+#endif /* FFMPEG_POSTPROCESS_INTERNAL_H */

Modified: branches/avidemux_2.5_branch_gruntster/avidemux/ADM_libraries/ADM_ffmpeg/ADM_libpostproc/postprocess_template.c
===================================================================
--- branches/avidemux_2.5_branch_gruntster/avidemux/ADM_libraries/ADM_ffmpeg/ADM_libpostproc/postprocess_template.c	2008-03-21 06:48:49 UTC (rev 3911)
+++ branches/avidemux_2.5_branch_gruntster/avidemux/ADM_libraries/ADM_ffmpeg/ADM_libpostproc/postprocess_template.c	2008-03-21 06:51:32 UTC (rev 3912)
@@ -16,7 +16,7 @@
  * You should have received a copy of the GNU General Public License
  * along with FFmpeg; if not, write to the Free Software
  * Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA
-*/
+ */
 
 /**
  * @file postprocess_template.c
@@ -73,7 +73,7 @@
         &quot;paddb &quot; #a &quot;, &quot; #b &quot; \n\t&quot;
 #endif
 
-//FIXME? |255-0| = 1 (shouldnt be a problem ...)
+//FIXME? |255-0| = 1 (should not be a problem ...)
 #ifdef HAVE_MMX
 /**
  * Check if the middle 8x8 Block in the given 8x16 block is flat
@@ -373,7 +373,8 @@
  * Experimental implementation of the filter (Algorithm 1) described in a paper from Ramkishor &amp; Karandikar
  * values are correctly clipped (MMX2)
  * values are wraparound (C)
- * conclusion: its fast, but introduces ugly horizontal patterns if there is a continious gradient
+ * Conclusion: It is fast, but introduces ugly horizontal patterns
+ * if there is a continuous gradient.
         0 8 16 24
         x = 8
         x/2 = 4
@@ -477,9 +478,9 @@
 /**
  * Experimental Filter 1
  * will not damage linear gradients
- * Flat blocks should look like they where passed through the (1,1,2,2,4,2,2,1,1) 9-Tap filter
- * can only smooth blocks at the expected locations (it cant smooth them if they did move)
- * MMX2 version does correct clipping C version doesnt
+ * Flat blocks should look like they were passed through the (1,1,2,2,4,2,2,1,1) 9-Tap filter
+ * can only smooth blocks at the expected locations (it cannot smooth them if they did move)
+ * MMX2 version does correct clipping C version does not
  */
 static inline void RENAME(vertX1Filter)(uint8_t *src, int stride, PPContext *co)
 {
@@ -1545,7 +1546,7 @@
 /**
  * Deinterlaces the given block by linearly interpolating every second line.
  * will be called for every 8x8 block and can read &amp; write from line 4-15
- * lines 0-3 have been passed through the deblock / dering filters allready, but can be read too
+ * lines 0-3 have been passed through the deblock / dering filters already, but can be read, too.
  * lines 4-12 will be read into the deblocking filter and should be deinterlaced
  */
 static inline void RENAME(deInterlaceInterpolateLinear)(uint8_t src[], int stride)
@@ -1597,7 +1598,7 @@
 /**
  * Deinterlaces the given block by cubic interpolating every second line.
  * will be called for every 8x8 block and can read &amp; write from line 4-15
- * lines 0-3 have been passed through the deblock / dering filters allready, but can be read too
+ * lines 0-3 have been passed through the deblock / dering filters already, but can be read, too.
  * lines 4-12 will be read into the deblocking filter and should be deinterlaced
  * this filter will read lines 3-15 and write 7-13
  */
@@ -1662,7 +1663,7 @@
 /**
  * Deinterlaces the given block by filtering every second line with a (-1 4 2 4 -1) filter.
  * will be called for every 8x8 block and can read &amp; write from line 4-15
- * lines 0-3 have been passed through the deblock / dering filters allready, but can be read too
+ * lines 0-3 have been passed through the deblock / dering filters already, but can be read, too.
  * lines 4-12 will be read into the deblocking filter and should be deinterlaced
  * this filter will read lines 4-13 and write 5-11
  */
@@ -1742,7 +1743,7 @@
 /**
  * Deinterlaces the given block by filtering every line with a (-1 2 6 2 -1) filter.
  * will be called for every 8x8 block and can read &amp; write from line 4-15
- * lines 0-3 have been passed through the deblock / dering filters allready, but can be read too
+ * lines 0-3 have been passed through the deblock / dering filters already, but can be read, too.
  * lines 4-12 will be read into the deblocking filter and should be deinterlaced
  * this filter will read lines 4-13 and write 4-11
  */
@@ -1844,7 +1845,7 @@
 /**
  * Deinterlaces the given block by filtering all lines with a (1 2 1) filter.
  * will be called for every 8x8 block and can read &amp; write from line 4-15
- * lines 0-3 have been passed through the deblock / dering filters allready, but can be read too
+ * lines 0-3 have been passed through the deblock / dering filters already, but can be read, too.
  * lines 4-12 will be read into the deblocking filter and should be deinterlaced
  * this filter will read lines 4-13 and write 4-11
  */
@@ -1946,7 +1947,7 @@
 /**
  * Deinterlaces the given block by applying a median filter to every second line.
  * will be called for every 8x8 block and can read &amp; write from line 4-15,
- * lines 0-3 have been passed through the deblock / dering filters allready, but can be read too
+ * lines 0-3 have been passed through the deblock / dering filters already, but can be read, too.
  * lines 4-12 will be read into the deblocking filter and should be deinterlaced
  */
 static inline void RENAME(deInterlaceMedian)(uint8_t src[], int stride)
@@ -3179,16 +3180,16 @@
 }
 #endif //HAVE_MMX
 
-static void RENAME(postProcess)(uint8_t src[], int srcStride, uint8_t dst[], int dstStride, int width, int height,
-        QP_STORE_T QPs[], int QPStride, int isColor, PPContext *c);
+static void RENAME(postProcess)(const uint8_t src[], int srcStride, uint8_t dst[], int dstStride, int width, int height,
+        const QP_STORE_T QPs[], int QPStride, int isColor, PPContext *c);
 
 /**
- * Copies a block from src to dst and fixes the blacklevel
- * levelFix == 0 -&gt; dont touch the brighness &amp; contrast
+ * Copies a block from src to dst and fixes the blacklevel.
+ * levelFix == 0 -&gt; do not touch the brighness &amp; contrast
  */
 #undef SCALED_CPY
 
-static inline void RENAME(blockCopy)(uint8_t dst[], int dstStride, uint8_t src[], int srcStride,
+static inline void RENAME(blockCopy)(uint8_t dst[], int dstStride, const uint8_t src[], int srcStride,
         int levelFix, int64_t *packedOffsetAndScale)
 {
 #ifndef HAVE_MMX
@@ -3345,10 +3346,10 @@
 /**
  * Filters array of bytes (Y or U or V values)
  */
-static void RENAME(postProcess)(uint8_t src[], int srcStride, uint8_t dst[], int dstStride, int width, int height,
-        QP_STORE_T QPs[], int QPStride, int isColor, PPContext *c2)
+static void RENAME(postProcess)(const uint8_t src[], int srcStride, uint8_t dst[], int dstStride, int width, int height,
+        const QP_STORE_T QPs[], int QPStride, int isColor, PPContext *c2)
 {
-        PPContext __attribute__((aligned(8))) c= *c2; //copy to stack for faster access
+        DECLARE_ALIGNED(8, PPContext, c)= *c2; //copy to stack for faster access
         int x,y;
 #ifdef COMPILE_TIME_MODE
         const int mode= COMPILE_TIME_MODE;
@@ -3415,7 +3416,7 @@
                         sum+= yHistogram[i];
                 }
 
-                /* we allways get a completly black picture first */
+                /* We always get a completely black picture first. */
                 maxClipped= (uint64_t)(sum * c.ppMode.maxClippedThreshold);
 
                 clipped= sum;
@@ -3461,10 +3462,10 @@
         /* copy &amp; deinterlace first row of blocks */
         y=-BLOCK_SIZE;
         {
-                uint8_t *srcBlock= &amp;(src[y*srcStride]);
+                const uint8_t *srcBlock= &amp;(src[y*srcStride]);
                 uint8_t *dstBlock= tempDst + dstStride;
 
-                // From this point on it is guranteed that we can read and write 16 lines downward
+                // From this point on it is guaranteed that we can read and write 16 lines downward
                 // finish 1 block before the next otherwise we might have a problem
                 // with the L1 Cache of the P4 ... or only a few blocks at a time or soemthing
                 for(x=0; x&lt;width; x+=BLOCK_SIZE)
@@ -3498,7 +3499,7 @@
                         );
 
 #elif defined(HAVE_3DNOW)
-//FIXME check if this is faster on an 3dnow chip or if its faster without the prefetch or ...
+//FIXME check if this is faster on an 3dnow chip or if it is faster without the prefetch or ...
 /*                        prefetch(srcBlock + (((x&gt;&gt;3)&amp;3) + 5)*srcStride + 32);
                         prefetch(srcBlock + (((x&gt;&gt;3)&amp;3) + 9)*srcStride + 32);
                         prefetchw(dstBlock + (((x&gt;&gt;3)&amp;3) + 5)*dstStride + 32);
@@ -3544,13 +3545,13 @@
         for(y=0; y&lt;height; y+=BLOCK_SIZE)
         {
                 //1% speedup if these are here instead of the inner loop
-                uint8_t *srcBlock= &amp;(src[y*srcStride]);
+                const uint8_t *srcBlock= &amp;(src[y*srcStride]);
                 uint8_t *dstBlock= &amp;(dst[y*dstStride]);
 #ifdef HAVE_MMX
                 uint8_t *tempBlock1= c.tempBlocks;
                 uint8_t *tempBlock2= c.tempBlocks + 8;
 #endif
-                int8_t *QPptr= &amp;QPs[(y&gt;&gt;qpVShift)*QPStride];
+                const int8_t *QPptr= &amp;QPs[(y&gt;&gt;qpVShift)*QPStride];
                 int8_t *nonBQPptr= &amp;c.nonBQPTable[(y&gt;&gt;qpVShift)*FFABS(QPStride)];
                 int QP=0;
                 /* can we mess with a 8x16 block from srcBlock/dstBlock downwards and 1 line upwards
@@ -3578,7 +3579,7 @@
                         srcBlock= tempSrc;
                 }
 
-                // From this point on it is guranteed that we can read and write 16 lines downward
+                // From this point on it is guaranteed that we can read and write 16 lines downward
                 // finish 1 block before the next otherwise we might have a problem
                 // with the L1 Cache of the P4 ... or only a few blocks at a time or soemthing
                 for(x=0; x&lt;width; x+=BLOCK_SIZE)
@@ -3642,7 +3643,7 @@
                         );
 
 #elif defined(HAVE_3DNOW)
-//FIXME check if this is faster on an 3dnow chip or if its faster without the prefetch or ...
+//FIXME check if this is faster on an 3dnow chip or if it is faster without the prefetch or ...
 /*                        prefetch(srcBlock + (((x&gt;&gt;3)&amp;3) + 5)*srcStride + 32);
                         prefetch(srcBlock + (((x&gt;&gt;3)&amp;3) + 9)*srcStride + 32);
                         prefetchw(dstBlock + (((x&gt;&gt;3)&amp;3) + 5)*dstStride + 32);
@@ -3717,7 +3718,7 @@
                                 else if(mode &amp; H_DEBLOCK)
                                 {
 #ifdef HAVE_ALTIVEC
-                                        unsigned char __attribute__ ((aligned(16))) tempBlock[272];
+                                        DECLARE_ALIGNED(16, unsigned char, tempBlock[272]);
                                         transpose_16x8_char_toPackedAlign_altivec(tempBlock, dstBlock - (4 + 1), stride);
 
                                         const int t=vertClassify_altivec(tempBlock-48, 16, &amp;c);

Modified: branches/avidemux_2.5_branch_gruntster/avidemux/ADM_libraries/ADM_ffmpeg/ADM_libswscale/rgb2rgb.c
===================================================================
--- branches/avidemux_2.5_branch_gruntster/avidemux/ADM_libraries/ADM_ffmpeg/ADM_libswscale/rgb2rgb.c	2008-03-21 06:48:49 UTC (rev 3911)
+++ branches/avidemux_2.5_branch_gruntster/avidemux/ADM_libraries/ADM_ffmpeg/ADM_libswscale/rgb2rgb.c	2008-03-21 06:51:32 UTC (rev 3912)
@@ -25,35 +25,28 @@
  * the C code (not assembly, mmx, ...) of this file can be used
  * under the LGPL license too
  */
-// MEANX
+#include &lt;inttypes.h&gt;
 #include &quot;config.h&quot;
-#include &quot;wrapper.h&quot;
-// /MEANX
-
 #include &quot;rgb2rgb.h&quot;
 #include &quot;swscale.h&quot;
 #include &quot;swscale_internal.h&quot;
 #include &quot;x86_cpu.h&quot;
 #include &quot;bswap.h&quot;
-// MEANX
 
-
-// /MEANX
-
 #define FAST_BGR2YV12 // use 7 bit coeffs instead of 15bit
 
-void (*rgb24to32)(const uint8_t *src,uint8_t *dst,long src_size);
-void (*rgb24to16)(const uint8_t *src,uint8_t *dst,long src_size);
-void (*rgb24to15)(const uint8_t *src,uint8_t *dst,long src_size);
-void (*rgb32to24)(const uint8_t *src,uint8_t *dst,long src_size);
-void (*rgb32to16)(const uint8_t *src,uint8_t *dst,long src_size);
-void (*rgb32to15)(const uint8_t *src,uint8_t *dst,long src_size);
-void (*rgb15to16)(const uint8_t *src,uint8_t *dst,long src_size);
-void (*rgb15to24)(const uint8_t *src,uint8_t *dst,long src_size);
-void (*rgb15to32)(const uint8_t *src,uint8_t *dst,long src_size);
-void (*rgb16to15)(const uint8_t *src,uint8_t *dst,long src_size);
-void (*rgb16to24)(const uint8_t *src,uint8_t *dst,long src_size);
-void (*rgb16to32)(const uint8_t *src,uint8_t *dst,long src_size);
+void (*rgb24to32)(const uint8_t *src, uint8_t *dst, long src_size);
+void (*rgb24to16)(const uint8_t *src, uint8_t *dst, long src_size);
+void (*rgb24to15)(const uint8_t *src, uint8_t *dst, long src_size);
+void (*rgb32to24)(const uint8_t *src, uint8_t *dst, long src_size);
+void (*rgb32to16)(const uint8_t *src, uint8_t *dst, long src_size);
+void (*rgb32to15)(const uint8_t *src, uint8_t *dst, long src_size);
+void (*rgb15to16)(const uint8_t *src, uint8_t *dst, long src_size);
+void (*rgb15to24)(const uint8_t *src, uint8_t *dst, long src_size);
+void (*rgb15to32)(const uint8_t *src, uint8_t *dst, long src_size);
+void (*rgb16to15)(const uint8_t *src, uint8_t *dst, long src_size);
+void (*rgb16to24)(const uint8_t *src, uint8_t *dst, long src_size);
+void (*rgb16to32)(const uint8_t *src, uint8_t *dst, long src_size);
 //void (*rgb24tobgr32)(const uint8_t *src, uint8_t *dst, long src_size);
 void (*rgb24tobgr24)(const uint8_t *src, uint8_t *dst, long src_size);
 void (*rgb24tobgr16)(const uint8_t *src, uint8_t *dst, long src_size);
@@ -95,53 +88,40 @@
                      long srcStride3, long dstStride);
 
 #if defined(ARCH_X86) &amp;&amp; defined(CONFIG_GPL)
-static const uint64_t mmx_null     __attribute__((aligned(8))) = 0x0000000000000000ULL;
-static const uint64_t mmx_one      __attribute__((aligned(8))) = 0xFFFFFFFFFFFFFFFFULL;
-static const uint64_t mask32b      attribute_used __attribute__((aligned(8))) = 0x000000FF000000FFULL;
-static const uint64_t mask32g      attribute_used __attribute__((aligned(8))) = 0x0000FF000000FF00ULL;
-static const uint64_t mask32r      attribute_used __attribute__((aligned(8))) = 0x00FF000000FF0000ULL;
-static const uint64_t mask32       __attribute__((aligned(8))) = 0x00FFFFFF00FFFFFFULL;
-static const uint64_t mask3216br   __attribute__((aligned(8))) = 0x00F800F800F800F8ULL;
-static const uint64_t mask3216g    __attribute__((aligned(8))) = 0x0000FC000000FC00ULL;
-static const uint64_t mask3215g    __attribute__((aligned(8))) = 0x0000F8000000F800ULL;
-static const uint64_t mul3216      __attribute__((aligned(8))) = 0x2000000420000004ULL;
-static const uint64_t mul3215      __attribute__((aligned(8))) = 0x2000000820000008ULL;
-static const uint64_t mask24b      attribute_used __attribute__((aligned(8))) = 0x00FF0000FF0000FFULL;
-static const uint64_t mask24g      attribute_used __attribute__((aligned(8))) = 0xFF0000FF0000FF00ULL;
-static const uint64_t mask24r      attribute_used __attribute__((aligned(8))) = 0x0000FF0000FF0000ULL;
-static const uint64_t mask24l      __attribute__((aligned(8))) = 0x0000000000FFFFFFULL;
-static const uint64_t mask24h      __attribute__((aligned(8))) = 0x0000FFFFFF000000ULL;
-static const uint64_t mask24hh     __attribute__((aligned(8))) = 0xffff000000000000ULL;
-static const uint64_t mask24hhh    __attribute__((aligned(8))) = 0xffffffff00000000ULL;
-static const uint64_t mask24hhhh   __attribute__((aligned(8))) = 0xffffffffffff0000ULL;
-static const uint64_t mask15b      __attribute__((aligned(8))) = 0x001F001F001F001FULL; /* 00000000 00011111  xxB */
-static const uint64_t mask15rg     __attribute__((aligned(8))) = 0x7FE07FE07FE07FE0ULL; /* 01111111 11100000  RGx */
-static const uint64_t mask15s      __attribute__((aligned(8))) = 0xFFE0FFE0FFE0FFE0ULL;
-static const uint64_t mask15g      __attribute__((aligned(8))) = 0x03E003E003E003E0ULL;
-static const uint64_t mask15r      __attribute__((aligned(8))) = 0x7C007C007C007C00ULL;
+DECLARE_ASM_CONST(8, uint64_t, mmx_null)     = 0x0000000000000000ULL;
+DECLARE_ASM_CONST(8, uint64_t, mmx_one)      = 0xFFFFFFFFFFFFFFFFULL;
+DECLARE_ASM_CONST(8, uint64_t, mask32b)      = 0x000000FF000000FFULL;
+DECLARE_ASM_CONST(8, uint64_t, mask32g)      = 0x0000FF000000FF00ULL;
+DECLARE_ASM_CONST(8, uint64_t, mask32r)      = 0x00FF000000FF0000ULL;
+DECLARE_ASM_CONST(8, uint64_t, mask32)       = 0x00FFFFFF00FFFFFFULL;
+DECLARE_ASM_CONST(8, uint64_t, mask3216br)   = 0x00F800F800F800F8ULL;
+DECLARE_ASM_CONST(8, uint64_t, mask3216g)    = 0x0000FC000000FC00ULL;
+DECLARE_ASM_CONST(8, uint64_t, mask3215g)    = 0x0000F8000000F800ULL;
+DECLARE_ASM_CONST(8, uint64_t, mul3216)      = 0x2000000420000004ULL;
+DECLARE_ASM_CONST(8, uint64_t, mul3215)      = 0x2000000820000008ULL;
+DECLARE_ASM_CONST(8, uint64_t, mask24b)      = 0x00FF0000FF0000FFULL;
+DECLARE_ASM_CONST(8, uint64_t, mask24g)      = 0xFF0000FF0000FF00ULL;
+DECLARE_ASM_CONST(8, uint64_t, mask24r)      = 0x0000FF0000FF0000ULL;
+DECLARE_ASM_CONST(8, uint64_t, mask24l)      = 0x0000000000FFFFFFULL;
+DECLARE_ASM_CONST(8, uint64_t, mask24h)      = 0x0000FFFFFF000000ULL;
+DECLARE_ASM_CONST(8, uint64_t, mask24hh)     = 0xffff000000000000ULL;
+DECLARE_ASM_CONST(8, uint64_t, mask24hhh)    = 0xffffffff00000000ULL;
+DECLARE_ASM_CONST(8, uint64_t, mask24hhhh)   = 0xffffffffffff0000ULL;
+DECLARE_ASM_CONST(8, uint64_t, mask15b)      = 0x001F001F001F001FULL; /* 00000000 00011111  xxB */
+DECLARE_ASM_CONST(8, uint64_t, mask15rg)     = 0x7FE07FE07FE07FE0ULL; /* 01111111 11100000  RGx */
+DECLARE_ASM_CONST(8, uint64_t, mask15s)      = 0xFFE0FFE0FFE0FFE0ULL;
+DECLARE_ASM_CONST(8, uint64_t, mask15g)      = 0x03E003E003E003E0ULL;
+DECLARE_ASM_CONST(8, uint64_t, mask15r)      = 0x7C007C007C007C00ULL;
 #define mask16b mask15b
-static const uint64_t mask16g      __attribute__((aligned(8))) = 0x07E007E007E007E0ULL;
-static const uint64_t mask16r      __attribute__((aligned(8))) = 0xF800F800F800F800ULL;
-static const uint64_t red_16mask   __attribute__((aligned(8))) = 0x0000f8000000f800ULL;
-static const uint64_t green_16mask __attribute__((aligned(8))) = 0x000007e0000007e0ULL;
-static const uint64_t blue_16mask  __attribute__((aligned(8))) = 0x0000001f0000001fULL;
-static const uint64_t red_15mask   __attribute__((aligned(8))) = 0x00007c0000007c00ULL;
-static const uint64_t green_15mask __attribute__((aligned(8))) = 0x000003e0000003e0ULL;
-static const uint64_t blue_15mask  __attribute__((aligned(8))) = 0x0000001f0000001fULL;
+DECLARE_ASM_CONST(8, uint64_t, mask16g)      = 0x07E007E007E007E0ULL;
+DECLARE_ASM_CONST(8, uint64_t, mask16r)      = 0xF800F800F800F800ULL;
+DECLARE_ASM_CONST(8, uint64_t, red_16mask)   = 0x0000f8000000f800ULL;
+DECLARE_ASM_CONST(8, uint64_t, green_16mask) = 0x000007e0000007e0ULL;
+DECLARE_ASM_CONST(8, uint64_t, blue_16mask)  = 0x0000001f0000001fULL;
+DECLARE_ASM_CONST(8, uint64_t, red_15mask)   = 0x00007c0000007c00ULL;
+DECLARE_ASM_CONST(8, uint64_t, green_15mask) = 0x000003e0000003e0ULL;
+DECLARE_ASM_CONST(8, uint64_t, blue_15mask)  = 0x0000001f0000001fULL;
 
-#ifdef FAST_BGR2YV12
-static const uint64_t bgr2YCoeff   attribute_used __attribute__((aligned(8))) = 0x000000210041000DULL;
-static const uint64_t bgr2UCoeff   attribute_used __attribute__((aligned(8))) = 0x0000FFEEFFDC0038ULL;
-static const uint64_t bgr2VCoeff   attribute_used __attribute__((aligned(8))) = 0x00000038FFD2FFF8ULL;
-#else
-static const uint64_t bgr2YCoeff   attribute_used __attribute__((aligned(8))) = 0x000020E540830C8BULL;
-static const uint64_t bgr2UCoeff   attribute_used __attribute__((aligned(8))) = 0x0000ED0FDAC23831ULL;
-static const uint64_t bgr2VCoeff   attribute_used __attribute__((aligned(8))) = 0x00003831D0E6F6EAULL;
-#endif
-static const uint64_t bgr2YOffset  attribute_used __attribute__((aligned(8))) = 0x1010101010101010ULL;
-static const uint64_t bgr2UVOffset attribute_used __attribute__((aligned(8))) = 0x8080808080808080ULL;
-static const uint64_t w1111        attribute_used __attribute__((aligned(8))) = 0x0001000100010001ULL;
-
 #if 0
 static volatile uint64_t __attribute__((aligned(8))) b5Dither;
 static volatile uint64_t __attribute__((aligned(8))) g5Dither;
@@ -238,20 +218,20 @@
 
 /*
     for (i=0; i&lt;num_pixels; i++)
-        ((unsigned *)dst)[i] = ((unsigned *)palette)[ src[i] ];
+        ((unsigned *)dst)[i] = ((unsigned *)palette)[src[i]];
 */
 
     for (i=0; i&lt;num_pixels; i++)
     {
         #ifdef WORDS_BIGENDIAN
-            dst[3]= palette[ src[i]*4+2 ];
-            dst[2]= palette[ src[i]*4+1 ];
-            dst[1]= palette[ src[i]*4+0 ];
+            dst[3]= palette[src[i]*4+2];
+            dst[2]= palette[src[i]*4+1];
+            dst[1]= palette[src[i]*4+0];
         #else
         //FIXME slow?
-            dst[0]= palette[ src[i]*4+2 ];
-            dst[1]= palette[ src[i]*4+1 ];
-            dst[2]= palette[ src[i]*4+0 ];
+            dst[0]= palette[src[i]*4+2];
+            dst[1]= palette[src[i]*4+1];
+            dst[2]= palette[src[i]*4+0];
             //dst[3]= 0; /* do we need this cleansing? */
         #endif
         dst+= 4;
@@ -264,14 +244,14 @@
     for (i=0; i&lt;num_pixels; i++)
     {
         #ifdef WORDS_BIGENDIAN
-            dst[3]= palette[ src[i]*4+0 ];
-            dst[2]= palette[ src[i]*4+1 ];
-            dst[1]= palette[ src[i]*4+2 ];
+            dst[3]= palette[src[i]*4+0];
+            dst[2]= palette[src[i]*4+1];
+            dst[1]= palette[src[i]*4+2];
         #else
             //FIXME slow?
-            dst[0]= palette[ src[i]*4+0 ];
-            dst[1]= palette[ src[i]*4+1 ];
-            dst[2]= palette[ src[i]*4+2 ];
+            dst[0]= palette[src[i]*4+0];
+            dst[1]= palette[src[i]*4+1];
+            dst[2]= palette[src[i]*4+2];
             //dst[3]= 0; /* do we need this cleansing? */
         #endif
 
@@ -288,14 +268,14 @@
 /*
     writes 1 byte o much and might cause alignment issues on some architectures?
     for (i=0; i&lt;num_pixels; i++)
-        ((unsigned *)(&amp;dst[i*3])) = ((unsigned *)palette)[ src[i] ];
+        ((unsigned *)(&amp;dst[i*3])) = ((unsigned *)palette)[src[i]];
 */
     for (i=0; i&lt;num_pixels; i++)
     {
         //FIXME slow?
-        dst[0]= palette[ src[i]*4+2 ];
-        dst[1]= palette[ src[i]*4+1 ];
-        dst[2]= palette[ src[i]*4+0 ];
+        dst[0]= palette[src[i]*4+2];
+        dst[1]= palette[src[i]*4+1];
+        dst[2]= palette[src[i]*4+0];
         dst+= 3;
     }
 }
@@ -306,14 +286,14 @@
 /*
     writes 1 byte o much and might cause alignment issues on some architectures?
     for (i=0; i&lt;num_pixels; i++)
-        ((unsigned *)(&amp;dst[i*3])) = ((unsigned *)palette)[ src[i] ];
+        ((unsigned *)(&amp;dst[i*3])) = ((unsigned *)palette)[src[i]];
 */
     for (i=0; i&lt;num_pixels; i++)
     {
         //FIXME slow?
-        dst[0]= palette[ src[i]*4+0 ];
-        dst[1]= palette[ src[i]*4+1 ];
-        dst[2]= palette[ src[i]*4+2 ];
+        dst[0]= palette[src[i]*4+0];
+        dst[1]= palette[src[i]*4+1];
+        dst[2]= palette[src[i]*4+2];
         dst+= 3;
     }
 }
@@ -325,13 +305,13 @@
 {
     long i;
     for (i=0; i&lt;num_pixels; i++)
-        ((uint16_t *)dst)[i] = ((uint16_t *)palette)[ src[i] ];
+        ((uint16_t *)dst)[i] = ((uint16_t *)palette)[src[i]];
 }
 void palette8tobgr16(const uint8_t *src, uint8_t *dst, long num_pixels, const uint8_t *palette)
 {
     long i;
     for (i=0; i&lt;num_pixels; i++)
-        ((uint16_t *)dst)[i] = bswap_16(((uint16_t *)palette)[ src[i] ]);
+        ((uint16_t *)dst)[i] = bswap_16(((uint16_t *)palette)[src[i]]);
 }
 
 /**
@@ -341,13 +321,13 @@
 {
     long i;
     for (i=0; i&lt;num_pixels; i++)
-        ((uint16_t *)dst)[i] = ((uint16_t *)palette)[ src[i] ];
+        ((uint16_t *)dst)[i] = ((uint16_t *)palette)[src[i]];
 }
 void palette8tobgr15(const uint8_t *src, uint8_t *dst, long num_pixels, const uint8_t *palette)
 {
     long i;
     for (i=0; i&lt;num_pixels; i++)
-        ((uint16_t *)dst)[i] = bswap_16(((uint16_t *)palette)[ src[i] ]);
+        ((uint16_t *)dst)[i] = bswap_16(((uint16_t *)palette)[src[i]]);
 }
 
 void rgb32tobgr24(const uint8_t *src, uint8_t *dst, long src_size)

Modified: branches/avidemux_2.5_branch_gruntster/avidemux/ADM_libraries/ADM_ffmpeg/ADM_libswscale/rgb2rgb.h
===================================================================
--- branches/avidemux_2.5_branch_gruntster/avidemux/ADM_libraries/ADM_ffmpeg/ADM_libswscale/rgb2rgb.h	2008-03-21 06:48:49 UTC (rev 3911)
+++ branches/avidemux_2.5_branch_gruntster/avidemux/ADM_libraries/ADM_ffmpeg/ADM_libswscale/rgb2rgb.h	2008-03-21 06:51:32 UTC (rev 3912)
@@ -23,8 +23,8 @@
  * Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA
  */
 
-#ifndef RGB2RGB_INCLUDED
-#define RGB2RGB_INCLUDED
+#ifndef FFMPEG_RGB2RGB_H
+#define FFMPEG_RGB2RGB_H
 
 #include &lt;inttypes.h&gt;
 
@@ -143,4 +143,4 @@
 
 void sws_rgb2rgb_init(int flags);
 
-#endif
+#endif /* FFMPEG_RGB2RGB_H */

Modified: branches/avidemux_2.5_branch_gruntster/avidemux/ADM_libraries/ADM_ffmpeg/ADM_libswscale/rgb2rgb_template.c
===================================================================
--- branches/avidemux_2.5_branch_gruntster/avidemux/ADM_libraries/ADM_ffmpeg/ADM_libswscale/rgb2rgb_template.c	2008-03-21 06:48:49 UTC (rev 3911)
+++ branches/avidemux_2.5_branch_gruntster/avidemux/ADM_libraries/ADM_ffmpeg/ADM_libswscale/rgb2rgb_template.c	2008-03-21 06:51:32 UTC (rev 3912)
@@ -23,15 +23,15 @@
  * along with FFmpeg; if not, write to the Free Software
  * Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA
  *
- * the C code (not assembly, mmx, ...) of this file can be used
- * under the LGPL license too
+ * The C code (not assembly, mmx, ...) of this file can be used
+ * under the LGPL license.
  */
 
 #include &lt;stddef.h&gt;
 #include &lt;inttypes.h&gt; /* for __WORDSIZE */
 
 #ifndef __WORDSIZE
-// #warning You have misconfigured system and probably will lose performance!
+// #warning You have a misconfigured system and will probably lose performance!
 #define __WORDSIZE MP_WORDSIZE
 #endif
 
@@ -53,7 +53,7 @@
 #define PREFETCH  &quot;prefetch&quot;
 #define PREFETCHW &quot;prefetchw&quot;
 #define PAVGB     &quot;pavgusb&quot;
-#elif defined ( HAVE_MMX2 )
+#elif defined (HAVE_MMX2)
 #define PREFETCH &quot;prefetchnta&quot;
 #define PREFETCHW &quot;prefetcht0&quot;
 #define PAVGB     &quot;pavgb&quot;
@@ -68,7 +68,7 @@
 #endif
 
 #ifdef HAVE_3DNOW
-/* On K6 femms is faster of emms. On K7 femms is directly mapped on emms. */
+/* On K6 femms is faster than emms. On K7 femms is directly mapped on emms. */
 #define EMMS     &quot;femms&quot;
 #else
 #define EMMS     &quot;emms&quot;
@@ -82,7 +82,7 @@
 #define SFENCE &quot; # nop&quot;
 #endif
 
-static inline void RENAME(rgb24to32)(const uint8_t *src,uint8_t *dst,long src_size)
+static inline void RENAME(rgb24to32)(const uint8_t *src, uint8_t *dst, long src_size)
 {
     uint8_t *dest = dst;
     const uint8_t *s = src;
@@ -92,12 +92,12 @@
     #endif
     end = s + src_size;
     #ifdef HAVE_MMX
-        __asm __volatile(PREFETCH&quot;    %0&quot;::&quot;m&quot;(*s):&quot;memory&quot;);
+        asm volatile(PREFETCH&quot;    %0&quot;::&quot;m&quot;(*s):&quot;memory&quot;);
         mm_end = end - 23;
-        __asm __volatile(&quot;movq        %0, %%mm7&quot;::&quot;m&quot;(mask32):&quot;memory&quot;);
+        asm volatile(&quot;movq        %0, %%mm7&quot;::&quot;m&quot;(mask32):&quot;memory&quot;);
         while (s &lt; mm_end)
         {
-            __asm __volatile(
+            asm volatile(
             PREFETCH&quot;    32%1           \n\t&quot;
             &quot;movd          %1, %%mm0    \n\t&quot;
             &quot;punpckldq    3%1, %%mm0    \n\t&quot;
@@ -121,8 +121,8 @@
             dest += 32;
             s += 24;
         }
-        __asm __volatile(SFENCE:::&quot;memory&quot;);
-        __asm __volatile(EMMS:::&quot;memory&quot;);
+        asm volatile(SFENCE:::&quot;memory&quot;);
+        asm volatile(EMMS:::&quot;memory&quot;);
     #endif
     while (s &lt; end)
     {
@@ -142,7 +142,7 @@
     }
 }
 
-static inline void RENAME(rgb32to24)(const uint8_t *src,uint8_t *dst,long src_size)
+static inline void RENAME(rgb32to24)(const uint8_t *src, uint8_t *dst, long src_size)
 {
     uint8_t *dest = dst;
     const uint8_t *s = src;
@@ -152,11 +152,11 @@
 #endif
     end = s + src_size;
 #ifdef HAVE_MMX
-    __asm __volatile(PREFETCH&quot;    %0&quot;::&quot;m&quot;(*s):&quot;memory&quot;);
+    asm volatile(PREFETCH&quot;    %0&quot;::&quot;m&quot;(*s):&quot;memory&quot;);
     mm_end = end - 31;
     while (s &lt; mm_end)
     {
-        __asm __volatile(
+        asm volatile(
         PREFETCH&quot;    32%1           \n\t&quot;
         &quot;movq          %1, %%mm0    \n\t&quot;
         &quot;movq         8%1, %%mm1    \n\t&quot;
@@ -207,8 +207,8 @@
         dest += 24;
         s += 32;
     }
-    __asm __volatile(SFENCE:::&quot;memory&quot;);
-    __asm __volatile(EMMS:::&quot;memory&quot;);
+    asm volatile(SFENCE:::&quot;memory&quot;);
+    asm volatile(EMMS:::&quot;memory&quot;);
 #endif
     while (s &lt; end)
     {
@@ -232,9 +232,9 @@
  Original by Strepto/Astral
  ported to gcc &amp; bugfixed : A'rpi
  MMX2, 3DNOW optimization by Nick Kurshev
- 32bit c version, and and&amp;add trick by Michael Niedermayer
+ 32 bit C version, and and&amp;add trick by Michael Niedermayer
 */
-static inline void RENAME(rgb15to16)(const uint8_t *src,uint8_t *dst,long src_size)
+static inline void RENAME(rgb15to16)(const uint8_t *src, uint8_t *dst, long src_size)
 {
     register const uint8_t* s=src;
     register uint8_t* d=dst;
@@ -242,12 +242,12 @@
     const uint8_t *mm_end;
     end = s + src_size;
 #ifdef HAVE_MMX
-    __asm __volatile(PREFETCH&quot;    %0&quot;::&quot;m&quot;(*s));
-    __asm __volatile(&quot;movq        %0, %%mm4&quot;::&quot;m&quot;(mask15s));
+    asm volatile(PREFETCH&quot;    %0&quot;::&quot;m&quot;(*s));
+    asm volatile(&quot;movq        %0, %%mm4&quot;::&quot;m&quot;(mask15s));
     mm_end = end - 15;
     while (s&lt;mm_end)
     {
-        __asm __volatile(
+        asm volatile(
         PREFETCH&quot;  32%1         \n\t&quot;
         &quot;movq        %1, %%mm0  \n\t&quot;
         &quot;movq       8%1, %%mm2  \n\t&quot;
@@ -265,8 +265,8 @@
         d+=16;
         s+=16;
     }
-    __asm __volatile(SFENCE:::&quot;memory&quot;);
-    __asm __volatile(EMMS:::&quot;memory&quot;);
+    asm volatile(SFENCE:::&quot;memory&quot;);
+    asm volatile(EMMS:::&quot;memory&quot;);
 #endif
     mm_end = end - 3;
     while (s &lt; mm_end)
@@ -283,7 +283,7 @@
     }
 }
 
-static inline void RENAME(rgb16to15)(const uint8_t *src,uint8_t *dst,long src_size)
+static inline void RENAME(rgb16to15)(const uint8_t *src, uint8_t *dst, long src_size)
 {
     register const uint8_t* s=src;
     register uint8_t* d=dst;
@@ -291,13 +291,13 @@
     const uint8_t *mm_end;
     end = s + src_size;
 #ifdef HAVE_MMX
-    __asm __volatile(PREFETCH&quot;    %0&quot;::&quot;m&quot;(*s));
-    __asm __volatile(&quot;movq        %0, %%mm7&quot;::&quot;m&quot;(mask15rg));
-    __asm __volatile(&quot;movq        %0, %%mm6&quot;::&quot;m&quot;(mask15b));
+    asm volatile(PREFETCH&quot;    %0&quot;::&quot;m&quot;(*s));
+    asm volatile(&quot;movq        %0, %%mm7&quot;::&quot;m&quot;(mask15rg));
+    asm volatile(&quot;movq        %0, %%mm6&quot;::&quot;m&quot;(mask15b));
     mm_end = end - 15;
     while (s&lt;mm_end)
     {
-        __asm __volatile(
+        asm volatile(
         PREFETCH&quot;  32%1         \n\t&quot;
         &quot;movq        %1, %%mm0  \n\t&quot;
         &quot;movq       8%1, %%mm2  \n\t&quot;
@@ -319,8 +319,8 @@
         d+=16;
         s+=16;
     }
-    __asm __volatile(SFENCE:::&quot;memory&quot;);
-    __asm __volatile(EMMS:::&quot;memory&quot;);
+    asm volatile(SFENCE:::&quot;memory&quot;);
+    asm volatile(EMMS:::&quot;memory&quot;);
 #endif
     mm_end = end - 3;
     while (s &lt; mm_end)
@@ -350,7 +350,7 @@
     end = s + src_size;
 #ifdef HAVE_MMX
     mm_end = end - 15;
-#if 1 //is faster only if multiplies are reasonable fast (FIXME figure out on which CPUs this is faster, on Athlon it is slightly faster)
+#if 1 //is faster only if multiplies are reasonably fast (FIXME figure out on which CPUs this is faster, on Athlon it is slightly faster)
     asm volatile(
     &quot;movq           %3, %%mm5   \n\t&quot;
     &quot;movq           %4, %%mm6   \n\t&quot;
@@ -386,14 +386,14 @@
     : &quot;r&quot; (mm_end), &quot;m&quot; (mask3216g), &quot;m&quot; (mask3216br), &quot;m&quot; (mul3216)
     );
 #else
-    __asm __volatile(PREFETCH&quot;    %0&quot;::&quot;m&quot;(*src):&quot;memory&quot;);
-    __asm __volatile(
+    asm volatile(PREFETCH&quot;    %0&quot;::&quot;m&quot;(*src):&quot;memory&quot;);
+    asm volatile(
         &quot;movq    %0, %%mm7    \n\t&quot;
         &quot;movq    %1, %%mm6    \n\t&quot;
         ::&quot;m&quot;(red_16mask),&quot;m&quot;(green_16mask));
     while (s &lt; mm_end)
     {
-        __asm __volatile(
+        asm volatile(
         PREFETCH&quot;    32%1           \n\t&quot;
         &quot;movd          %1, %%mm0    \n\t&quot;
         &quot;movd         4%1, %%mm3    \n\t&quot;
@@ -427,8 +427,8 @@
         s += 16;
     }
 #endif
-    __asm __volatile(SFENCE:::&quot;memory&quot;);
-    __asm __volatile(EMMS:::&quot;memory&quot;);
+    asm volatile(SFENCE:::&quot;memory&quot;);
+    asm volatile(EMMS:::&quot;memory&quot;);
 #endif
     while (s &lt; end)
     {
@@ -447,15 +447,15 @@
     uint16_t *d = (uint16_t *)dst;
     end = s + src_size;
 #ifdef HAVE_MMX
-    __asm __volatile(PREFETCH&quot;    %0&quot;::&quot;m&quot;(*src):&quot;memory&quot;);
-    __asm __volatile(
+    asm volatile(PREFETCH&quot;    %0&quot;::&quot;m&quot;(*src):&quot;memory&quot;);
+    asm volatile(
         &quot;movq          %0, %%mm7    \n\t&quot;
         &quot;movq          %1, %%mm6    \n\t&quot;
         ::&quot;m&quot;(red_16mask),&quot;m&quot;(green_16mask));
     mm_end = end - 15;
     while (s &lt; mm_end)
     {
-        __asm __volatile(
+        asm volatile(
         PREFETCH&quot;    32%1           \n\t&quot;
         &quot;movd          %1, %%mm0    \n\t&quot;
         &quot;movd         4%1, %%mm3    \n\t&quot;
@@ -488,8 +488,8 @@
         d += 4;
         s += 16;
     }
-    __asm __volatile(SFENCE:::&quot;memory&quot;);
-    __asm __volatile(EMMS:::&quot;memory&quot;);
+    asm volatile(SFENCE:::&quot;memory&quot;);
+    asm volatile(EMMS:::&quot;memory&quot;);
 #endif
     while (s &lt; end)
     {
@@ -509,7 +509,7 @@
     end = s + src_size;
 #ifdef HAVE_MMX
     mm_end = end - 15;
-#if 1 //is faster only if multiplies are reasonable fast (FIXME figure out on which CPUs this is faster, on Athlon it is slightly faster)
+#if 1 //is faster only if multiplies are reasonably fast (FIXME figure out on which CPUs this is faster, on Athlon it is slightly faster)
     asm volatile(
     &quot;movq           %3, %%mm5   \n\t&quot;
     &quot;movq           %4, %%mm6   \n\t&quot;
@@ -545,14 +545,14 @@
     : &quot;r&quot; (mm_end), &quot;m&quot; (mask3215g), &quot;m&quot; (mask3216br), &quot;m&quot; (mul3215)
     );
 #else
-    __asm __volatile(PREFETCH&quot;    %0&quot;::&quot;m&quot;(*src):&quot;memory&quot;);
-    __asm __volatile(
+    asm volatile(PREFETCH&quot;    %0&quot;::&quot;m&quot;(*src):&quot;memory&quot;);
+    asm volatile(
         &quot;movq          %0, %%mm7    \n\t&quot;
         &quot;movq          %1, %%mm6    \n\t&quot;
         ::&quot;m&quot;(red_15mask),&quot;m&quot;(green_15mask));
     while (s &lt; mm_end)
     {
-        __asm __volatile(
+        asm volatile(
         PREFETCH&quot;    32%1           \n\t&quot;
         &quot;movd          %1, %%mm0    \n\t&quot;
         &quot;movd         4%1, %%mm3    \n\t&quot;
@@ -586,8 +586,8 @@
         s += 16;
     }
 #endif
-    __asm __volatile(SFENCE:::&quot;memory&quot;);
-    __asm __volatile(EMMS:::&quot;memory&quot;);
+    asm volatile(SFENCE:::&quot;memory&quot;);
+    asm volatile(EMMS:::&quot;memory&quot;);
 #endif
     while (s &lt; end)
     {
@@ -606,15 +606,15 @@
     uint16_t *d = (uint16_t *)dst;
     end = s + src_size;
 #ifdef HAVE_MMX
-    __asm __volatile(PREFETCH&quot;    %0&quot;::&quot;m&quot;(*src):&quot;memory&quot;);
-    __asm __volatile(
+    asm volatile(PREFETCH&quot;    %0&quot;::&quot;m&quot;(*src):&quot;memory&quot;);
+    asm volatile(
         &quot;movq          %0, %%mm7    \n\t&quot;
         &quot;movq          %1, %%mm6    \n\t&quot;
         ::&quot;m&quot;(red_15mask),&quot;m&quot;(green_15mask));
     mm_end = end - 15;
     while (s &lt; mm_end)
     {
-        __asm __volatile(
+        asm volatile(
         PREFETCH&quot;    32%1           \n\t&quot;
         &quot;movd          %1, %%mm0    \n\t&quot;
         &quot;movd         4%1, %%mm3    \n\t&quot;
@@ -647,8 +647,8 @@
         d += 4;
         s += 16;
     }
-    __asm __volatile(SFENCE:::&quot;memory&quot;);
-    __asm __volatile(EMMS:::&quot;memory&quot;);
+    asm volatile(SFENCE:::&quot;memory&quot;);
+    asm volatile(EMMS:::&quot;memory&quot;);
 #endif
     while (s &lt; end)
     {
@@ -667,15 +667,15 @@
     uint16_t *d = (uint16_t *)dst;
     end = s + src_size;
 #ifdef HAVE_MMX
-    __asm __volatile(PREFETCH&quot;    %0&quot;::&quot;m&quot;(*src):&quot;memory&quot;);
-    __asm __volatile(
+    asm volatile(PREFETCH&quot;    %0&quot;::&quot;m&quot;(*src):&quot;memory&quot;);
+    asm volatile(
         &quot;movq         %0, %%mm7     \n\t&quot;
         &quot;movq         %1, %%mm6     \n\t&quot;
         ::&quot;m&quot;(red_16mask),&quot;m&quot;(green_16mask));
     mm_end = end - 11;
     while (s &lt; mm_end)
     {
-        __asm __volatile(
+        asm volatile(
         PREFETCH&quot;    32%1           \n\t&quot;
         &quot;movd          %1, %%mm0    \n\t&quot;
         &quot;movd         3%1, %%mm3    \n\t&quot;
@@ -708,8 +708,8 @@
         d += 4;
         s += 12;
     }
-    __asm __volatile(SFENCE:::&quot;memory&quot;);
-    __asm __volatile(EMMS:::&quot;memory&quot;);
+    asm volatile(SFENCE:::&quot;memory&quot;);
+    asm volatile(EMMS:::&quot;memory&quot;);
 #endif
     while (s &lt; end)
     {
@@ -730,15 +730,15 @@
     uint16_t *d = (uint16_t *)dst;
     end = s + src_size;
 #ifdef HAVE_MMX
-    __asm __volatile(PREFETCH&quot;    %0&quot;::&quot;m&quot;(*src):&quot;memory&quot;);
-    __asm __volatile(
+    asm volatile(PREFETCH&quot;    %0&quot;::&quot;m&quot;(*src):&quot;memory&quot;);
+    asm volatile(
         &quot;movq         %0, %%mm7     \n\t&quot;
         &quot;movq         %1, %%mm6     \n\t&quot;
         ::&quot;m&quot;(red_16mask),&quot;m&quot;(green_16mask));
     mm_end = end - 15;
     while (s &lt; mm_end)
     {
-        __asm __volatile(
+        asm volatile(
         PREFETCH&quot;    32%1           \n\t&quot;
         &quot;movd          %1, %%mm0    \n\t&quot;
         &quot;movd         3%1, %%mm3    \n\t&quot;
@@ -771,8 +771,8 @@
         d += 4;
         s += 12;
     }
-    __asm __volatile(SFENCE:::&quot;memory&quot;);
-    __asm __volatile(EMMS:::&quot;memory&quot;);
+    asm volatile(SFENCE:::&quot;memory&quot;);
+    asm volatile(EMMS:::&quot;memory&quot;);
 #endif
     while (s &lt; end)
     {
@@ -793,15 +793,15 @@
     uint16_t *d = (uint16_t *)dst;
     end = s + src_size;
 #ifdef HAVE_MMX
-    __asm __volatile(PREFETCH&quot;    %0&quot;::&quot;m&quot;(*src):&quot;memory&quot;);
-    __asm __volatile(
+    asm volatile(PREFETCH&quot;    %0&quot;::&quot;m&quot;(*src):&quot;memory&quot;);
+    asm volatile(
         &quot;movq          %0, %%mm7    \n\t&quot;
         &quot;movq          %1, %%mm6    \n\t&quot;
         ::&quot;m&quot;(red_15mask),&quot;m&quot;(green_15mask));
     mm_end = end - 11;
     while (s &lt; mm_end)
     {
-        __asm __volatile(
+        asm volatile(
         PREFETCH&quot;    32%1           \n\t&quot;
         &quot;movd          %1, %%mm0    \n\t&quot;
         &quot;movd         3%1, %%mm3    \n\t&quot;
@@ -834,8 +834,8 @@
         d += 4;
         s += 12;
     }
-    __asm __volatile(SFENCE:::&quot;memory&quot;);
-    __asm __volatile(EMMS:::&quot;memory&quot;);
+    asm volatile(SFENCE:::&quot;memory&quot;);
+    asm volatile(EMMS:::&quot;memory&quot;);
 #endif
     while (s &lt; end)
     {
@@ -856,15 +856,15 @@
     uint16_t *d = (uint16_t *)dst;
     end = s + src_size;
 #ifdef HAVE_MMX
-    __asm __volatile(PREFETCH&quot;    %0&quot;::&quot;m&quot;(*src):&quot;memory&quot;);
-    __asm __volatile(
+    asm volatile(PREFETCH&quot;    %0&quot;::&quot;m&quot;(*src):&quot;memory&quot;);
+    asm volatile(
         &quot;movq         %0, %%mm7     \n\t&quot;
         &quot;movq         %1, %%mm6     \n\t&quot;
         ::&quot;m&quot;(red_15mask),&quot;m&quot;(green_15mask));
     mm_end = end - 15;
     while (s &lt; mm_end)
     {
-        __asm __volatile(
+        asm volatile(
         PREFETCH&quot;   32%1            \n\t&quot;
         &quot;movd         %1, %%mm0     \n\t&quot;
         &quot;movd        3%1, %%mm3     \n\t&quot;
@@ -897,8 +897,8 @@
         d += 4;
         s += 12;
     }
-    __asm __volatile(SFENCE:::&quot;memory&quot;);
-    __asm __volatile(EMMS:::&quot;memory&quot;);
+    asm volatile(SFENCE:::&quot;memory&quot;);
+    asm volatile(EMMS:::&quot;memory&quot;);
 #endif
     while (s &lt; end)
     {
@@ -910,12 +910,10 @@
 }
 
 /*
-  I use here less accurate approximation by simply
- left-shifting the input
-  value and filling the low order bits with
- zeroes. This method improves png's
-  compression but this scheme cannot reproduce white exactly, since it does not
-  generate an all-ones maximum value; the net effect is to darken the
+  I use less accurate approximation here by simply left-shifting the input
+  value and filling the low order bits with zeroes. This method improves PNG
+  compression but this scheme cannot reproduce white exactly, since it does
+  not generate an all-ones maximum value; the net effect is to darken the
   image slightly.
 
   The better method should be &quot;left bit replication&quot;:
@@ -942,11 +940,11 @@
     const uint16_t *s = (uint16_t *)src;
     end = s + src_size/2;
 #ifdef HAVE_MMX
-    __asm __volatile(PREFETCH&quot;    %0&quot;::&quot;m&quot;(*s):&quot;memory&quot;);
+    asm volatile(PREFETCH&quot;    %0&quot;::&quot;m&quot;(*s):&quot;memory&quot;);
     mm_end = end - 7;
     while (s &lt; mm_end)
     {
-        __asm __volatile(
+        asm volatile(
         PREFETCH&quot;    32%1           \n\t&quot;
         &quot;movq          %1, %%mm0    \n\t&quot;
         &quot;movq          %1, %%mm1    \n\t&quot;
@@ -1009,7 +1007,7 @@
         :&quot;m&quot;(*s),&quot;m&quot;(mask15b),&quot;m&quot;(mask15g),&quot;m&quot;(mask15r), &quot;m&quot;(mmx_null)
         :&quot;memory&quot;);
         /* Borrowed 32 to 24 */
-        __asm __volatile(
+        asm volatile(
         &quot;movq       %%mm0, %%mm4    \n\t&quot;
         &quot;movq       %%mm3, %%mm5    \n\t&quot;
         &quot;movq       %%mm6, %%mm0    \n\t&quot;
@@ -1061,8 +1059,8 @@
         d += 24;
         s += 8;
     }
-    __asm __volatile(SFENCE:::&quot;memory&quot;);
-    __asm __volatile(EMMS:::&quot;memory&quot;);
+    asm volatile(SFENCE:::&quot;memory&quot;);
+    asm volatile(EMMS:::&quot;memory&quot;);
 #endif
     while (s &lt; end)
     {
@@ -1084,11 +1082,11 @@
     const uint16_t *s = (const uint16_t *)src;
     end = s + src_size/2;
 #ifdef HAVE_MMX
-    __asm __volatile(PREFETCH&quot;    %0&quot;::&quot;m&quot;(*s):&quot;memory&quot;);
+    asm volatile(PREFETCH&quot;    %0&quot;::&quot;m&quot;(*s):&quot;memory&quot;);
     mm_end = end - 7;
     while (s &lt; mm_end)
     {
-        __asm __volatile(
+        asm volatile(
         PREFETCH&quot;    32%1           \n\t&quot;
         &quot;movq          %1, %%mm0    \n\t&quot;
         &quot;movq          %1, %%mm1    \n\t&quot;
@@ -1150,7 +1148,7 @@
         :&quot;m&quot;(*s),&quot;m&quot;(mask16b),&quot;m&quot;(mask16g),&quot;m&quot;(mask16r),&quot;m&quot;(mmx_null)
         :&quot;memory&quot;);
         /* Borrowed 32 to 24 */
-        __asm __volatile(
+        asm volatile(
         &quot;movq       %%mm0, %%mm4    \n\t&quot;
         &quot;movq       %%mm3, %%mm5    \n\t&quot;
         &quot;movq       %%mm6, %%mm0    \n\t&quot;
@@ -1202,8 +1200,8 @@
         d += 24;
         s += 8;
     }
-    __asm __volatile(SFENCE:::&quot;memory&quot;);
-    __asm __volatile(EMMS:::&quot;memory&quot;);
+    asm volatile(SFENCE:::&quot;memory&quot;);
+    asm volatile(EMMS:::&quot;memory&quot;);
 #endif
     while (s &lt; end)
     {
@@ -1225,12 +1223,12 @@
     const uint16_t *s = (const uint16_t *)src;
     end = s + src_size/2;
 #ifdef HAVE_MMX
-    __asm __volatile(PREFETCH&quot;    %0&quot;::&quot;m&quot;(*s):&quot;memory&quot;);
-    __asm __volatile(&quot;pxor    %%mm7,%%mm7    \n\t&quot;:::&quot;memory&quot;);
+    asm volatile(PREFETCH&quot;    %0&quot;::&quot;m&quot;(*s):&quot;memory&quot;);
+    asm volatile(&quot;pxor    %%mm7,%%mm7    \n\t&quot;:::&quot;memory&quot;);
     mm_end = end - 3;
     while (s &lt; mm_end)
     {
-        __asm __volatile(
+        asm volatile(
         PREFETCH&quot;    32%1           \n\t&quot;
         &quot;movq          %1, %%mm0    \n\t&quot;
         &quot;movq          %1, %%mm1    \n\t&quot;
@@ -1266,12 +1264,12 @@
         d += 16;
         s += 4;
     }
-    __asm __volatile(SFENCE:::&quot;memory&quot;);
-    __asm __volatile(EMMS:::&quot;memory&quot;);
+    asm volatile(SFENCE:::&quot;memory&quot;);
+    asm volatile(EMMS:::&quot;memory&quot;);
 #endif
     while (s &lt; end)
     {
-#if 0 //slightly slower on athlon
+#if 0 //slightly slower on Athlon
         int bgr= *s++;
         *((uint32_t*)d)++ = ((bgr&amp;0x1F)&lt;&lt;3) + ((bgr&amp;0x3E0)&lt;&lt;6) + ((bgr&amp;0x7C00)&lt;&lt;9);
 #else
@@ -1303,12 +1301,12 @@
     const uint16_t *s = (uint16_t *)src;
     end = s + src_size/2;
 #ifdef HAVE_MMX
-    __asm __volatile(PREFETCH&quot;    %0&quot;::&quot;m&quot;(*s):&quot;memory&quot;);
-    __asm __volatile(&quot;pxor    %%mm7,%%mm7    \n\t&quot;:::&quot;memory&quot;);
+    asm volatile(PREFETCH&quot;    %0&quot;::&quot;m&quot;(*s):&quot;memory&quot;);
+    asm volatile(&quot;pxor    %%mm7,%%mm7    \n\t&quot;:::&quot;memory&quot;);
     mm_end = end - 3;
     while (s &lt; mm_end)
     {
-        __asm __volatile(
+        asm volatile(
         PREFETCH&quot;    32%1           \n\t&quot;
         &quot;movq          %1, %%mm0    \n\t&quot;
         &quot;movq          %1, %%mm1    \n\t&quot;
@@ -1344,8 +1342,8 @@
         d += 16;
         s += 4;
     }
-    __asm __volatile(SFENCE:::&quot;memory&quot;);
-    __asm __volatile(EMMS:::&quot;memory&quot;);
+    asm volatile(SFENCE:::&quot;memory&quot;);
+    asm volatile(EMMS:::&quot;memory&quot;);
 #endif
     while (s &lt; end)
     {
@@ -1370,7 +1368,7 @@
     long idx = 15 - src_size;
     uint8_t *s = (uint8_t *) src-idx, *d = dst-idx;
 #ifdef HAVE_MMX
-    __asm __volatile(
+    asm volatile(
     &quot;test          %0, %0           \n\t&quot;
     &quot;jns           2f               \n\t&quot;
     PREFETCH&quot;       (%1, %0)        \n\t&quot;
@@ -1477,8 +1475,8 @@
     : &quot;r&quot; (src-mmx_size), &quot;r&quot;(dst-mmx_size)
     );
 
-    __asm __volatile(SFENCE:::&quot;memory&quot;);
-    __asm __volatile(EMMS:::&quot;memory&quot;);
+    asm volatile(SFENCE:::&quot;memory&quot;);
+    asm volatile(EMMS:::&quot;memory&quot;);
 
     if (mmx_size==23) return; //finihsed, was multiple of 8
 
@@ -1507,7 +1505,7 @@
     for (y=0; y&lt;height; y++)
     {
 #ifdef HAVE_MMX
-//FIXME handle 2 lines a once (fewer prefetch, reuse some chrom, but very likely limited by mem anyway)
+//FIXME handle 2 lines at once (fewer prefetches, reuse some chroma, but very likely memory-limited anyway)
         asm volatile(
         &quot;xor                 %%&quot;REG_a&quot;, %%&quot;REG_a&quot;   \n\t&quot;
         ASMALIGN(4)
@@ -1623,7 +1621,7 @@
         }
 #endif
 #endif
-        if ((y&amp;(vertLumPerChroma-1))==(vertLumPerChroma-1) )
+        if ((y&amp;(vertLumPerChroma-1)) == vertLumPerChroma-1)
         {
             usrc += chromStride;
             vsrc += chromStride;
@@ -1639,9 +1637,8 @@
 }
 
 /**
- *
- * height should be a multiple of 2 and width should be a multiple of 16 (if this is a
- * problem for anyone then tell me, and ill fix it)
+ * Height should be a multiple of 2 and width should be a multiple of 16 (if
+ * this is a problem for anyone then tell me, and I will fix it).
  */
 static inline void RENAME(yv12toyuy2)(const uint8_t *ysrc, const uint8_t *usrc, const uint8_t *vsrc, uint8_t *dst,
                                       long width, long height,
@@ -1660,7 +1657,7 @@
     for (y=0; y&lt;height; y++)
     {
 #ifdef HAVE_MMX
-//FIXME handle 2 lines a once (fewer prefetch, reuse some chrom, but very likely limited by mem anyway)
+//FIXME handle 2 lines at once (fewer prefetches, reuse some chroma, but very likely memory-limited anyway)
         asm volatile(
         &quot;xor                %%&quot;REG_a&quot;, %%&quot;REG_a&quot;    \n\t&quot;
         ASMALIGN(4)
@@ -1695,7 +1692,7 @@
         : &quot;%&quot;REG_a
         );
 #else
-//FIXME adapt the alpha asm code from yv12-&gt;yuy2
+//FIXME adapt the Alpha ASM code from yv12-&gt;yuy2
 
 #if __WORDSIZE &gt;= 64
         int i;
@@ -1730,7 +1727,7 @@
         }
 #endif
 #endif
-        if ((y&amp;(vertLumPerChroma-1))==(vertLumPerChroma-1) )
+        if ((y&amp;(vertLumPerChroma-1)) == vertLumPerChroma-1)
         {
             usrc += chromStride;
             vsrc += chromStride;
@@ -1746,9 +1743,8 @@
 }
 
 /**
- *
- * height should be a multiple of 2 and width should be a multiple of 16 (if this is a
- * problem for anyone then tell me, and ill fix it)
+ * Height should be a multiple of 2 and width should be a multiple of 16 (if
+ * this is a problem for anyone then tell me, and I will fix it).
  */
 static inline void RENAME(yv12touyvy)(const uint8_t *ysrc, const uint8_t *usrc, const uint8_t *vsrc, uint8_t *dst,
                                       long width, long height,
@@ -1759,8 +1755,7 @@
 }
 
 /**
- *
- * width should be a multiple of 16
+ * Width should be a multiple of 16.
  */
 static inline void RENAME(yuv422ptoyuy2)(const uint8_t *ysrc, const uint8_t *usrc, const uint8_t *vsrc, uint8_t *dst,
                                          long width, long height,
@@ -1770,9 +1765,8 @@
 }
 
 /**
- *
- * height should be a multiple of 2 and width should be a multiple of 16 (if this is a
- * problem for anyone then tell me, and ill fix it)
+ * Height should be a multiple of 2 and width should be a multiple of 16 (if
+ * this is a problem for anyone then tell me, and I will fix it).
  */
 static inline void RENAME(yuy2toyv12)(const uint8_t *src, uint8_t *ydst, uint8_t *udst, uint8_t *vdst,
                                       long width, long height,
@@ -2007,10 +2001,10 @@
 }
 
 /**
- *
- * height should be a multiple of 2 and width should be a multiple of 16 (if this is a
- * problem for anyone then tell me, and ill fix it)
- * chrominance data is only taken from every secound line others are ignored FIXME write HQ version
+ * Height should be a multiple of 2 and width should be a multiple of 16 (if
+ * this is a problem for anyone then tell me, and I will fix it).
+ * Chrominance data is only taken from every secound line, others are ignored.
+ * FIXME: Write HQ version.
  */
 static inline void RENAME(uyvytoyv12)(const uint8_t *src, uint8_t *ydst, uint8_t *udst, uint8_t *vdst,
                                       long width, long height,
@@ -2133,10 +2127,11 @@
 }
 
 /**
- *
- * height should be a multiple of 2 and width should be a multiple of 2 (if this is a
- * problem for anyone then tell me, and ill fix it)
- * chrominance data is only taken from every secound line others are ignored in the C version FIXME write HQ version
+ * Height should be a multiple of 2 and width should be a multiple of 2 (if
+ * this is a problem for anyone then tell me, and I will fix it).
+ * Chrominance data is only taken from every secound line,
+ * others are ignored in the C version.
+ * FIXME: Write HQ version.
  */
 static inline void RENAME(rgb24toyv12)(const uint8_t *src, uint8_t *ydst, uint8_t *udst, uint8_t *vdst,
                                        long width, long height,
@@ -2152,8 +2147,8 @@
         {
             asm volatile(
             &quot;mov                        %2, %%&quot;REG_a&quot;   \n\t&quot;
-            &quot;movq     &quot;MANGLE(bgr2YCoeff)&quot;, %%mm6       \n\t&quot;
-            &quot;movq          &quot;MANGLE(w1111)&quot;, %%mm5       \n\t&quot;
+            &quot;movq  &quot;MANGLE(ff_bgr2YCoeff)&quot;, %%mm6       \n\t&quot;
+            &quot;movq       &quot;MANGLE(ff_w1111)&quot;, %%mm5       \n\t&quot;
             &quot;pxor                    %%mm7, %%mm7       \n\t&quot;
             &quot;lea (%%&quot;REG_a&quot;, %%&quot;REG_a&quot;, 2), %%&quot;REG_d&quot;   \n\t&quot;
             ASMALIGN(4)
@@ -2211,7 +2206,7 @@
             &quot;psraw                      $7, %%mm4       \n\t&quot;
 
             &quot;packuswb                %%mm4, %%mm0       \n\t&quot;
-            &quot;paddusb &quot;MANGLE(bgr2YOffset)&quot;, %%mm0       \n\t&quot;
+            &quot;paddusb &quot;MANGLE(ff_bgr2YOffset)&quot;, %%mm0    \n\t&quot;
 
             MOVNTQ&quot;                  %%mm0, (%1, %%&quot;REG_a&quot;) \n\t&quot;
             &quot;add                        $8,      %%&quot;REG_a&quot;  \n\t&quot;
@@ -2225,8 +2220,8 @@
         src -= srcStride*2;
         asm volatile(
         &quot;mov                        %4, %%&quot;REG_a&quot;   \n\t&quot;
-        &quot;movq          &quot;MANGLE(w1111)&quot;, %%mm5       \n\t&quot;
-        &quot;movq     &quot;MANGLE(bgr2UCoeff)&quot;, %%mm6       \n\t&quot;
+        &quot;movq       &quot;MANGLE(ff_w1111)&quot;, %%mm5       \n\t&quot;
+        &quot;movq  &quot;MANGLE(ff_bgr2UCoeff)&quot;, %%mm6       \n\t&quot;
         &quot;pxor                    %%mm7, %%mm7       \n\t&quot;
         &quot;lea (%%&quot;REG_a&quot;, %%&quot;REG_a&quot;, 2), %%&quot;REG_d&quot;   \n\t&quot;
         &quot;add                 %%&quot;REG_d&quot;, %%&quot;REG_d&quot;   \n\t&quot;
@@ -2275,8 +2270,8 @@
         &quot;psrlw                      $2, %%mm0       \n\t&quot;
         &quot;psrlw                      $2, %%mm2       \n\t&quot;
 #endif
-        &quot;movq     &quot;MANGLE(bgr2VCoeff)&quot;, %%mm1       \n\t&quot;
-        &quot;movq     &quot;MANGLE(bgr2VCoeff)&quot;, %%mm3       \n\t&quot;
+        &quot;movq  &quot;MANGLE(ff_bgr2VCoeff)&quot;, %%mm1       \n\t&quot;
+        &quot;movq  &quot;MANGLE(ff_bgr2VCoeff)&quot;, %%mm3       \n\t&quot;
 
         &quot;pmaddwd                 %%mm0, %%mm1       \n\t&quot;
         &quot;pmaddwd                 %%mm2, %%mm3       \n\t&quot;
@@ -2333,12 +2328,12 @@
         &quot;paddw                   %%mm1, %%mm5       \n\t&quot;
         &quot;paddw                   %%mm3, %%mm2       \n\t&quot;
         &quot;paddw                   %%mm5, %%mm2       \n\t&quot;
-        &quot;movq          &quot;MANGLE(w1111)&quot;, %%mm5       \n\t&quot;
+        &quot;movq       &quot;MANGLE(ff_w1111)&quot;, %%mm5       \n\t&quot;
         &quot;psrlw                      $2, %%mm4       \n\t&quot;
         &quot;psrlw                      $2, %%mm2       \n\t&quot;
 #endif
-        &quot;movq     &quot;MANGLE(bgr2VCoeff)&quot;, %%mm1       \n\t&quot;
-        &quot;movq     &quot;MANGLE(bgr2VCoeff)&quot;, %%mm3       \n\t&quot;
+        &quot;movq  &quot;MANGLE(ff_bgr2VCoeff)&quot;, %%mm1       \n\t&quot;
+        &quot;movq  &quot;MANGLE(ff_bgr2VCoeff)&quot;, %%mm3       \n\t&quot;
 
         &quot;pmaddwd                 %%mm4, %%mm1       \n\t&quot;
         &quot;pmaddwd                 %%mm2, %%mm3       \n\t&quot;
@@ -2362,7 +2357,7 @@
         &quot;punpckldq               %%mm4, %%mm0           \n\t&quot;
         &quot;punpckhdq               %%mm4, %%mm1           \n\t&quot;
         &quot;packsswb                %%mm1, %%mm0           \n\t&quot;
-        &quot;paddb  &quot;MANGLE(bgr2UVOffset)&quot;, %%mm0           \n\t&quot;
+        &quot;paddb &quot;MANGLE(ff_bgr2UVOffset)&quot;, %%mm0         \n\t&quot;
         &quot;movd                    %%mm0, (%2, %%&quot;REG_a&quot;) \n\t&quot;
         &quot;punpckhdq               %%mm0, %%mm0           \n\t&quot;
         &quot;movd                    %%mm0, (%3, %%&quot;REG_a&quot;) \n\t&quot;

Modified: branches/avidemux_2.5_branch_gruntster/avidemux/ADM_libraries/ADM_ffmpeg/ADM_libswscale/swscale.c
===================================================================
--- branches/avidemux_2.5_branch_gruntster/avidemux/ADM_libraries/ADM_ffmpeg/ADM_libswscale/swscale.c	2008-03-21 06:48:49 UTC (rev 3911)
+++ branches/avidemux_2.5_branch_gruntster/avidemux/ADM_libraries/ADM_ffmpeg/ADM_libswscale/swscale.c	2008-03-21 06:51:32 UTC (rev 3912)
@@ -38,7 +38,7 @@
 */
 
 /*
-tested special converters (most are tested actually but i didnt write it down ...)
+tested special converters (most are tested actually, but I did not write it down ...)
  YV12 -&gt; BGR16
  YV12 -&gt; YV12
  BGR15 -&gt; BGR16
@@ -53,16 +53,13 @@
   BGR32 -&gt; BGR24 &amp; RGB32 -&gt; RGB24
   BGR24 -&gt; YV12
 */
-// MEANX
-#include &quot;config.h&quot;
-// /MEANX
 
 #include &lt;inttypes.h&gt;
 #include &lt;string.h&gt;
 #include &lt;math.h&gt;
 #include &lt;stdio.h&gt;
 #include &lt;unistd.h&gt;
-#include &lt;limits.h&gt; // MEANX
+#include &quot;config.h&quot;
 #include &lt;assert.h&gt;
 #ifdef HAVE_SYS_MMAN_H
 #include &lt;sys/mman.h&gt;
@@ -75,20 +72,8 @@
 #include &quot;x86_cpu.h&quot;
 #include &quot;bswap.h&quot;
 #include &quot;rgb2rgb.h&quot;
+#include &quot;libavcodec/opt.h&quot;
 
-// MEANX
-#include &quot;wrapper.h&quot;
-
-// /MEANX
-
-
-
-#include &quot;../ADM_lavcodec/opt.h&quot; //MEANX
-//**** Does not build as shared on amd64... MEANX
-#ifdef ARCH_X86_64
-#undef RUNTIME_CPUDETECT
-#endif
-
 #undef MOVNTQ
 #undef PAVGB
 
@@ -117,6 +102,7 @@
 
 #define isSupportedIn(x)    (       \
            (x)==PIX_FMT_YUV420P     \
+        || (x)==PIX_FMT_YUVA420P    \
         || (x)==PIX_FMT_YUYV422     \
         || (x)==PIX_FMT_UYVY422     \
         || (x)==PIX_FMT_RGB32       \
@@ -139,6 +125,7 @@
         || (x)==PIX_FMT_RGB8        \
         || (x)==PIX_FMT_BGR4_BYTE   \
         || (x)==PIX_FMT_RGB4_BYTE   \
+        || (x)==PIX_FMT_YUV440P     \
     )
 #define isSupportedOut(x)   (       \
            (x)==PIX_FMT_YUV420P     \
@@ -182,7 +169,7 @@
 Special versions: fast Y 1:1 scaling (no interpolation in y direction)
 
 TODO
-more intelligent missalignment avoidance for the horizontal scaler
+more intelligent misalignment avoidance for the horizontal scaler
 write special vertical cubic upscale version
 Optimize C code (yv12 / minmax)
 add support for packed pixel yuv input &amp; output
@@ -193,51 +180,51 @@
 */
 
 #if defined(ARCH_X86) &amp;&amp; defined (CONFIG_GPL)
-static uint64_t attribute_used __attribute__((aligned(8))) bF8=       0xF8F8F8F8F8F8F8F8LL;
-static uint64_t attribute_used __attribute__((aligned(8))) bFC=       0xFCFCFCFCFCFCFCFCLL;
-static uint64_t                __attribute__((aligned(8))) w10=       0x0010001000100010LL;
-static uint64_t attribute_used __attribute__((aligned(8))) w02=       0x0002000200020002LL;
-static uint64_t attribute_used __attribute__((aligned(8))) bm00001111=0x00000000FFFFFFFFLL;
-static uint64_t attribute_used __attribute__((aligned(8))) bm00000111=0x0000000000FFFFFFLL;
-static uint64_t attribute_used __attribute__((aligned(8))) bm11111000=0xFFFFFFFFFF000000LL;
-static uint64_t attribute_used __attribute__((aligned(8))) bm01010101=0x00FF00FF00FF00FFLL;
+DECLARE_ASM_CONST(8, uint64_t, bF8)=       0xF8F8F8F8F8F8F8F8LL;
+DECLARE_ASM_CONST(8, uint64_t, bFC)=       0xFCFCFCFCFCFCFCFCLL;
+DECLARE_ASM_CONST(8, uint64_t, w10)=       0x0010001000100010LL;
+DECLARE_ASM_CONST(8, uint64_t, w02)=       0x0002000200020002LL;
+DECLARE_ASM_CONST(8, uint64_t, bm00001111)=0x00000000FFFFFFFFLL;
+DECLARE_ASM_CONST(8, uint64_t, bm00000111)=0x0000000000FFFFFFLL;
+DECLARE_ASM_CONST(8, uint64_t, bm11111000)=0xFFFFFFFFFF000000LL;
+DECLARE_ASM_CONST(8, uint64_t, bm01010101)=0x00FF00FF00FF00FFLL;
 
 static volatile uint64_t attribute_used __attribute__((aligned(8))) b5Dither;
 static volatile uint64_t attribute_used __attribute__((aligned(8))) g5Dither;
 static volatile uint64_t attribute_used __attribute__((aligned(8))) g6Dither;
 static volatile uint64_t attribute_used __attribute__((aligned(8))) r5Dither;
 
-static uint64_t __attribute__((aligned(8))) dither4[2]={
+const DECLARE_ALIGNED(8, uint64_t, ff_dither4[2]) = {
         0x0103010301030103LL,
         0x0200020002000200LL,};
 
-static uint64_t __attribute__((aligned(8))) dither8[2]={
+const DECLARE_ALIGNED(8, uint64_t, ff_dither8[2]) = {
         0x0602060206020602LL,
         0x0004000400040004LL,};
 
-static uint64_t                __attribute__((aligned(8))) b16Mask=   0x001F001F001F001FLL;
-static uint64_t attribute_used __attribute__((aligned(8))) g16Mask=   0x07E007E007E007E0LL;
-static uint64_t attribute_used __attribute__((aligned(8))) r16Mask=   0xF800F800F800F800LL;
-static uint64_t                __attribute__((aligned(8))) b15Mask=   0x001F001F001F001FLL;
-static uint64_t attribute_used __attribute__((aligned(8))) g15Mask=   0x03E003E003E003E0LL;
-static uint64_t attribute_used __attribute__((aligned(8))) r15Mask=   0x7C007C007C007C00LL;
+DECLARE_ASM_CONST(8, uint64_t, b16Mask)=   0x001F001F001F001FLL;
+DECLARE_ASM_CONST(8, uint64_t, g16Mask)=   0x07E007E007E007E0LL;
+DECLARE_ASM_CONST(8, uint64_t, r16Mask)=   0xF800F800F800F800LL;
+DECLARE_ASM_CONST(8, uint64_t, b15Mask)=   0x001F001F001F001FLL;
+DECLARE_ASM_CONST(8, uint64_t, g15Mask)=   0x03E003E003E003E0LL;
+DECLARE_ASM_CONST(8, uint64_t, r15Mask)=   0x7C007C007C007C00LL;
 
-static uint64_t attribute_used __attribute__((aligned(8))) M24A=      0x00FF0000FF0000FFLL;
-static uint64_t attribute_used __attribute__((aligned(8))) M24B=      0xFF0000FF0000FF00LL;
-static uint64_t attribute_used __attribute__((aligned(8))) M24C=      0x0000FF0000FF0000LL;
+DECLARE_ALIGNED(8, const uint64_t, ff_M24A)         = 0x00FF0000FF0000FFLL;
+DECLARE_ALIGNED(8, const uint64_t, ff_M24B)         = 0xFF0000FF0000FF00LL;
+DECLARE_ALIGNED(8, const uint64_t, ff_M24C)         = 0x0000FF0000FF0000LL;
 
 #ifdef FAST_BGR2YV12
-static const uint64_t bgr2YCoeff   attribute_used __attribute__((aligned(8))) = 0x000000210041000DULL;
-static const uint64_t bgr2UCoeff   attribute_used __attribute__((aligned(8))) = 0x0000FFEEFFDC0038ULL;
-static const uint64_t bgr2VCoeff   attribute_used __attribute__((aligned(8))) = 0x00000038FFD2FFF8ULL;
+DECLARE_ALIGNED(8, const uint64_t, ff_bgr2YCoeff)   = 0x000000210041000DULL;
+DECLARE_ALIGNED(8, const uint64_t, ff_bgr2UCoeff)   = 0x0000FFEEFFDC0038ULL;
+DECLARE_ALIGNED(8, const uint64_t, ff_bgr2VCoeff)   = 0x00000038FFD2FFF8ULL;
 #else
-static const uint64_t bgr2YCoeff   attribute_used __attribute__((aligned(8))) = 0x000020E540830C8BULL;
-static const uint64_t bgr2UCoeff   attribute_used __attribute__((aligned(8))) = 0x0000ED0FDAC23831ULL;
-static const uint64_t bgr2VCoeff   attribute_used __attribute__((aligned(8))) = 0x00003831D0E6F6EAULL;
+DECLARE_ALIGNED(8, const uint64_t, ff_bgr2YCoeff)   = 0x000020E540830C8BULL;
+DECLARE_ALIGNED(8, const uint64_t, ff_bgr2UCoeff)   = 0x0000ED0FDAC23831ULL;
+DECLARE_ALIGNED(8, const uint64_t, ff_bgr2VCoeff)   = 0x00003831D0E6F6EAULL;
 #endif /* FAST_BGR2YV12 */
-static const uint64_t bgr2YOffset  attribute_used __attribute__((aligned(8))) = 0x1010101010101010ULL;
-static const uint64_t bgr2UVOffset attribute_used __attribute__((aligned(8))) = 0x8080808080808080ULL;
-static const uint64_t w1111        attribute_used __attribute__((aligned(8))) = 0x0001000100010001ULL;
+DECLARE_ALIGNED(8, const uint64_t, ff_bgr2YOffset)  = 0x1010101010101010ULL;
+DECLARE_ALIGNED(8, const uint64_t, ff_bgr2UVOffset) = 0x8080808080808080ULL;
+DECLARE_ALIGNED(8, const uint64_t, ff_w1111)        = 0x0001000100010001ULL;
 #endif /* defined(ARCH_X86) */
 
 // clipping helper table for C implementations:
@@ -254,18 +241,13 @@
 static const char * sws_context_to_name(void * ptr) {
     return &quot;swscaler&quot;;
 }
-/* MEANX ????? */
-#ifndef offsetof
-#    define offsetof(T,F) ((unsigned int)((char *)&amp;((T *)0)-&gt;F))
-#endif
-/* MEANX */
 
 #define OFFSET(x) offsetof(SwsContext, x)
 #define DEFAULT 0
 #define VE AV_OPT_FLAG_VIDEO_PARAM | AV_OPT_FLAG_ENCODING_PARAM
 
 static const AVOption options[] = {
-    { &quot;sws_flags&quot;, &quot;scaler/cpu flags&quot;, OFFSET(flags), FF_OPT_TYPE_FLAGS, DEFAULT, INT_MIN, INT_MAX, VE, &quot;sws_flags&quot; },
+    { &quot;sws_flags&quot;, &quot;scaler/cpu flags&quot;, OFFSET(flags), FF_OPT_TYPE_FLAGS, DEFAULT, 0, UINT_MAX, VE, &quot;sws_flags&quot; },
     { &quot;fast_bilinear&quot;, &quot;fast bilinear&quot;, 0, FF_OPT_TYPE_CONST, SWS_FAST_BILINEAR, INT_MIN, INT_MAX, VE, &quot;sws_flags&quot; },
     { &quot;bilinear&quot;, &quot;bilinear&quot;, 0, FF_OPT_TYPE_CONST, SWS_BILINEAR, INT_MIN, INT_MAX, VE, &quot;sws_flags&quot; },
     { &quot;bicubic&quot;, &quot;bicubic&quot;, 0, FF_OPT_TYPE_CONST, SWS_BICUBIC, INT_MIN, INT_MAX, VE, &quot;sws_flags&quot; },
@@ -292,13 +274,15 @@
 #undef VE
 #undef DEFAULT
 
-static AVClass sws_context_class = { &quot;SWScaler&quot;, sws_context_to_name, options };
+static const AVClass sws_context_class = { &quot;SWScaler&quot;, sws_context_to_name, options };
 
-char *sws_format_name(enum PixelFormat format)
+const char *sws_format_name(enum PixelFormat format)
 {
     switch (format) {
         case PIX_FMT_YUV420P:
             return &quot;yuv420p&quot;;
+        case PIX_FMT_YUVA420P:
+            return &quot;yuva420p&quot;;
         case PIX_FMT_YUYV422:
             return &quot;yuyv422&quot;;
         case PIX_FMT_RGB24:
@@ -371,6 +355,8 @@
             return &quot;nv12&quot;;
         case PIX_FMT_NV21:
             return &quot;nv21&quot;;
+        case PIX_FMT_YUV440P:
+            return &quot;yuv440p&quot;;
         default:
             return &quot;Unknown format&quot;;
     }
@@ -381,7 +367,7 @@
 {
     volatile int i= bF8+bFC+w10+
     bm00001111+bm00000111+bm11111000+b16Mask+g16Mask+r16Mask+b15Mask+g15Mask+r15Mask+
-    M24A+M24B+M24C+w02 + b5Dither+g5Dither+r5Dither+g6Dither+dither4[0]+dither8[0]+bm01010101;
+    ff_M24A+ff_M24B+ff_M24C+w02 + b5Dither+g5Dither+r5Dither+g6Dither+ff_dither4[0]+ff_dither8[0]+bm01010101;
     if (i) i=0;
 }
 #endif
@@ -402,7 +388,7 @@
         dest[i]= av_clip_uint8(val&gt;&gt;19);
     }
 
-    if (uDest != NULL)
+    if (uDest)
         for (i=0; i&lt;chrDstW; i++)
         {
             int u=1&lt;&lt;18;
@@ -411,7 +397,7 @@
             for (j=0; j&lt;chrFilterSize; j++)
             {
                 u += chrSrc[j][i] * chrFilter[j];
-                v += chrSrc[j][i + 2048] * chrFilter[j];
+                v += chrSrc[j][i + VOFW] * chrFilter[j];
             }
 
             uDest[i]= av_clip_uint8(u&gt;&gt;19);
@@ -435,7 +421,7 @@
         dest[i]= av_clip_uint8(val&gt;&gt;19);
     }
 
-    if (uDest == NULL)
+    if (!uDest)
         return;
 
     if (dstFormat == PIX_FMT_NV12)
@@ -447,7 +433,7 @@
             for (j=0; j&lt;chrFilterSize; j++)
             {
                 u += chrSrc[j][i] * chrFilter[j];
-                v += chrSrc[j][i + 2048] * chrFilter[j];
+                v += chrSrc[j][i + VOFW] * chrFilter[j];
             }
 
             uDest[2*i]= av_clip_uint8(u&gt;&gt;19);
@@ -462,7 +448,7 @@
             for (j=0; j&lt;chrFilterSize; j++)
             {
                 u += chrSrc[j][i] * chrFilter[j];
-                v += chrSrc[j][i + 2048] * chrFilter[j];
+                v += chrSrc[j][i + VOFW] * chrFilter[j];
             }
 
             uDest[2*i]= av_clip_uint8(v&gt;&gt;19);
@@ -488,7 +474,7 @@
         for (j=0; j&lt;chrFilterSize; j++)\
         {\
             U += chrSrc[j][i] * chrFilter[j];\
-            V += chrSrc[j][i+2048] * chrFilter[j];\
+            V += chrSrc[j][i+VOFW] * chrFilter[j];\
         }\
         Y1&gt;&gt;=19;\
         Y2&gt;&gt;=19;\
@@ -518,7 +504,7 @@
         int Y1= (buf0[i2  ]*yalpha1+buf1[i2  ]*yalpha)&gt;&gt;19;           \
         int Y2= (buf0[i2+1]*yalpha1+buf1[i2+1]*yalpha)&gt;&gt;19;           \
         int U= (uvbuf0[i     ]*uvalpha1+uvbuf1[i     ]*uvalpha)&gt;&gt;19;  \
-        int V= (uvbuf0[i+2048]*uvalpha1+uvbuf1[i+2048]*uvalpha)&gt;&gt;19;  \
+        int V= (uvbuf0[i+VOFW]*uvalpha1+uvbuf1[i+VOFW]*uvalpha)&gt;&gt;19;  \
 
 #define YSCALE_YUV_2_RGB2_C(type) \
     YSCALE_YUV_2_PACKED2_C\
@@ -533,7 +519,7 @@
         int Y1= buf0[i2  ]&gt;&gt;7;\
         int Y2= buf0[i2+1]&gt;&gt;7;\
         int U= (uvbuf1[i     ])&gt;&gt;7;\
-        int V= (uvbuf1[i+2048])&gt;&gt;7;\
+        int V= (uvbuf1[i+VOFW])&gt;&gt;7;\
 
 #define YSCALE_YUV_2_RGB1_C(type) \
     YSCALE_YUV_2_PACKED1_C\
@@ -548,7 +534,7 @@
         int Y1= buf0[i2  ]&gt;&gt;7;\
         int Y2= buf0[i2+1]&gt;&gt;7;\
         int U= (uvbuf0[i     ] + uvbuf1[i     ])&gt;&gt;8;\
-        int V= (uvbuf0[i+2048] + uvbuf1[i+2048])&gt;&gt;8;\
+        int V= (uvbuf0[i+VOFW] + uvbuf1[i+VOFW])&gt;&gt;8;\
 
 #define YSCALE_YUV_2_RGB1B_C(type) \
     YSCALE_YUV_2_PACKED1B_C\
@@ -1557,7 +1543,7 @@
 
 #if defined(RUNTIME_CPUDETECT) &amp;&amp; defined (CONFIG_GPL)
 #if defined(ARCH_X86)
-    // ordered per speed fasterst first
+    // ordered per speed fastest first
     if (flags &amp; SWS_CPU_CAPS_MMX2)
         return swScale_MMX2;
     else if (flags &amp; SWS_CPU_CAPS_3DNOW)
@@ -1611,9 +1597,9 @@
     }
     dst = dstParam[1] + dstStride[1]*srcSliceY/2;
     if (c-&gt;dstFormat == PIX_FMT_NV12)
-        interleaveBytes( src[1],src[2],dst,c-&gt;srcW/2,srcSliceH/2,srcStride[1],srcStride[2],dstStride[0] );
+        interleaveBytes(src[1], src[2], dst, c-&gt;srcW/2, srcSliceH/2, srcStride[1], srcStride[2], dstStride[0]);
     else
-        interleaveBytes( src[2],src[1],dst,c-&gt;srcW/2,srcSliceH/2,srcStride[2],srcStride[1],dstStride[0] );
+        interleaveBytes(src[2], src[1], dst, c-&gt;srcW/2, srcSliceH/2, srcStride[2], srcStride[1], dstStride[0]);
 
     return srcSliceH;
 }
@@ -1622,7 +1608,7 @@
                                int srcSliceH, uint8_t* dstParam[], int dstStride[]){
     uint8_t *dst=dstParam[0] + dstStride[0]*srcSliceY;
 
-    yv12toyuy2( src[0],src[1],src[2],dst,c-&gt;srcW,srcSliceH,srcStride[0],srcStride[1],dstStride[0] );
+    yv12toyuy2(src[0], src[1], src[2], dst, c-&gt;srcW, srcSliceH, srcStride[0], srcStride[1], dstStride[0]);
 
     return srcSliceH;
 }
@@ -1631,7 +1617,7 @@
                                int srcSliceH, uint8_t* dstParam[], int dstStride[]){
     uint8_t *dst=dstParam[0] + dstStride[0]*srcSliceY;
 
-    yv12touyvy( src[0],src[1],src[2],dst,c-&gt;srcW,srcSliceH,srcStride[0],srcStride[1],dstStride[0] );
+    yv12touyvy(src[0], src[1], src[2], dst, c-&gt;srcW, srcSliceH, srcStride[0], srcStride[1], dstStride[0]);
 
     return srcSliceH;
 }
@@ -1663,7 +1649,7 @@
         case 0x83: conv= rgb15to32; break;
         case 0x84: conv= rgb16to32; break;
         case 0x86: conv= rgb24to32; break;
-        default: av_log(c, AV_LOG_ERROR, &quot;swScaler: internal error %s -&gt; %s converter\n&quot;,
+        default: av_log(c, AV_LOG_ERROR, &quot;internal error %s -&gt; %s converter\n&quot;,
                         sws_format_name(srcFormat), sws_format_name(dstFormat)); break;
         }
     }else if (  (isBGR(srcFormat) &amp;&amp; isRGB(dstFormat))
@@ -1685,17 +1671,17 @@
         case 0x84: conv= rgb16tobgr32; break;
         case 0x86: conv= rgb24tobgr32; break;
         case 0x88: conv= rgb32tobgr32; break;
-        default: av_log(c, AV_LOG_ERROR, &quot;swScaler: internal error %s -&gt; %s converter\n&quot;,
+        default: av_log(c, AV_LOG_ERROR, &quot;internal error %s -&gt; %s converter\n&quot;,
                         sws_format_name(srcFormat), sws_format_name(dstFormat)); break;
         }
     }else{
-        av_log(c, AV_LOG_ERROR, &quot;swScaler: internal error %s -&gt; %s converter\n&quot;,
+        av_log(c, AV_LOG_ERROR, &quot;internal error %s -&gt; %s converter\n&quot;,
                sws_format_name(srcFormat), sws_format_name(dstFormat));
     }
 
     if(conv)
     {
-        if (dstStride[0]*srcBpp == srcStride[0]*dstBpp)
+        if (dstStride[0]*srcBpp == srcStride[0]*dstBpp &amp;&amp; srcStride[0] &gt; 0)
             conv(src[0], dst[0] + dstStride[0]*srcSliceY, srcSliceH*srcStride[0]);
         else
         {
@@ -1894,6 +1880,7 @@
         *v=0;
         break;
     case PIX_FMT_YUV420P:
+    case PIX_FMT_YUVA420P:
     case PIX_FMT_GRAY16BE:
     case PIX_FMT_GRAY16LE:
     case PIX_FMT_GRAY8: //FIXME remove after different subsamplings are fully implemented
@@ -1902,6 +1889,10 @@
         *h=1;
         *v=1;
         break;
+    case PIX_FMT_YUV440P:
+        *h=0;
+        *v=1;
+        break;
     case PIX_FMT_YUV410P:
         *h=2;
         *v=2;
@@ -2022,6 +2013,9 @@
         case PIX_FMT_YUVJ444P:
             *format = PIX_FMT_YUV444P;
             return 1;
+        case PIX_FMT_YUVJ440P:
+            *format = PIX_FMT_YUV440P;
+            return 1;
         default:
             return 0;
     }
@@ -2056,7 +2050,7 @@
 #endif
 #endif /* RUNTIME_CPUDETECT */
     if (clip_table[512] != 255) globalInit();
-    if (rgb15to16 == NULL) sws_rgb2rgb_init(flags);
+    if (!rgb15to16) sws_rgb2rgb_init(flags);
 
     unscaled = (srcW == dstW &amp;&amp; srcH == dstH);
     needsDither= (isBGR(dstFormat) || isRGB(dstFormat))
@@ -2068,12 +2062,12 @@
 
     if (!isSupportedIn(srcFormat))
     {
-        av_log(NULL, AV_LOG_ERROR, &quot;swScaler: %s is not supported as input format\n&quot;, sws_format_name(srcFormat));
+        av_log(NULL, AV_LOG_ERROR, &quot;swScaler: %s is not supported as input pixel format\n&quot;, sws_format_name(srcFormat));
         return NULL;
     }
     if (!isSupportedOut(dstFormat))
     {
-        av_log(NULL, AV_LOG_ERROR, &quot;swScaler: %s is not supported as output format\n&quot;, sws_format_name(dstFormat));
+        av_log(NULL, AV_LOG_ERROR, &quot;swScaler: %s is not supported as output pixel format\n&quot;, sws_format_name(dstFormat));
         return NULL;
     }
 
@@ -2084,6 +2078,10 @@
                srcW, srcH, dstW, dstH);
         return NULL;
     }
+    if(srcW &gt; VOFW || dstW &gt; VOFW){
+        av_log(NULL, AV_LOG_ERROR, &quot;swScaler: Compile time max width is &quot;AV_STRINGIFY(VOFW)&quot; change VOF/VOFW and recompile\n&quot;);
+        return NULL;
+    }
 
     if (!dstFilter) dstFilter= &dummyFilter;
     if (!srcFilter) srcFilter= &dummyFilter;
@@ -2103,14 +2101,14 @@
     c-&gt;vRounder= 4* 0x0001000100010001ULL;
 
     usesHFilter= usesVFilter= 0;
-    if (dstFilter-&gt;lumV!=NULL &amp;&amp; dstFilter-&gt;lumV-&gt;length&gt;1) usesVFilter=1;
-    if (dstFilter-&gt;lumH!=NULL &amp;&amp; dstFilter-&gt;lumH-&gt;length&gt;1) usesHFilter=1;
-    if (dstFilter-&gt;chrV!=NULL &amp;&amp; dstFilter-&gt;chrV-&gt;length&gt;1) usesVFilter=1;
-    if (dstFilter-&gt;chrH!=NULL &amp;&amp; dstFilter-&gt;chrH-&gt;length&gt;1) usesHFilter=1;
-    if (srcFilter-&gt;lumV!=NULL &amp;&amp; srcFilter-&gt;lumV-&gt;length&gt;1) usesVFilter=1;
-    if (srcFilter-&gt;lumH!=NULL &amp;&amp; srcFilter-&gt;lumH-&gt;length&gt;1) usesHFilter=1;
-    if (srcFilter-&gt;chrV!=NULL &amp;&amp; srcFilter-&gt;chrV-&gt;length&gt;1) usesVFilter=1;
-    if (srcFilter-&gt;chrH!=NULL &amp;&amp; srcFilter-&gt;chrH-&gt;length&gt;1) usesHFilter=1;
+    if (dstFilter-&gt;lumV &amp;&amp; dstFilter-&gt;lumV-&gt;length&gt;1) usesVFilter=1;
+    if (dstFilter-&gt;lumH &amp;&amp; dstFilter-&gt;lumH-&gt;length&gt;1) usesHFilter=1;
+    if (dstFilter-&gt;chrV &amp;&amp; dstFilter-&gt;chrV-&gt;length&gt;1) usesVFilter=1;
+    if (dstFilter-&gt;chrH &amp;&amp; dstFilter-&gt;chrH-&gt;length&gt;1) usesHFilter=1;
+    if (srcFilter-&gt;lumV &amp;&amp; srcFilter-&gt;lumV-&gt;length&gt;1) usesVFilter=1;
+    if (srcFilter-&gt;lumH &amp;&amp; srcFilter-&gt;lumH-&gt;length&gt;1) usesHFilter=1;
+    if (srcFilter-&gt;chrV &amp;&amp; srcFilter-&gt;chrV-&gt;length&gt;1) usesVFilter=1;
+    if (srcFilter-&gt;chrH &amp;&amp; srcFilter-&gt;chrH-&gt;length&gt;1) usesHFilter=1;
 
     getSubSampleFactors(&amp;c-&gt;chrSrcHSubSample, &amp;c-&gt;chrSrcVSubSample, srcFormat);
     getSubSampleFactors(&amp;c-&gt;chrDstHSubSample, &amp;c-&gt;chrDstVSubSample, dstFormat);
@@ -2164,7 +2162,7 @@
         }
 #endif
 
-        if ( srcFormat==PIX_FMT_YUV410P &amp;&amp; dstFormat==PIX_FMT_YUV420P )
+        if (srcFormat==PIX_FMT_YUV410P &amp;&amp; dstFormat==PIX_FMT_YUV420P)
         {
             c-&gt;swScale= yvu9toyv12Wrapper;
         }
@@ -2220,7 +2218,7 @@
         /* simple copy */
         if (  srcFormat == dstFormat
             || (isPlanarYUV(srcFormat) &amp;&amp; isGray(dstFormat))
-            || (isPlanarYUV(dstFormat) &amp;&amp; isGray(srcFormat)) )
+            || (isPlanarYUV(dstFormat) &amp;&amp; isGray(srcFormat)))
         {
             c-&gt;swScale= simpleCopy;
         }
@@ -2246,7 +2244,7 @@
 
         if (c-&gt;swScale){
             if (flags&amp;SWS_PRINT_INFO)
-                av_log(c, AV_LOG_INFO, &quot;SwScaler: using unscaled %s -&gt; %s special converter\n&quot;,
+                av_log(c, AV_LOG_INFO, &quot;using unscaled %s -&gt; %s special converter\n&quot;,
                                 sws_format_name(srcFormat), sws_format_name(dstFormat));
             return c;
         }
@@ -2258,7 +2256,7 @@
         if (!c-&gt;canMMX2BeUsed &amp;&amp; dstW &gt;=srcW &amp;&amp; (srcW&amp;15)==0 &amp;&amp; (flags&amp;SWS_FAST_BILINEAR))
         {
             if (flags&amp;SWS_PRINT_INFO)
-                av_log(c, AV_LOG_INFO, &quot;SwScaler: output Width is not a multiple of 32 -&gt; no MMX2 scaler\n&quot;);
+                av_log(c, AV_LOG_INFO, &quot;output Width is not a multiple of 32 -&gt; no MMX2 scaler\n&quot;);
         }
         if (usesHFilter) c-&gt;canMMX2BeUsed=0;
     }
@@ -2379,7 +2377,7 @@
         nextSlice&gt;&gt;= c-&gt;chrSrcVSubSample;
         nextSlice&lt;&lt;= c-&gt;chrSrcVSubSample;
         if (c-&gt;vLumFilterPos[i   ] + c-&gt;vLumBufSize &lt; nextSlice)
-            c-&gt;vLumBufSize= nextSlice - c-&gt;vLumFilterPos[i   ];
+            c-&gt;vLumBufSize= nextSlice - c-&gt;vLumFilterPos[i];
         if (c-&gt;vChrFilterPos[chrI] + c-&gt;vChrBufSize &lt; (nextSlice&gt;&gt;c-&gt;chrSrcVSubSample))
             c-&gt;vChrBufSize= (nextSlice&gt;&gt;c-&gt;chrSrcVSubSample) - c-&gt;vChrFilterPos[chrI];
     }
@@ -2390,13 +2388,15 @@
     //Note we need at least one pixel more at the end because of the mmx code (just in case someone wanna replace the 4000/8000)
     /* align at 16 bytes for AltiVec */
     for (i=0; i&lt;c-&gt;vLumBufSize; i++)
-        c-&gt;lumPixBuf[i]= c-&gt;lumPixBuf[i+c-&gt;vLumBufSize]= av_mallocz(4000);
+        c-&gt;lumPixBuf[i]= c-&gt;lumPixBuf[i+c-&gt;vLumBufSize]= av_mallocz(VOF+1);
     for (i=0; i&lt;c-&gt;vChrBufSize; i++)
-        c-&gt;chrPixBuf[i]= c-&gt;chrPixBuf[i+c-&gt;vChrBufSize]= av_malloc(8000);
+        c-&gt;chrPixBuf[i]= c-&gt;chrPixBuf[i+c-&gt;vChrBufSize]= av_malloc((VOF+1)*2);
 
     //try to avoid drawing green stuff between the right end and the stride end
-    for (i=0; i&lt;c-&gt;vChrBufSize; i++) memset(c-&gt;chrPixBuf[i], 64, 8000);
+    for (i=0; i&lt;c-&gt;vChrBufSize; i++) memset(c-&gt;chrPixBuf[i], 64, (VOF+1)*2);
 
+    assert(2*VOFW == VOF);
+
     ASSERT(c-&gt;chrDstH &lt;= dstH)
 
     if (flags&amp;SWS_PRINT_INFO)
@@ -2407,29 +2407,29 @@
         char *dither= &quot;&quot;;
 #endif
         if (flags&amp;SWS_FAST_BILINEAR)
-            av_log(c, AV_LOG_INFO, &quot;SwScaler: FAST_BILINEAR scaler, &quot;);
+            av_log(c, AV_LOG_INFO, &quot;FAST_BILINEAR scaler, &quot;);
         else if (flags&amp;SWS_BILINEAR)
-            av_log(c, AV_LOG_INFO, &quot;SwScaler: BILINEAR scaler, &quot;);
+            av_log(c, AV_LOG_INFO, &quot;BILINEAR scaler, &quot;);
         else if (flags&amp;SWS_BICUBIC)
-            av_log(c, AV_LOG_INFO, &quot;SwScaler: BICUBIC scaler, &quot;);
+            av_log(c, AV_LOG_INFO, &quot;BICUBIC scaler, &quot;);
         else if (flags&amp;SWS_X)
-            av_log(c, AV_LOG_INFO, &quot;SwScaler: Experimental scaler, &quot;);
+            av_log(c, AV_LOG_INFO, &quot;Experimental scaler, &quot;);
         else if (flags&amp;SWS_POINT)
-            av_log(c, AV_LOG_INFO, &quot;SwScaler: Nearest Neighbor / POINT scaler, &quot;);
+            av_log(c, AV_LOG_INFO, &quot;Nearest Neighbor / POINT scaler, &quot;);
         else if (flags&amp;SWS_AREA)
-            av_log(c, AV_LOG_INFO, &quot;SwScaler: Area Averageing scaler, &quot;);
+            av_log(c, AV_LOG_INFO, &quot;Area Averageing scaler, &quot;);
         else if (flags&amp;SWS_BICUBLIN)
-            av_log(c, AV_LOG_INFO, &quot;SwScaler: luma BICUBIC / chroma BILINEAR scaler, &quot;);
+            av_log(c, AV_LOG_INFO, &quot;luma BICUBIC / chroma BILINEAR scaler, &quot;);
         else if (flags&amp;SWS_GAUSS)
-            av_log(c, AV_LOG_INFO, &quot;SwScaler: Gaussian scaler, &quot;);
+            av_log(c, AV_LOG_INFO, &quot;Gaussian scaler, &quot;);
         else if (flags&amp;SWS_SINC)
-            av_log(c, AV_LOG_INFO, &quot;SwScaler: Sinc scaler, &quot;);
+            av_log(c, AV_LOG_INFO, &quot;Sinc scaler, &quot;);
         else if (flags&amp;SWS_LANCZOS)
-            av_log(c, AV_LOG_INFO, &quot;SwScaler: Lanczos scaler, &quot;);
+            av_log(c, AV_LOG_INFO, &quot;Lanczos scaler, &quot;);
         else if (flags&amp;SWS_SPLINE)
-            av_log(c, AV_LOG_INFO, &quot;SwScaler: Bicubic spline scaler, &quot;);
+            av_log(c, AV_LOG_INFO, &quot;Bicubic spline scaler, &quot;);
         else
-            av_log(c, AV_LOG_INFO, &quot;SwScaler: ehh flags invalid?! &quot;);
+            av_log(c, AV_LOG_INFO, &quot;ehh flags invalid?! &quot;);
 
         if (dstFormat==PIX_FMT_BGR555 || dstFormat==PIX_FMT_BGR565)
             av_log(c, AV_LOG_INFO, &quot;from %s to%s %s &quot;,
@@ -2455,70 +2455,70 @@
         if (flags &amp; SWS_CPU_CAPS_MMX)
         {
             if (c-&gt;canMMX2BeUsed &amp;&amp; (flags&amp;SWS_FAST_BILINEAR))
-                av_log(c, AV_LOG_VERBOSE, &quot;SwScaler: using FAST_BILINEAR MMX2 scaler for horizontal scaling\n&quot;);
+                av_log(c, AV_LOG_VERBOSE, &quot;using FAST_BILINEAR MMX2 scaler for horizontal scaling\n&quot;);
             else
             {
                 if (c-&gt;hLumFilterSize==4)
-                    av_log(c, AV_LOG_VERBOSE, &quot;SwScaler: using 4-tap MMX scaler for horizontal luminance scaling\n&quot;);
+                    av_log(c, AV_LOG_VERBOSE, &quot;using 4-tap MMX scaler for horizontal luminance scaling\n&quot;);
                 else if (c-&gt;hLumFilterSize==8)
-                    av_log(c, AV_LOG_VERBOSE, &quot;SwScaler: using 8-tap MMX scaler for horizontal luminance scaling\n&quot;);
+                    av_log(c, AV_LOG_VERBOSE, &quot;using 8-tap MMX scaler for horizontal luminance scaling\n&quot;);
                 else
-                    av_log(c, AV_LOG_VERBOSE, &quot;SwScaler: using n-tap MMX scaler for horizontal luminance scaling\n&quot;);
+                    av_log(c, AV_LOG_VERBOSE, &quot;using n-tap MMX scaler for horizontal luminance scaling\n&quot;);
 
                 if (c-&gt;hChrFilterSize==4)
-                    av_log(c, AV_LOG_VERBOSE, &quot;SwScaler: using 4-tap MMX scaler for horizontal chrominance scaling\n&quot;);
+                    av_log(c, AV_LOG_VERBOSE, &quot;using 4-tap MMX scaler for horizontal chrominance scaling\n&quot;);
                 else if (c-&gt;hChrFilterSize==8)
-                    av_log(c, AV_LOG_VERBOSE, &quot;SwScaler: using 8-tap MMX scaler for horizontal chrominance scaling\n&quot;);
+                    av_log(c, AV_LOG_VERBOSE, &quot;using 8-tap MMX scaler for horizontal chrominance scaling\n&quot;);
                 else
-                    av_log(c, AV_LOG_VERBOSE, &quot;SwScaler: using n-tap MMX scaler for horizontal chrominance scaling\n&quot;);
+                    av_log(c, AV_LOG_VERBOSE, &quot;using n-tap MMX scaler for horizontal chrominance scaling\n&quot;);
             }
         }
         else
         {
 #if defined(ARCH_X86)
-            av_log(c, AV_LOG_VERBOSE, &quot;SwScaler: using X86-Asm scaler for horizontal scaling\n&quot;);
+            av_log(c, AV_LOG_VERBOSE, &quot;using X86-Asm scaler for horizontal scaling\n&quot;);
 #else
             if (flags &amp; SWS_FAST_BILINEAR)
-                av_log(c, AV_LOG_VERBOSE, &quot;SwScaler: using FAST_BILINEAR C scaler for horizontal scaling\n&quot;);
+                av_log(c, AV_LOG_VERBOSE, &quot;using FAST_BILINEAR C scaler for horizontal scaling\n&quot;);
             else
-                av_log(c, AV_LOG_VERBOSE, &quot;SwScaler: using C scaler for horizontal scaling\n&quot;);
+                av_log(c, AV_LOG_VERBOSE, &quot;using C scaler for horizontal scaling\n&quot;);
 #endif
         }
         if (isPlanarYUV(dstFormat))
         {
             if (c-&gt;vLumFilterSize==1)
-                av_log(c, AV_LOG_VERBOSE, &quot;SwScaler: using 1-tap %s \&quot;scaler\&quot; for vertical scaling (YV12 like)\n&quot;, (flags &amp; SWS_CPU_CAPS_MMX) ? &quot;MMX&quot; : &quot;C&quot;);
+                av_log(c, AV_LOG_VERBOSE, &quot;using 1-tap %s \&quot;scaler\&quot; for vertical scaling (YV12 like)\n&quot;, (flags &amp; SWS_CPU_CAPS_MMX) ? &quot;MMX&quot; : &quot;C&quot;);
             else
-                av_log(c, AV_LOG_VERBOSE, &quot;SwScaler: using n-tap %s scaler for vertical scaling (YV12 like)\n&quot;, (flags &amp; SWS_CPU_CAPS_MMX) ? &quot;MMX&quot; : &quot;C&quot;);
+                av_log(c, AV_LOG_VERBOSE, &quot;using n-tap %s scaler for vertical scaling (YV12 like)\n&quot;, (flags &amp; SWS_CPU_CAPS_MMX) ? &quot;MMX&quot; : &quot;C&quot;);
         }
         else
         {
             if (c-&gt;vLumFilterSize==1 &amp;&amp; c-&gt;vChrFilterSize==2)
-                av_log(c, AV_LOG_VERBOSE, &quot;SwScaler: using 1-tap %s \&quot;scaler\&quot; for vertical luminance scaling (BGR)\n&quot;
-                       &quot;SwScaler:       2-tap scaler for vertical chrominance scaling (BGR)\n&quot;,(flags &amp; SWS_CPU_CAPS_MMX) ? &quot;MMX&quot; : &quot;C&quot;);
+                av_log(c, AV_LOG_VERBOSE, &quot;using 1-tap %s \&quot;scaler\&quot; for vertical luminance scaling (BGR)\n&quot;
+                       &quot;      2-tap scaler for vertical chrominance scaling (BGR)\n&quot;, (flags &amp; SWS_CPU_CAPS_MMX) ? &quot;MMX&quot; : &quot;C&quot;);
             else if (c-&gt;vLumFilterSize==2 &amp;&amp; c-&gt;vChrFilterSize==2)
-                av_log(c, AV_LOG_VERBOSE, &quot;SwScaler: using 2-tap linear %s scaler for vertical scaling (BGR)\n&quot;, (flags &amp; SWS_CPU_CAPS_MMX) ? &quot;MMX&quot; : &quot;C&quot;);
+                av_log(c, AV_LOG_VERBOSE, &quot;using 2-tap linear %s scaler for vertical scaling (BGR)\n&quot;, (flags &amp; SWS_CPU_CAPS_MMX) ? &quot;MMX&quot; : &quot;C&quot;);
             else
-                av_log(c, AV_LOG_VERBOSE, &quot;SwScaler: using n-tap %s scaler for vertical scaling (BGR)\n&quot;, (flags &amp; SWS_CPU_CAPS_MMX) ? &quot;MMX&quot; : &quot;C&quot;);
+                av_log(c, AV_LOG_VERBOSE, &quot;using n-tap %s scaler for vertical scaling (BGR)\n&quot;, (flags &amp; SWS_CPU_CAPS_MMX) ? &quot;MMX&quot; : &quot;C&quot;);
         }
 
         if (dstFormat==PIX_FMT_BGR24)
-            av_log(c, AV_LOG_VERBOSE, &quot;SwScaler: using %s YV12-&gt;BGR24 Converter\n&quot;,
+            av_log(c, AV_LOG_VERBOSE, &quot;using %s YV12-&gt;BGR24 Converter\n&quot;,
                    (flags &amp; SWS_CPU_CAPS_MMX2) ? &quot;MMX2&quot; : ((flags &amp; SWS_CPU_CAPS_MMX) ? &quot;MMX&quot; : &quot;C&quot;));
         else if (dstFormat==PIX_FMT_RGB32)
-            av_log(c, AV_LOG_VERBOSE, &quot;SwScaler: using %s YV12-&gt;BGR32 Converter\n&quot;, (flags &amp; SWS_CPU_CAPS_MMX) ? &quot;MMX&quot; : &quot;C&quot;);
+            av_log(c, AV_LOG_VERBOSE, &quot;using %s YV12-&gt;BGR32 Converter\n&quot;, (flags &amp; SWS_CPU_CAPS_MMX) ? &quot;MMX&quot; : &quot;C&quot;);
         else if (dstFormat==PIX_FMT_BGR565)
-            av_log(c, AV_LOG_VERBOSE, &quot;SwScaler: using %s YV12-&gt;BGR16 Converter\n&quot;, (flags &amp; SWS_CPU_CAPS_MMX) ? &quot;MMX&quot; : &quot;C&quot;);
+            av_log(c, AV_LOG_VERBOSE, &quot;using %s YV12-&gt;BGR16 Converter\n&quot;, (flags &amp; SWS_CPU_CAPS_MMX) ? &quot;MMX&quot; : &quot;C&quot;);
         else if (dstFormat==PIX_FMT_BGR555)
-            av_log(c, AV_LOG_VERBOSE, &quot;SwScaler: using %s YV12-&gt;BGR15 Converter\n&quot;, (flags &amp; SWS_CPU_CAPS_MMX) ? &quot;MMX&quot; : &quot;C&quot;);
+            av_log(c, AV_LOG_VERBOSE, &quot;using %s YV12-&gt;BGR15 Converter\n&quot;, (flags &amp; SWS_CPU_CAPS_MMX) ? &quot;MMX&quot; : &quot;C&quot;);
 
-        av_log(c, AV_LOG_VERBOSE, &quot;SwScaler: %dx%d -&gt; %dx%d\n&quot;, srcW, srcH, dstW, dstH);
+        av_log(c, AV_LOG_VERBOSE, &quot;%dx%d -&gt; %dx%d\n&quot;, srcW, srcH, dstW, dstH);
     }
     if (flags &amp; SWS_PRINT_INFO)
     {
-        av_log(c, AV_LOG_DEBUG, &quot;SwScaler:Lum srcW=%d srcH=%d dstW=%d dstH=%d xInc=%d yInc=%d\n&quot;,
+        av_log(c, AV_LOG_DEBUG, &quot;Lum srcW=%d srcH=%d dstW=%d dstH=%d xInc=%d yInc=%d\n&quot;,
                c-&gt;srcW, c-&gt;srcH, c-&gt;dstW, c-&gt;dstH, c-&gt;lumXInc, c-&gt;lumYInc);
-        av_log(c, AV_LOG_DEBUG, &quot;SwScaler:Chr srcW=%d srcH=%d dstW=%d dstH=%d xInc=%d yInc=%d\n&quot;,
+        av_log(c, AV_LOG_DEBUG, &quot;Chr srcW=%d srcH=%d dstW=%d dstH=%d xInc=%d yInc=%d\n&quot;,
                c-&gt;chrSrcW, c-&gt;chrSrcH, c-&gt;chrDstW, c-&gt;chrDstH, c-&gt;chrXInc, c-&gt;chrYInc);
     }
 
@@ -2527,7 +2527,7 @@
 }
 
 /**
- * swscale warper, so we don't need to export the SwsContext.
+ * swscale wrapper, so we don't need to export the SwsContext.
  * assumes planar YUV to be in YUV order instead of YVU
  */
 int sws_scale(SwsContext *c, uint8_t* src[], int srcStride[], int srcSliceY,
@@ -2536,7 +2536,7 @@
     uint8_t* src2[4]= {src[0], src[1], src[2]};
     uint32_t pal[256];
     if (c-&gt;sliceDir == 0 &amp;&amp; srcSliceY != 0 &amp;&amp; srcSliceY + srcSliceH != c-&gt;srcH) {
-        av_log(c, AV_LOG_ERROR, &quot;swScaler: slices start in the middle!\n&quot;);
+        av_log(c, AV_LOG_ERROR, &quot;Slices start in the middle!\n&quot;);
         return 0;
     }
     if (c-&gt;sliceDir == 0) {
@@ -2581,7 +2581,7 @@
 }
 
 /**
- * swscale warper, so we don't need to export the SwsContext
+ * swscale wrapper, so we don't need to export the SwsContext
  */
 int sws_scale_ordered(SwsContext *c, uint8_t* src[], int srcStride[], int srcSliceY,
                       int srcSliceH, uint8_t* dst[], int dstStride[]){
@@ -2663,7 +2663,7 @@
     for (i=0; i&lt;length; i++)
     {
         double dist= i-middle;
-        coeff[i]= exp( -dist*dist/(2*variance*variance) ) / sqrt(2*variance*PI);
+        coeff[i]= exp(-dist*dist/(2*variance*variance)) / sqrt(2*variance*PI);
     }
 
     sws_normalizeVec(vec, 1.0);
@@ -2965,18 +2965,23 @@
                                         int dstW, int dstH, int dstFormat, int flags,
                                         SwsFilter *srcFilter, SwsFilter *dstFilter, double *param)
 {
-    if (context != NULL) {
-        if ((context-&gt;srcW != srcW) || (context-&gt;srcH != srcH) ||
-            (context-&gt;srcFormat != srcFormat) ||
-            (context-&gt;dstW != dstW) || (context-&gt;dstH != dstH) ||
-            (context-&gt;dstFormat != dstFormat) || (context-&gt;flags != flags) ||
-            (context-&gt;param != param))
+    static const double default_param[2] = {SWS_PARAM_DEFAULT, SWS_PARAM_DEFAULT};
+
+    if (!param)
+        param = default_param;
+
+    if (context) {
+        if (context-&gt;srcW != srcW || context-&gt;srcH != srcH ||
+            context-&gt;srcFormat != srcFormat ||
+            context-&gt;dstW != dstW || context-&gt;dstH != dstH ||
+            context-&gt;dstFormat != dstFormat || context-&gt;flags != flags ||
+            context-&gt;param[0] != param[0] || context-&gt;param[1] != param[1])
         {
             sws_freeContext(context);
             context = NULL;
         }
     }
-    if (context == NULL) {
+    if (!context) {
         return sws_getContext(srcW, srcH, srcFormat,
                               dstW, dstH, dstFormat, flags,
                               srcFilter, dstFilter, param);

Modified: branches/avidemux_2.5_branch_gruntster/avidemux/ADM_libraries/ADM_ffmpeg/ADM_libswscale/swscale.h
===================================================================
--- branches/avidemux_2.5_branch_gruntster/avidemux/ADM_libraries/ADM_ffmpeg/ADM_libswscale/swscale.h	2008-03-21 06:48:49 UTC (rev 3911)
+++ branches/avidemux_2.5_branch_gruntster/avidemux/ADM_libraries/ADM_ffmpeg/ADM_libswscale/swscale.h	2008-03-21 06:51:32 UTC (rev 3912)
@@ -18,11 +18,8 @@
  * Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA
  */
 
-#ifndef SWSCALE_H
-#define SWSCALE_H
-#ifdef __cplusplus
-extern &quot;C&quot; {
-#endif
+#ifndef FFMPEG_SWSCALE_H
+#define FFMPEG_SWSCALE_H
 
 /**
  * @file swscale.h
@@ -30,13 +27,18 @@
  *     external api for the swscale stuff
  */
 
-#include &quot;avutil.h&quot;
+#include &quot;libavutil/avutil.h&quot;
 
-#define AV_STRINGIFY(s)         AV_TOSTRING(s)
-#define AV_TOSTRING(s) #s
+#define LIBSWSCALE_VERSION_MAJOR 0
+#define LIBSWSCALE_VERSION_MINOR 5
+#define LIBSWSCALE_VERSION_MICRO 1
 
-#define LIBSWSCALE_VERSION_INT  ((0&lt;&lt;16)+(5&lt;&lt;8)+0)
-#define LIBSWSCALE_VERSION      0.5.0
+#define LIBSWSCALE_VERSION_INT  AV_VERSION_INT(LIBSWSCALE_VERSION_MAJOR, \
+                                               LIBSWSCALE_VERSION_MINOR, \
+                                               LIBSWSCALE_VERSION_MICRO)
+#define LIBSWSCALE_VERSION      AV_VERSION(LIBSWSCALE_VERSION_MAJOR, \
+                                           LIBSWSCALE_VERSION_MINOR, \
+                                           LIBSWSCALE_VERSION_MICRO)
 #define LIBSWSCALE_BUILD        LIBSWSCALE_VERSION_INT
 
 #define LIBSWSCALE_IDENT        &quot;SwS&quot; AV_STRINGIFY(LIBSWSCALE_VERSION)
@@ -140,8 +142,5 @@
                                         int srcW, int srcH, int srcFormat,
                                         int dstW, int dstH, int dstFormat, int flags,
                                         SwsFilter *srcFilter, SwsFilter *dstFilter, double *param);
-#ifdef __cplusplus
-}
-#endif
 
-#endif
+#endif /* FFMPEG_SWSCALE_H */

Modified: branches/avidemux_2.5_branch_gruntster/avidemux/ADM_libraries/ADM_ffmpeg/ADM_libswscale/swscale_altivec_template.c
===================================================================
--- branches/avidemux_2.5_branch_gruntster/avidemux/ADM_libraries/ADM_ffmpeg/ADM_libswscale/swscale_altivec_template.c	2008-03-21 06:48:49 UTC (rev 3911)
+++ branches/avidemux_2.5_branch_gruntster/avidemux/ADM_libraries/ADM_ffmpeg/ADM_libswscale/swscale_altivec_template.c	2008-03-21 06:51:32 UTC (rev 3912)
@@ -2,7 +2,7 @@
  * AltiVec-enhanced yuv2yuvX
  *
  * Copyright (C) 2004 Romain Dolbeau &lt;<A HREF="https://lists.berlios.de/mailman/listinfo/avidemux-svn-commit">romain at dolbeau.org</A>&gt;
- * based on the equivalent C code in &quot;postproc/swscale.c&quot;
+ * based on the equivalent C code in swscale.c
  *
  * This file is part of FFmpeg.
  *
@@ -21,22 +21,16 @@
  * Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA
  */
 
-#ifdef CONFIG_DARWIN
-#define AVV(x...) (x)
-#else
-#define AVV(x...) {x}
-#endif
-
 #define vzero vec_splat_s32(0)
 
 static inline void
 altivec_packIntArrayToCharArray(int *val, uint8_t* dest, int dstW) {
     register int i;
     vector unsigned int altivec_vectorShiftInt19 =
-        vec_add(vec_splat_u32(10),vec_splat_u32(9));
+        vec_add(vec_splat_u32(10), vec_splat_u32(9));
     if ((unsigned long)dest % 16) {
-        /* badly aligned store, we force store alignement */
-        /* and will handle load misalignement on val w/ vec_perm */
+        /* badly aligned store, we force store alignment */
+        /* and will handle load misalignment on val w/ vec_perm */
         vector unsigned char perm1;
         vector signed int v1;
         for (i = 0 ; (i &lt; dstW) &amp;&amp;
@@ -52,10 +46,10 @@
             vector signed int v3 = vec_ld(offset + 32, val);
             vector signed int v4 = vec_ld(offset + 48, val);
             vector signed int v5 = vec_ld(offset + 64, val);
-            vector signed int v12 = vec_perm(v1,v2,perm1);
-            vector signed int v23 = vec_perm(v2,v3,perm1);
-            vector signed int v34 = vec_perm(v3,v4,perm1);
-            vector signed int v45 = vec_perm(v4,v5,perm1);
+            vector signed int v12 = vec_perm(v1, v2, perm1);
+            vector signed int v23 = vec_perm(v2, v3, perm1);
+            vector signed int v34 = vec_perm(v3, v4, perm1);
+            vector signed int v45 = vec_perm(v4, v5, perm1);
 
             vector signed int vA = vec_sra(v12, altivec_vectorShiftInt19);
             vector signed int vB = vec_sra(v23, altivec_vectorShiftInt19);
@@ -143,7 +137,7 @@
                 val[i] += lumSrc[j][i] * lumFilter[j];
             }
         }
-        altivec_packIntArrayToCharArray(val,dest,dstW);
+        altivec_packIntArrayToCharArray(val, dest, dstW);
     }
     if (uDest != 0) {
         int  __attribute__ ((aligned (16))) u[chrDstW];
@@ -209,8 +203,8 @@
                 v[i] += chrSrc[j][i + 2048] * chrFilter[j];
             }
         }
-        altivec_packIntArrayToCharArray(u,uDest,chrDstW);
-        altivec_packIntArrayToCharArray(v,vDest,chrDstW);
+        altivec_packIntArrayToCharArray(u, uDest, chrDstW);
+        altivec_packIntArrayToCharArray(v, vDest, chrDstW);
     }
 }
 
@@ -258,9 +252,9 @@
         // and we're going to use vec_mule, so we chose
         // carefully how to &quot;unpack&quot; the elements into the even slots
         if ((i &lt;&lt; 3) % 16)
-            filter_v = vec_mergel(filter_v,(vector signed short)vzero);
+            filter_v = vec_mergel(filter_v, (vector signed short)vzero);
         else
-            filter_v = vec_mergeh(filter_v,(vector signed short)vzero);
+            filter_v = vec_mergeh(filter_v, (vector signed short)vzero);
 
         val_vEven = vec_mule(src_v, filter_v);
         val_s = vec_sums(val_vEven, vzero);
@@ -360,7 +354,7 @@
             src_v0 = src_v1;
         }
 
-        if (j &lt; (filterSize-7)) {
+        if (j &lt; filterSize-7) {
             // loading src_v0 is useless, it's already done above
             //vector unsigned char src_v0 = vec_ld(srcPos + j, src);
             vector unsigned char src_v1, src_vF;
@@ -393,7 +387,7 @@
 static inline int yv12toyuy2_unscaled_altivec(SwsContext *c, uint8_t* src[], int srcStride[], int srcSliceY,
                                               int srcSliceH, uint8_t* dstParam[], int dstStride_a[]) {
     uint8_t *dst=dstParam[0] + dstStride_a[0]*srcSliceY;
-    // yv12toyuy2( src[0],src[1],src[2],dst,c-&gt;srcW,srcSliceH,srcStride[0],srcStride[1],dstStride[0] );
+    // yv12toyuy2(src[0], src[1], src[2], dst, c-&gt;srcW, srcSliceH, srcStride[0], srcStride[1], dstStride[0]);
     uint8_t *ysrc = src[0];
     uint8_t *usrc = src[1];
     uint8_t *vsrc = src[2];
@@ -407,7 +401,7 @@
     register unsigned int y;
 
     if (width&amp;15) {
-        yv12toyuy2( ysrc, usrc, vsrc, dst,c-&gt;srcW,srcSliceH, lumStride, chromStride, dstStride);
+        yv12toyuy2(ysrc, usrc, vsrc, dst, c-&gt;srcW, srcSliceH, lumStride, chromStride, dstStride);
         return srcSliceH;
     }
 
@@ -456,7 +450,7 @@
             vec_st(v_yuy2_0, (i &lt;&lt; 1), dst);
             vec_st(v_yuy2_1, (i &lt;&lt; 1) + 16, dst);
         }
-        if ( (y&amp;(vertLumPerChroma-1))==(vertLumPerChroma-1) ) {
+        if ((y&amp;(vertLumPerChroma-1)) == vertLumPerChroma-1) {
             usrc += chromStride;
             vsrc += chromStride;
         }
@@ -470,7 +464,7 @@
 static inline int yv12touyvy_unscaled_altivec(SwsContext *c, uint8_t* src[], int srcStride[], int srcSliceY,
                                               int srcSliceH, uint8_t* dstParam[], int dstStride_a[]) {
     uint8_t *dst=dstParam[0] + dstStride_a[0]*srcSliceY;
-    // yv12toyuy2( src[0],src[1],src[2],dst,c-&gt;srcW,srcSliceH,srcStride[0],srcStride[1],dstStride[0] );
+    // yv12toyuy2(src[0], src[1], src[2], dst, c-&gt;srcW, srcSliceH, srcStride[0], srcStride[1], dstStride[0]);
     uint8_t *ysrc = src[0];
     uint8_t *usrc = src[1];
     uint8_t *vsrc = src[2];
@@ -484,7 +478,7 @@
     register unsigned int y;
 
     if (width&amp;15) {
-        yv12touyvy( ysrc, usrc, vsrc, dst,c-&gt;srcW,srcSliceH, lumStride, chromStride, dstStride);
+        yv12touyvy(ysrc, usrc, vsrc, dst, c-&gt;srcW, srcSliceH, lumStride, chromStride, dstStride);
         return srcSliceH;
     }
 
@@ -533,7 +527,7 @@
             vec_st(v_uyvy_0, (i &lt;&lt; 1), dst);
             vec_st(v_uyvy_1, (i &lt;&lt; 1) + 16, dst);
         }
-        if ( (y&amp;(vertLumPerChroma-1))==(vertLumPerChroma-1) ) {
+        if ((y&amp;(vertLumPerChroma-1)) == vertLumPerChroma-1) {
             usrc += chromStride;
             vsrc += chromStride;
         }

Modified: branches/avidemux_2.5_branch_gruntster/avidemux/ADM_libraries/ADM_ffmpeg/ADM_libswscale/swscale_internal.h
===================================================================
--- branches/avidemux_2.5_branch_gruntster/avidemux/ADM_libraries/ADM_ffmpeg/ADM_libswscale/swscale_internal.h	2008-03-21 06:48:49 UTC (rev 3911)
+++ branches/avidemux_2.5_branch_gruntster/avidemux/ADM_libraries/ADM_ffmpeg/ADM_libswscale/swscale_internal.h	2008-03-21 06:51:32 UTC (rev 3912)
@@ -18,23 +18,22 @@
  * Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA
  */
 
-#ifndef SWSCALE_INTERNAL_H
-#define SWSCALE_INTERNAL_H
+#ifndef FFMPEG_SWSCALE_INTERNAL_H
+#define FFMPEG_SWSCALE_INTERNAL_H
 
+#include &quot;config.h&quot;
+
 #ifdef HAVE_ALTIVEC_H
 #include &lt;altivec.h&gt;
 #endif
 
 #include &quot;avutil.h&quot;
 
-#ifdef CONFIG_DARWIN
-#define AVV(x...) (x)
-#else
-#define AVV(x...) {x}
-#endif
-
 #define MAX_FILTER_SIZE 256
 
+#define VOFW 2048
+#define VOF  (VOFW*2)
+
 typedef int (*SwsFunc)(struct SwsContext *context, uint8_t* src[], int srcStride[], int srcSliceY,
              int srcSliceH, uint8_t* dst[], int dstStride[]);
 
@@ -43,11 +42,11 @@
     /**
      * info on struct for av_log
      */
-    AVClass *av_class;
+    const AVClass *av_class;
 
     /**
-     *
-     * Note the src,dst,srcStride,dstStride will be copied, in the sws_scale() warper so they can freely be modified here
+     * Note that src, dst, srcStride, dstStride will be copied in the
+     * sws_scale() wrapper so they can be freely modified here.
      */
     SwsFunc swScale;
     int srcW, srcH, dstH;
@@ -74,7 +73,7 @@
     int16_t *vChrFilter;
     int16_t *vChrFilterPos;
 
-    uint8_t formatConvBuffer[4000]; //FIXME dynamic alloc, but we have to change a lot of code for this to be useful
+    uint8_t formatConvBuffer[VOF]; //FIXME dynamic alloc, but we have to change a lot of code for this to be useful
 
     int hLumFilterSize;
     int hChrFilterSize;
@@ -177,6 +176,9 @@
     uint32_t gmask        __attribute__((aligned(4)));
 #endif
 
+#ifdef HAVE_VIS
+    uint64_t sparc_coeffs[10] __attribute__((aligned(8)));
+#endif
 
 } SwsContext;
 //FIXME check init (where 0)
@@ -184,8 +186,15 @@
 SwsFunc yuv2rgb_get_func_ptr (SwsContext *c);
 int yuv2rgb_c_init_tables (SwsContext *c, const int inv_table[4], int fullRange, int brightness, int contrast, int saturation);
 
-char *sws_format_name(int format);
+void yuv2rgb_altivec_init_tables (SwsContext *c, const int inv_table[4],int brightness,int contrast, int saturation);
+SwsFunc yuv2rgb_init_altivec (SwsContext *c);
+void altivec_yuv2packedX (SwsContext *c,
+                          int16_t *lumFilter, int16_t **lumSrc, int lumFilterSize,
+                          int16_t *chrFilter, int16_t **chrSrc, int chrFilterSize,
+                          uint8_t *dest, int dstW, int dstY);
 
+const char *sws_format_name(int format);
+
 //FIXME replace this with something faster
 #define isPlanarYUV(x)  (           \
            (x)==PIX_FMT_YUV410P     \
@@ -193,6 +202,7 @@
         || (x)==PIX_FMT_YUV411P     \
         || (x)==PIX_FMT_YUV422P     \
         || (x)==PIX_FMT_YUV444P     \
+        || (x)==PIX_FMT_YUV440P     \
         || (x)==PIX_FMT_NV12        \
         || (x)==PIX_FMT_NV21        \
     )
@@ -265,4 +275,7 @@
     }
 }
 
-#endif
+extern const DECLARE_ALIGNED(8, uint64_t, ff_dither4[2]);
+extern const DECLARE_ALIGNED(8, uint64_t, ff_dither8[2]);
+
+#endif /* FFMPEG_SWSCALE_INTERNAL_H */

Modified: branches/avidemux_2.5_branch_gruntster/avidemux/ADM_libraries/ADM_ffmpeg/ADM_libswscale/swscale_template.c
===================================================================
--- branches/avidemux_2.5_branch_gruntster/avidemux/ADM_libraries/ADM_ffmpeg/ADM_libswscale/swscale_template.c	2008-03-21 06:48:49 UTC (rev 3911)
+++ branches/avidemux_2.5_branch_gruntster/avidemux/ADM_libraries/ADM_ffmpeg/ADM_libswscale/swscale_template.c	2008-03-21 06:51:32 UTC (rev 3912)
@@ -39,7 +39,7 @@
 #ifdef HAVE_3DNOW
 #define PREFETCH  &quot;prefetch&quot;
 #define PREFETCHW &quot;prefetchw&quot;
-#elif defined ( HAVE_MMX2 )
+#elif defined (HAVE_MMX2)
 #define PREFETCH &quot;prefetchnta&quot;
 #define PREFETCHW &quot;prefetcht0&quot;
 #else
@@ -80,8 +80,8 @@
     ASMALIGN(4) /* FIXME Unroll? */\
     &quot;1:                                                 \n\t&quot;\
     &quot;movq                      8(%%&quot;REG_d&quot;), %%mm0      \n\t&quot; /* filterCoeff */\
-    &quot;movq   &quot; #x &quot;(%%&quot;REG_S&quot;, %%&quot;REG_a&quot;, 2), %%mm2      \n\t&quot; /* srcData */\
-    &quot;movq 8+&quot; #x &quot;(%%&quot;REG_S&quot;, %%&quot;REG_a&quot;, 2), %%mm5      \n\t&quot; /* srcData */\
+    &quot;movq   &quot;  x &quot;(%%&quot;REG_S&quot;, %%&quot;REG_a&quot;, 2), %%mm2      \n\t&quot; /* srcData */\
+    &quot;movq 8+&quot;  x &quot;(%%&quot;REG_S&quot;, %%&quot;REG_a&quot;, 2), %%mm5      \n\t&quot; /* srcData */\
     &quot;add                                $16, %%&quot;REG_d&quot;  \n\t&quot;\
     &quot;mov                        (%%&quot;REG_d&quot;), %%&quot;REG_S&quot;  \n\t&quot;\
     &quot;test                         %%&quot;REG_S&quot;, %%&quot;REG_S&quot;  \n\t&quot;\
@@ -117,10 +117,10 @@
     &quot;mov                        (%%&quot;REG_d&quot;), %%&quot;REG_S&quot;  \n\t&quot;\
     ASMALIGN(4) \
     &quot;1:                                                 \n\t&quot;\
-    &quot;movq   &quot; #x &quot;(%%&quot;REG_S&quot;, %%&quot;REG_a&quot;, 2), %%mm0      \n\t&quot; /* srcData */\
-    &quot;movq 8+&quot; #x &quot;(%%&quot;REG_S&quot;, %%&quot;REG_a&quot;, 2), %%mm2      \n\t&quot; /* srcData */\
+    &quot;movq   &quot;  x &quot;(%%&quot;REG_S&quot;, %%&quot;REG_a&quot;, 2), %%mm0      \n\t&quot; /* srcData */\
+    &quot;movq 8+&quot;  x &quot;(%%&quot;REG_S&quot;, %%&quot;REG_a&quot;, 2), %%mm2      \n\t&quot; /* srcData */\
     &quot;mov                       4(%%&quot;REG_d&quot;), %%&quot;REG_S&quot;  \n\t&quot;\
-    &quot;movq   &quot; #x &quot;(%%&quot;REG_S&quot;, %%&quot;REG_a&quot;, 2), %%mm1      \n\t&quot; /* srcData */\
+    &quot;movq   &quot;  x &quot;(%%&quot;REG_S&quot;, %%&quot;REG_a&quot;, 2), %%mm1      \n\t&quot; /* srcData */\
     &quot;movq                             %%mm0, %%mm3      \n\t&quot;\
     &quot;punpcklwd                        %%mm1, %%mm0      \n\t&quot;\
     &quot;punpckhwd                        %%mm1, %%mm3      \n\t&quot;\
@@ -129,7 +129,7 @@
     &quot;pmaddwd                          %%mm1, %%mm3      \n\t&quot;\
     &quot;paddd                            %%mm0, %%mm4      \n\t&quot;\
     &quot;paddd                            %%mm3, %%mm5      \n\t&quot;\
-    &quot;movq 8+&quot; #x &quot;(%%&quot;REG_S&quot;, %%&quot;REG_a&quot;, 2), %%mm3      \n\t&quot; /* srcData */\
+    &quot;movq 8+&quot;  x &quot;(%%&quot;REG_S&quot;, %%&quot;REG_a&quot;, 2), %%mm3      \n\t&quot; /* srcData */\
     &quot;mov                      16(%%&quot;REG_d&quot;), %%&quot;REG_S&quot;  \n\t&quot;\
     &quot;add                                $16, %%&quot;REG_d&quot;  \n\t&quot;\
     &quot;test                         %%&quot;REG_S&quot;, %%&quot;REG_S&quot;  \n\t&quot;\
@@ -202,7 +202,7 @@
     &quot;2:                                             \n\t&quot;\
     &quot;movq               8(%%&quot;REG_d&quot;), %%mm0         \n\t&quot; /* filterCoeff */\
     &quot;movq     (%%&quot;REG_S&quot;, %%&quot;REG_a&quot;), %%mm2         \n\t&quot; /* UsrcData */\
-    &quot;movq 4096(%%&quot;REG_S&quot;, %%&quot;REG_a&quot;), %%mm5         \n\t&quot; /* VsrcData */\
+    &quot;movq &quot;AV_STRINGIFY(VOF)&quot;(%%&quot;REG_S&quot;, %%&quot;REG_a&quot;), %%mm5         \n\t&quot; /* VsrcData */\
     &quot;add                         $16, %%&quot;REG_d&quot;     \n\t&quot;\
     &quot;mov                 (%%&quot;REG_d&quot;), %%&quot;REG_S&quot;     \n\t&quot;\
     &quot;pmulhw                    %%mm0, %%mm2         \n\t&quot;\
@@ -252,7 +252,7 @@
     ASMALIGN(4)\
     &quot;2:                                             \n\t&quot;\
     &quot;movq     (%%&quot;REG_S&quot;, %%&quot;REG_a&quot;), %%mm0         \n\t&quot; /* UsrcData */\
-    &quot;movq 4096(%%&quot;REG_S&quot;, %%&quot;REG_a&quot;), %%mm2         \n\t&quot; /* VsrcData */\
+    &quot;movq &quot;AV_STRINGIFY(VOF)&quot;(%%&quot;REG_S&quot;, %%&quot;REG_a&quot;), %%mm2         \n\t&quot; /* VsrcData */\
     &quot;mov                4(%%&quot;REG_d&quot;), %%&quot;REG_S&quot;     \n\t&quot;\
     &quot;movq     (%%&quot;REG_S&quot;, %%&quot;REG_a&quot;), %%mm1         \n\t&quot; /* UsrcData */\
     &quot;movq                      %%mm0, %%mm3         \n\t&quot;\
@@ -263,7 +263,7 @@
     &quot;pmaddwd                   %%mm1, %%mm3         \n\t&quot;\
     &quot;paddd                     %%mm0, %%mm4         \n\t&quot;\
     &quot;paddd                     %%mm3, %%mm5         \n\t&quot;\
-    &quot;movq 4096(%%&quot;REG_S&quot;, %%&quot;REG_a&quot;), %%mm3         \n\t&quot; /* VsrcData */\
+    &quot;movq &quot;AV_STRINGIFY(VOF)&quot;(%%&quot;REG_S&quot;, %%&quot;REG_a&quot;), %%mm3         \n\t&quot; /* VsrcData */\
     &quot;mov               16(%%&quot;REG_d&quot;), %%&quot;REG_S&quot;     \n\t&quot;\
     &quot;add                         $16, %%&quot;REG_d&quot;     \n\t&quot;\
     &quot;test                  %%&quot;REG_S&quot;, %%&quot;REG_S&quot;     \n\t&quot;\
@@ -388,10 +388,10 @@
     &quot;pmulhw               %%mm6, %%mm0  \n\t&quot; /* (buf0[eax] - buf1[eax])yalpha1&gt;&gt;16*/\
     &quot;pmulhw               %%mm5, %%mm2  \n\t&quot; /* (uvbuf0[eax] - uvbuf1[eax])uvalpha1&gt;&gt;16*/\
     &quot;psraw                   $4, %%mm1  \n\t&quot; /* buf0[eax] - buf1[eax] &gt;&gt;4*/\
-    &quot;movq 4096(%2, %%&quot;REG_a&quot;,2), %%mm4  \n\t&quot; /* uvbuf0[eax+2048]*/\
+    &quot;movq &quot;AV_STRINGIFY(VOF)&quot;(%2, %%&quot;REG_a&quot;,2), %%mm4  \n\t&quot; /* uvbuf0[eax+2048]*/\
     &quot;psraw                   $4, %%mm3  \n\t&quot; /* uvbuf0[eax] - uvbuf1[eax] &gt;&gt;4*/\
     &quot;paddw                %%mm0, %%mm1  \n\t&quot; /* buf0[eax]yalpha1 + buf1[eax](1-yalpha1) &gt;&gt;16*/\
-    &quot;movq 4096(%3, %%&quot;REG_a&quot;,2), %%mm0  \n\t&quot; /* uvbuf1[eax+2048]*/\
+    &quot;movq &quot;AV_STRINGIFY(VOF)&quot;(%3, %%&quot;REG_a&quot;,2), %%mm0  \n\t&quot; /* uvbuf1[eax+2048]*/\
     &quot;paddw                %%mm2, %%mm3  \n\t&quot; /* uvbuf0[eax]uvalpha1 - uvbuf1[eax](1-uvalpha1)*/\
     &quot;psubw                %%mm0, %%mm4  \n\t&quot; /* uvbuf0[eax+2048] - uvbuf1[eax+2048]*/\
     &quot;psubw        &quot;MANGLE(w80)&quot;, %%mm1  \n\t&quot; /* 8(Y-16)*/\
@@ -434,8 +434,8 @@
     &quot;1:                                 \n\t&quot;\
     &quot;movq     (%2, &quot;#index&quot;), %%mm2     \n\t&quot; /* uvbuf0[eax]*/\
     &quot;movq     (%3, &quot;#index&quot;), %%mm3     \n\t&quot; /* uvbuf1[eax]*/\
-    &quot;movq 4096(%2, &quot;#index&quot;), %%mm5     \n\t&quot; /* uvbuf0[eax+2048]*/\
-    &quot;movq 4096(%3, &quot;#index&quot;), %%mm4     \n\t&quot; /* uvbuf1[eax+2048]*/\
+    &quot;movq &quot;AV_STRINGIFY(VOF)&quot;(%2, &quot;#index&quot;), %%mm5     \n\t&quot; /* uvbuf0[eax+2048]*/\
+    &quot;movq &quot;AV_STRINGIFY(VOF)&quot;(%3, &quot;#index&quot;), %%mm4     \n\t&quot; /* uvbuf1[eax+2048]*/\
     &quot;psubw             %%mm3, %%mm2     \n\t&quot; /* uvbuf0[eax] - uvbuf1[eax]*/\
     &quot;psubw             %%mm4, %%mm5     \n\t&quot; /* uvbuf0[eax+2048] - uvbuf1[eax+2048]*/\
     &quot;movq &quot;CHR_MMX_FILTER_OFFSET&quot;+8(&quot;#c&quot;), %%mm0    \n\t&quot;\
@@ -466,8 +466,8 @@
     &quot;1:                                 \n\t&quot;\
     &quot;movq     (%2, &quot;#index&quot;), %%mm2     \n\t&quot; /* uvbuf0[eax]*/\
     &quot;movq     (%3, &quot;#index&quot;), %%mm3     \n\t&quot; /* uvbuf1[eax]*/\
-    &quot;movq 4096(%2, &quot;#index&quot;), %%mm5     \n\t&quot; /* uvbuf0[eax+2048]*/\
-    &quot;movq 4096(%3, &quot;#index&quot;), %%mm4     \n\t&quot; /* uvbuf1[eax+2048]*/\
+    &quot;movq &quot;AV_STRINGIFY(VOF)&quot;(%2, &quot;#index&quot;), %%mm5     \n\t&quot; /* uvbuf0[eax+2048]*/\
+    &quot;movq &quot;AV_STRINGIFY(VOF)&quot;(%3, &quot;#index&quot;), %%mm4     \n\t&quot; /* uvbuf1[eax+2048]*/\
     &quot;psubw             %%mm3, %%mm2     \n\t&quot; /* uvbuf0[eax] - uvbuf1[eax]*/\
     &quot;psubw             %%mm4, %%mm5     \n\t&quot; /* uvbuf0[eax+2048] - uvbuf1[eax+2048]*/\
     &quot;movq &quot;CHR_MMX_FILTER_OFFSET&quot;+8(&quot;#c&quot;), %%mm0    \n\t&quot;\
@@ -531,7 +531,7 @@
     ASMALIGN(4)\
     &quot;1:                                 \n\t&quot;\
     &quot;movq     (%2, &quot;#index&quot;), %%mm3     \n\t&quot; /* uvbuf0[eax]*/\
-    &quot;movq 4096(%2, &quot;#index&quot;), %%mm4     \n\t&quot; /* uvbuf0[eax+2048]*/\
+    &quot;movq &quot;AV_STRINGIFY(VOF)&quot;(%2, &quot;#index&quot;), %%mm4     \n\t&quot; /* uvbuf0[eax+2048]*/\
     &quot;psraw                $7, %%mm3     \n\t&quot; \
     &quot;psraw                $7, %%mm4     \n\t&quot; \
     &quot;movq  (%0, &quot;#index&quot;, 2), %%mm1     \n\t&quot; /*buf0[eax]*/\
@@ -546,7 +546,7 @@
     ASMALIGN(4)\
     &quot;1:                                 \n\t&quot;\
     &quot;movq     (%2, &quot;#index&quot;), %%mm3     \n\t&quot; /* uvbuf0[eax]*/\
-    &quot;movq 4096(%2, &quot;#index&quot;), %%mm4     \n\t&quot; /* uvbuf0[eax+2048]*/\
+    &quot;movq &quot;AV_STRINGIFY(VOF)&quot;(%2, &quot;#index&quot;), %%mm4     \n\t&quot; /* uvbuf0[eax+2048]*/\
     &quot;psraw                $4, %%mm3     \n\t&quot; /* uvbuf0[eax] - uvbuf1[eax] &gt;&gt;4*/\
     &quot;psraw                $4, %%mm4     \n\t&quot; /* uvbuf0[eax+2048] - uvbuf1[eax+2048] &gt;&gt;4*/\
     &quot;psubw  &quot;U_OFFSET&quot;(&quot;#c&quot;), %%mm3     \n\t&quot; /* (U-128)8*/\
@@ -596,8 +596,8 @@
     &quot;1:                                 \n\t&quot;\
     &quot;movq     (%2, &quot;#index&quot;), %%mm2     \n\t&quot; /* uvbuf0[eax]*/\
     &quot;movq     (%3, &quot;#index&quot;), %%mm3     \n\t&quot; /* uvbuf1[eax]*/\
-    &quot;movq 4096(%2, &quot;#index&quot;), %%mm5     \n\t&quot; /* uvbuf0[eax+2048]*/\
-    &quot;movq 4096(%3, &quot;#index&quot;), %%mm4     \n\t&quot; /* uvbuf1[eax+2048]*/\
+    &quot;movq &quot;AV_STRINGIFY(VOF)&quot;(%2, &quot;#index&quot;), %%mm5     \n\t&quot; /* uvbuf0[eax+2048]*/\
+    &quot;movq &quot;AV_STRINGIFY(VOF)&quot;(%3, &quot;#index&quot;), %%mm4     \n\t&quot; /* uvbuf1[eax+2048]*/\
     &quot;paddw             %%mm2, %%mm3     \n\t&quot; /* uvbuf0[eax] + uvbuf1[eax]*/\
     &quot;paddw             %%mm5, %%mm4     \n\t&quot; /* uvbuf0[eax+2048] + uvbuf1[eax+2048]*/\
     &quot;psrlw                $8, %%mm3     \n\t&quot; \
@@ -615,8 +615,8 @@
     &quot;1:                                 \n\t&quot;\
     &quot;movq     (%2, &quot;#index&quot;), %%mm2     \n\t&quot; /* uvbuf0[eax]*/\
     &quot;movq     (%3, &quot;#index&quot;), %%mm3     \n\t&quot; /* uvbuf1[eax]*/\
-    &quot;movq 4096(%2, &quot;#index&quot;), %%mm5     \n\t&quot; /* uvbuf0[eax+2048]*/\
-    &quot;movq 4096(%3, &quot;#index&quot;), %%mm4     \n\t&quot; /* uvbuf1[eax+2048]*/\
+    &quot;movq &quot;AV_STRINGIFY(VOF)&quot;(%2, &quot;#index&quot;), %%mm5     \n\t&quot; /* uvbuf0[eax+2048]*/\
+    &quot;movq &quot;AV_STRINGIFY(VOF)&quot;(%3, &quot;#index&quot;), %%mm4     \n\t&quot; /* uvbuf1[eax+2048]*/\
     &quot;paddw             %%mm2, %%mm3     \n\t&quot; /* uvbuf0[eax] + uvbuf1[eax]*/\
     &quot;paddw             %%mm5, %%mm4     \n\t&quot; /* uvbuf0[eax+2048] + uvbuf1[eax+2048]*/\
     &quot;psrlw                $5, %%mm3     \n\t&quot; /*FIXME might overflow*/\
@@ -855,8 +855,8 @@
 
 #define WRITEBGR24MMX2(dst, dstw, index) \
     /* mm2=B, %%mm4=G, %%mm5=R, %%mm7=0 */\
-    &quot;movq &quot;MANGLE(M24A)&quot;, %%mm0 \n\t&quot;\
-    &quot;movq &quot;MANGLE(M24C)&quot;, %%mm7 \n\t&quot;\
+    &quot;movq &quot;MANGLE(ff_M24A)&quot;, %%mm0 \n\t&quot;\
+    &quot;movq &quot;MANGLE(ff_M24C)&quot;, %%mm7 \n\t&quot;\
     &quot;pshufw $0x50, %%mm2, %%mm1 \n\t&quot; /* B3 B2 B3 B2  B1 B0 B1 B0 */\
     &quot;pshufw $0x50, %%mm4, %%mm3 \n\t&quot; /* G3 G2 G3 G2  G1 G0 G1 G0 */\
     &quot;pshufw $0x00, %%mm5, %%mm6 \n\t&quot; /* R1 R0 R1 R0  R1 R0 R1 R0 */\
@@ -875,7 +875,7 @@
     &quot;pshufw $0x55, %%mm4, %%mm3 \n\t&quot; /* G4 G3 G4 G3  G4 G3 G4 G3 */\
     &quot;pshufw $0xA5, %%mm5, %%mm6 \n\t&quot; /* R5 R4 R5 R4  R3 R2 R3 R2 */\
 \
-    &quot;pand &quot;MANGLE(M24B)&quot;, %%mm1 \n\t&quot; /* B5       B4        B3    */\
+    &quot;pand &quot;MANGLE(ff_M24B)&quot;, %%mm1 \n\t&quot; /* B5       B4        B3    */\
     &quot;pand   %%mm7, %%mm3        \n\t&quot; /*       G4        G3       */\
     &quot;pand   %%mm0, %%mm6        \n\t&quot; /*    R4        R3       R2 */\
 \
@@ -889,7 +889,7 @@
 \
     &quot;pand   %%mm7, %%mm1        \n\t&quot; /*       B7        B6       */\
     &quot;pand   %%mm0, %%mm3        \n\t&quot; /*    G7        G6       G5 */\
-    &quot;pand &quot;MANGLE(M24B)&quot;, %%mm6 \n\t&quot; /* R7       R6        R5    */\
+    &quot;pand &quot;MANGLE(ff_M24B)&quot;, %%mm6 \n\t&quot; /* R7       R6        R5    */\
 \
     &quot;por    %%mm1, %%mm3        \n\t&quot;\
     &quot;por    %%mm3, %%mm6        \n\t&quot;\
@@ -934,18 +934,18 @@
 #ifdef HAVE_MMX
     if (c-&gt;flags &amp; SWS_ACCURATE_RND){
         if (uDest){
-            YSCALEYUV2YV12X_ACCURATE(   0, CHR_MMX_FILTER_OFFSET, uDest, chrDstW)
-            YSCALEYUV2YV12X_ACCURATE(4096, CHR_MMX_FILTER_OFFSET, vDest, chrDstW)
+            YSCALEYUV2YV12X_ACCURATE(   &quot;0&quot;, CHR_MMX_FILTER_OFFSET, uDest, chrDstW)
+            YSCALEYUV2YV12X_ACCURATE(AV_STRINGIFY(VOF), CHR_MMX_FILTER_OFFSET, vDest, chrDstW)
         }
 
-        YSCALEYUV2YV12X_ACCURATE(0, LUM_MMX_FILTER_OFFSET, dest, dstW)
+        YSCALEYUV2YV12X_ACCURATE(&quot;0&quot;, LUM_MMX_FILTER_OFFSET, dest, dstW)
     }else{
         if (uDest){
-            YSCALEYUV2YV12X(   0, CHR_MMX_FILTER_OFFSET, uDest, chrDstW)
-            YSCALEYUV2YV12X(4096, CHR_MMX_FILTER_OFFSET, vDest, chrDstW)
+            YSCALEYUV2YV12X(   &quot;0&quot;, CHR_MMX_FILTER_OFFSET, uDest, chrDstW)
+            YSCALEYUV2YV12X(AV_STRINGIFY(VOF), CHR_MMX_FILTER_OFFSET, vDest, chrDstW)
         }
 
-        YSCALEYUV2YV12X(0, LUM_MMX_FILTER_OFFSET, dest, dstW)
+        YSCALEYUV2YV12X(&quot;0&quot;, LUM_MMX_FILTER_OFFSET, dest, dstW)
     }
 #else
 #ifdef HAVE_ALTIVEC
@@ -957,7 +957,7 @@
             chrFilter, chrSrc, chrFilterSize,
             dest, uDest, vDest, dstW, chrDstW);
 #endif //!HAVE_ALTIVEC
-#endif
+#endif /* HAVE_MMX */
 }
 
 static inline void RENAME(yuv2nv12X)(SwsContext *c, int16_t *lumFilter, int16_t **lumSrc, int lumFilterSize,
@@ -973,7 +973,7 @@
                                     uint8_t *dest, uint8_t *uDest, uint8_t *vDest, long dstW, long chrDstW)
 {
 #ifdef HAVE_MMX
-    if (uDest != NULL)
+    if (uDest)
     {
         asm volatile(
             YSCALEYUV2YV121
@@ -984,7 +984,7 @@
 
         asm volatile(
             YSCALEYUV2YV121
-            :: &quot;r&quot; (chrSrc + 2048 + chrDstW), &quot;r&quot; (vDest + chrDstW),
+            :: &quot;r&quot; (chrSrc + VOFW + chrDstW), &quot;r&quot; (vDest + chrDstW),
             &quot;g&quot; (-chrDstW)
             : &quot;%&quot;REG_a
         );
@@ -1010,11 +1010,11 @@
         dest[i]= val;
     }
 
-    if (uDest != NULL)
+    if (uDest)
         for (i=0; i&lt;chrDstW; i++)
         {
             int u=chrSrc[i]&gt;&gt;7;
-            int v=chrSrc[i + 2048]&gt;&gt;7;
+            int v=chrSrc[i + VOFW]&gt;&gt;7;
 
             if ((u|v)&amp;256){
                 if (u&lt;0)        u=0;
@@ -1161,7 +1161,7 @@
             return;
         }
     }
-#endif
+#endif /* HAVE_MMX */
 #ifdef HAVE_ALTIVEC
     /* The following list of supported dstFormat values should
        match what's found in the body of altivec_yuv2packedX() */
@@ -1333,7 +1333,7 @@
             : &quot;%&quot;REG_a
             );
             break;
-#endif
+#endif /* HAVE_MMX */
         case PIX_FMT_BGR32:
 #ifndef HAVE_MMX
         case PIX_FMT_RGB32:
@@ -1348,7 +1348,7 @@
                     // vertical linear interpolation &amp;&amp; yuv2rgb in a single step:
                     int Y=yuvtab_2568[((buf0[i]*yalpha1+buf1[i]*yalpha)&gt;&gt;19)];
                     int U=((uvbuf0[i]*uvalpha1+uvbuf1[i]*uvalpha)&gt;&gt;19);
-                    int V=((uvbuf0[i+2048]*uvalpha1+uvbuf1[i+2048]*uvalpha)&gt;&gt;19);
+                    int V=((uvbuf0[i+VOFW]*uvalpha1+uvbuf1[i+VOFW]*uvalpha)&gt;&gt;19);
                     dest[0]=clip_table[((Y + yuvtab_40cf[U]) &gt;&gt;13)];
                     dest[1]=clip_table[((Y + yuvtab_1a1e[V] + yuvtab_0c92[U]) &gt;&gt;13)];
                     dest[2]=clip_table[((Y + yuvtab_3343[V]) &gt;&gt;13)];
@@ -1362,7 +1362,7 @@
                     // vertical linear interpolation &amp;&amp; yuv2rgb in a single step:
                     int Y=yuvtab_2568[((buf0[i]*yalpha1+buf1[i]*yalpha)&gt;&gt;19)];
                     int U=((uvbuf0[i]*uvalpha1+uvbuf1[i]*uvalpha)&gt;&gt;19);
-                    int V=((uvbuf0[i+2048]*uvalpha1+uvbuf1[i+2048]*uvalpha)&gt;&gt;19);
+                    int V=((uvbuf0[i+VOFW]*uvalpha1+uvbuf1[i+VOFW]*uvalpha)&gt;&gt;19);
                     dest[0]=clip_table[((Y + yuvtab_40cf[U]) &gt;&gt;13)];
                     dest[1]=clip_table[((Y + yuvtab_1a1e[V] + yuvtab_0c92[U]) &gt;&gt;13)];
                     dest[2]=clip_table[((Y + yuvtab_3343[V]) &gt;&gt;13)];
@@ -1376,7 +1376,7 @@
                     // vertical linear interpolation &amp;&amp; yuv2rgb in a single step:
                     int Y=yuvtab_2568[((buf0[i]*yalpha1+buf1[i]*yalpha)&gt;&gt;19)];
                     int U=((uvbuf0[i]*uvalpha1+uvbuf1[i]*uvalpha)&gt;&gt;19);
-                    int V=((uvbuf0[i+2048]*uvalpha1+uvbuf1[i+2048]*uvalpha)&gt;&gt;19);
+                    int V=((uvbuf0[i+VOFW]*uvalpha1+uvbuf1[i+VOFW]*uvalpha)&gt;&gt;19);
 
                     ((uint16_t*)dest)[i] =
                         clip_table16b[(Y + yuvtab_40cf[U]) &gt;&gt;13] |
@@ -1391,7 +1391,7 @@
                     // vertical linear interpolation &amp;&amp; yuv2rgb in a single step:
                     int Y=yuvtab_2568[((buf0[i]*yalpha1+buf1[i]*yalpha)&gt;&gt;19)];
                     int U=((uvbuf0[i]*uvalpha1+uvbuf1[i]*uvalpha)&gt;&gt;19);
-                    int V=((uvbuf0[i+2048]*uvalpha1+uvbuf1[i+2048]*uvalpha)&gt;&gt;19);
+                    int V=((uvbuf0[i+VOFW]*uvalpha1+uvbuf1[i+VOFW]*uvalpha)&gt;&gt;19);
 
                     ((uint16_t*)dest)[i] =
                         clip_table15b[(Y + yuvtab_40cf[U]) &gt;&gt;13] |
@@ -1513,7 +1513,7 @@
     }
 
 #ifdef HAVE_MMX
-    if ( uvalpha &lt; 2048 ) // note this is not correct (shifts chrominance by 0.5 pixels) but it is a bit faster
+    if (uvalpha &lt; 2048) // note this is not correct (shifts chrominance by 0.5 pixels) but it is a bit faster
     {
         switch(dstFormat)
         {
@@ -1691,8 +1691,8 @@
             return;
         }
     }
-#endif
-    if ( uvalpha &lt; 2048 )
+#endif /* HAVE_MMX */
+    if (uvalpha &lt; 2048)
     {
         YSCALE_YUV_2_ANYRGB_C(YSCALE_YUV_2_RGB1_C, YSCALE_YUV_2_PACKED1_C)
     }else{
@@ -1831,7 +1831,7 @@
         int g= (((uint32_t*)src)[i]&gt;&gt;8)&amp;0xFF;
         int r= (((uint32_t*)src)[i]&gt;&gt;16)&amp;0xFF;
 
-        dst[i]= ((RY*r + GY*g + BY*b + (33&lt;&lt;(RGB2YUV_SHIFT-1)) )&gt;&gt;RGB2YUV_SHIFT);
+        dst[i]= ((RY*r + GY*g + BY*b + (33&lt;&lt;(RGB2YUV_SHIFT-1)))&gt;&gt;RGB2YUV_SHIFT);
     }
 }
 
@@ -1859,8 +1859,8 @@
 #ifdef HAVE_MMX
     asm volatile(
     &quot;mov                        %2, %%&quot;REG_a&quot;   \n\t&quot;
-    &quot;movq     &quot;MANGLE(bgr2YCoeff)&quot;, %%mm6       \n\t&quot;
-    &quot;movq          &quot;MANGLE(w1111)&quot;, %%mm5       \n\t&quot;
+    &quot;movq  &quot;MANGLE(ff_bgr2YCoeff)&quot;, %%mm6       \n\t&quot;
+    &quot;movq       &quot;MANGLE(ff_w1111)&quot;, %%mm5       \n\t&quot;
     &quot;pxor                    %%mm7, %%mm7       \n\t&quot;
     &quot;lea (%%&quot;REG_a&quot;, %%&quot;REG_a&quot;, 2), %%&quot;REG_d&quot;   \n\t&quot;
     ASMALIGN(4)
@@ -1918,7 +1918,7 @@
     &quot;psraw                      $7, %%mm4       \n\t&quot;
 
     &quot;packuswb                %%mm4, %%mm0       \n\t&quot;
-    &quot;paddusb &quot;MANGLE(bgr2YOffset)&quot;, %%mm0       \n\t&quot;
+    &quot;paddusb &quot;MANGLE(ff_bgr2YOffset)&quot;, %%mm0    \n\t&quot;
 
     &quot;movq                    %%mm0, (%1, %%&quot;REG_a&quot;) \n\t&quot;
     &quot;add                        $8, %%&quot;REG_a&quot;   \n\t&quot;
@@ -1934,9 +1934,9 @@
         int g= src[i*3+1];
         int r= src[i*3+2];
 
-        dst[i]= ((RY*r + GY*g + BY*b + (33&lt;&lt;(RGB2YUV_SHIFT-1)) )&gt;&gt;RGB2YUV_SHIFT);
+        dst[i]= ((RY*r + GY*g + BY*b + (33&lt;&lt;(RGB2YUV_SHIFT-1)))&gt;&gt;RGB2YUV_SHIFT);
     }
-#endif
+#endif /* HAVE_MMX */
 }
 
 static inline void RENAME(bgr24ToUV)(uint8_t *dstU, uint8_t *dstV, uint8_t *src1, uint8_t *src2, long width)
@@ -1944,8 +1944,8 @@
 #ifdef HAVE_MMX
     asm volatile(
     &quot;mov                        %3, %%&quot;REG_a&quot;   \n\t&quot;
-    &quot;movq          &quot;MANGLE(w1111)&quot;, %%mm5       \n\t&quot;
-    &quot;movq     &quot;MANGLE(bgr2UCoeff)&quot;, %%mm6       \n\t&quot;
+    &quot;movq       &quot;MANGLE(ff_w1111)&quot;, %%mm5       \n\t&quot;
+    &quot;movq  &quot;MANGLE(ff_bgr2UCoeff)&quot;, %%mm6       \n\t&quot;
     &quot;pxor                    %%mm7, %%mm7       \n\t&quot;
     &quot;lea (%%&quot;REG_a&quot;, %%&quot;REG_a&quot;, 2), %%&quot;REG_d&quot;   \n\t&quot;
     &quot;add                 %%&quot;REG_d&quot;, %%&quot;REG_d&quot;   \n\t&quot;
@@ -1977,8 +1977,8 @@
     &quot;psrlw                      $1, %%mm0       \n\t&quot;
     &quot;psrlw                      $1, %%mm2       \n\t&quot;
 #endif
-    &quot;movq     &quot;MANGLE(bgr2VCoeff)&quot;, %%mm1       \n\t&quot;
-    &quot;movq     &quot;MANGLE(bgr2VCoeff)&quot;, %%mm3       \n\t&quot;
+    &quot;movq  &quot;MANGLE(ff_bgr2VCoeff)&quot;, %%mm1       \n\t&quot;
+    &quot;movq  &quot;MANGLE(ff_bgr2VCoeff)&quot;, %%mm3       \n\t&quot;
 
     &quot;pmaddwd                 %%mm0, %%mm1       \n\t&quot;
     &quot;pmaddwd                 %%mm2, %%mm3       \n\t&quot;
@@ -2019,12 +2019,12 @@
     &quot;punpcklbw              %%mm7, %%mm5       \n\t&quot;
     &quot;punpcklbw              %%mm7, %%mm2       \n\t&quot;
     &quot;paddw                  %%mm5, %%mm2       \n\t&quot;
-    &quot;movq         &quot;MANGLE(w1111)&quot;, %%mm5       \n\t&quot;
+    &quot;movq      &quot;MANGLE(ff_w1111)&quot;, %%mm5       \n\t&quot;
     &quot;psrlw                     $2, %%mm4       \n\t&quot;
     &quot;psrlw                     $2, %%mm2       \n\t&quot;
 #endif
-    &quot;movq    &quot;MANGLE(bgr2VCoeff)&quot;, %%mm1       \n\t&quot;
-    &quot;movq    &quot;MANGLE(bgr2VCoeff)&quot;, %%mm3       \n\t&quot;
+    &quot;movq &quot;MANGLE(ff_bgr2VCoeff)&quot;, %%mm1       \n\t&quot;
+    &quot;movq &quot;MANGLE(ff_bgr2VCoeff)&quot;, %%mm3       \n\t&quot;
 
     &quot;pmaddwd                %%mm4, %%mm1       \n\t&quot;
     &quot;pmaddwd                %%mm2, %%mm3       \n\t&quot;
@@ -2048,7 +2048,7 @@
     &quot;punpckldq              %%mm4, %%mm0       \n\t&quot;
     &quot;punpckhdq              %%mm4, %%mm1       \n\t&quot;
     &quot;packsswb               %%mm1, %%mm0       \n\t&quot;
-    &quot;paddb &quot;MANGLE(bgr2UVOffset)&quot;, %%mm0       \n\t&quot;
+    &quot;paddb &quot;MANGLE(ff_bgr2UVOffset)&quot;, %%mm0    \n\t&quot;
 
     &quot;movd                   %%mm0, (%1, %%&quot;REG_a&quot;)  \n\t&quot;
     &quot;punpckhdq              %%mm0, %%mm0            \n\t&quot;
@@ -2069,11 +2069,11 @@
         dstU[i]= ((RU*r + GU*g + BU*b)&gt;&gt;(RGB2YUV_SHIFT+1)) + 128;
         dstV[i]= ((RV*r + GV*g + BV*b)&gt;&gt;(RGB2YUV_SHIFT+1)) + 128;
     }
-#endif
+#endif /* HAVE_MMX */
     assert(src1 == src2);
 }
 
-static inline void RENAME(bgr16ToY)(uint8_t *dst, uint8_t *src, int width)
+static inline void RENAME(rgb16ToY)(uint8_t *dst, uint8_t *src, int width)
 {
     int i;
     for (i=0; i&lt;width; i++)
@@ -2087,7 +2087,7 @@
     }
 }
 
-static inline void RENAME(bgr16ToUV)(uint8_t *dstU, uint8_t *dstV, uint8_t *src1, uint8_t *src2, int width)
+static inline void RENAME(rgb16ToUV)(uint8_t *dstU, uint8_t *dstV, uint8_t *src1, uint8_t *src2, int width)
 {
     int i;
     assert(src1==src2);
@@ -2109,7 +2109,7 @@
     }
 }
 
-static inline void RENAME(bgr15ToY)(uint8_t *dst, uint8_t *src, int width)
+static inline void RENAME(rgb15ToY)(uint8_t *dst, uint8_t *src, int width)
 {
     int i;
     for (i=0; i&lt;width; i++)
@@ -2123,7 +2123,7 @@
     }
 }
 
-static inline void RENAME(bgr15ToUV)(uint8_t *dstU, uint8_t *dstV, uint8_t *src1, uint8_t *src2, int width)
+static inline void RENAME(rgb15ToUV)(uint8_t *dstU, uint8_t *dstV, uint8_t *src1, uint8_t *src2, int width)
 {
     int i;
     assert(src1==src2);
@@ -2155,7 +2155,7 @@
         int g= (((uint32_t*)src)[i]&gt;&gt;8)&amp;0xFF;
         int b= (((uint32_t*)src)[i]&gt;&gt;16)&amp;0xFF;
 
-        dst[i]= ((RY*r + GY*g + BY*b + (33&lt;&lt;(RGB2YUV_SHIFT-1)) )&gt;&gt;RGB2YUV_SHIFT);
+        dst[i]= ((RY*r + GY*g + BY*b + (33&lt;&lt;(RGB2YUV_SHIFT-1)))&gt;&gt;RGB2YUV_SHIFT);
     }
 }
 
@@ -2187,7 +2187,7 @@
         int g= src[i*3+1];
         int b= src[i*3+2];
 
-        dst[i]= ((RY*r + GY*g + BY*b + (33&lt;&lt;(RGB2YUV_SHIFT-1)) )&gt;&gt;RGB2YUV_SHIFT);
+        dst[i]= ((RY*r + GY*g + BY*b + (33&lt;&lt;(RGB2YUV_SHIFT-1)))&gt;&gt;RGB2YUV_SHIFT);
     }
 }
 
@@ -2206,7 +2206,7 @@
     }
 }
 
-static inline void RENAME(rgb16ToY)(uint8_t *dst, uint8_t *src, int width)
+static inline void RENAME(bgr16ToY)(uint8_t *dst, uint8_t *src, int width)
 {
     int i;
     for (i=0; i&lt;width; i++)
@@ -2220,7 +2220,7 @@
     }
 }
 
-static inline void RENAME(rgb16ToUV)(uint8_t *dstU, uint8_t *dstV, uint8_t *src1, uint8_t *src2, int width)
+static inline void RENAME(bgr16ToUV)(uint8_t *dstU, uint8_t *dstV, uint8_t *src1, uint8_t *src2, int width)
 {
     int i;
     assert(src1 == src2);
@@ -2239,7 +2239,7 @@
     }
 }
 
-static inline void RENAME(rgb15ToY)(uint8_t *dst, uint8_t *src, int width)
+static inline void RENAME(bgr15ToY)(uint8_t *dst, uint8_t *src, int width)
 {
     int i;
     for (i=0; i&lt;width; i++)
@@ -2253,7 +2253,7 @@
     }
 }
 
-static inline void RENAME(rgb15ToUV)(uint8_t *dstU, uint8_t *dstV, uint8_t *src1, uint8_t *src2, int width)
+static inline void RENAME(bgr15ToUV)(uint8_t *dstU, uint8_t *dstV, uint8_t *src1, uint8_t *src2, int width)
 {
     int i;
     assert(src1 == src2);
@@ -2476,8 +2476,8 @@
         dst[i] = av_clip(val&gt;&gt;7, 0, (1&lt;&lt;15)-1); // the cubic equation does overflow ...
         //dst[i] = val&gt;&gt;7;
     }
-#endif
-#endif
+#endif /* HAVE_ALTIVEC */
+#endif /* HAVE_MMX */
 }
       // *** horizontal scale Y line to temp buffer
 static inline void RENAME(hyscale)(uint16_t *dst, long dstWidth, uint8_t *src, int srcW, int xInc,
@@ -2594,7 +2594,7 @@
             &quot;add               %%&quot;REG_a&quot;, %%&quot;REG_D&quot; \n\t&quot;\
             &quot;xor               %%&quot;REG_a&quot;, %%&quot;REG_a&quot; \n\t&quot;\
 
-#endif
+#endif /* ARCH_X86_64 */
 
 FUNNY_Y_CODE
 FUNNY_Y_CODE
@@ -2622,7 +2622,7 @@
         }
         else
         {
-#endif
+#endif /* HAVE_MMX2 */
         long xInc_shr16 = xInc &gt;&gt; 16;
         uint16_t xInc_mask = xInc &amp; 0xffff;
         //NO MMX just normal asm ...
@@ -2678,7 +2678,7 @@
             dst[i]= (src[xx]&lt;&lt;7) + (src[xx+1] - src[xx])*xalpha;
             xpos+=xInc;
         }
-#endif
+#endif /* defined(ARCH_X86) */
     }
 }
 
@@ -2690,63 +2690,63 @@
 {
     if (srcFormat==PIX_FMT_YUYV422)
     {
-        RENAME(yuy2ToUV)(formatConvBuffer, formatConvBuffer+2048, src1, src2, srcW);
+        RENAME(yuy2ToUV)(formatConvBuffer, formatConvBuffer+VOFW, src1, src2, srcW);
         src1= formatConvBuffer;
-        src2= formatConvBuffer+2048;
+        src2= formatConvBuffer+VOFW;
     }
     else if (srcFormat==PIX_FMT_UYVY422)
     {
-        RENAME(uyvyToUV)(formatConvBuffer, formatConvBuffer+2048, src1, src2, srcW);
+        RENAME(uyvyToUV)(formatConvBuffer, formatConvBuffer+VOFW, src1, src2, srcW);
         src1= formatConvBuffer;
-        src2= formatConvBuffer+2048;
+        src2= formatConvBuffer+VOFW;
     }
     else if (srcFormat==PIX_FMT_RGB32)
     {
-        RENAME(bgr32ToUV)(formatConvBuffer, formatConvBuffer+2048, src1, src2, srcW);
+        RENAME(bgr32ToUV)(formatConvBuffer, formatConvBuffer+VOFW, src1, src2, srcW);
         src1= formatConvBuffer;
-        src2= formatConvBuffer+2048;
+        src2= formatConvBuffer+VOFW;
     }
     else if (srcFormat==PIX_FMT_BGR24)
     {
-        RENAME(bgr24ToUV)(formatConvBuffer, formatConvBuffer+2048, src1, src2, srcW);
+        RENAME(bgr24ToUV)(formatConvBuffer, formatConvBuffer+VOFW, src1, src2, srcW);
         src1= formatConvBuffer;
-        src2= formatConvBuffer+2048;
+        src2= formatConvBuffer+VOFW;
     }
     else if (srcFormat==PIX_FMT_BGR565)
     {
-        RENAME(bgr16ToUV)(formatConvBuffer, formatConvBuffer+2048, src1, src2, srcW);
+        RENAME(bgr16ToUV)(formatConvBuffer, formatConvBuffer+VOFW, src1, src2, srcW);
         src1= formatConvBuffer;
-        src2= formatConvBuffer+2048;
+        src2= formatConvBuffer+VOFW;
     }
     else if (srcFormat==PIX_FMT_BGR555)
     {
-        RENAME(bgr15ToUV)(formatConvBuffer, formatConvBuffer+2048, src1, src2, srcW);
+        RENAME(bgr15ToUV)(formatConvBuffer, formatConvBuffer+VOFW, src1, src2, srcW);
         src1= formatConvBuffer;
-        src2= formatConvBuffer+2048;
+        src2= formatConvBuffer+VOFW;
     }
     else if (srcFormat==PIX_FMT_BGR32)
     {
-        RENAME(rgb32ToUV)(formatConvBuffer, formatConvBuffer+2048, src1, src2, srcW);
+        RENAME(rgb32ToUV)(formatConvBuffer, formatConvBuffer+VOFW, src1, src2, srcW);
         src1= formatConvBuffer;
-        src2= formatConvBuffer+2048;
+        src2= formatConvBuffer+VOFW;
     }
     else if (srcFormat==PIX_FMT_RGB24)
     {
-        RENAME(rgb24ToUV)(formatConvBuffer, formatConvBuffer+2048, src1, src2, srcW);
+        RENAME(rgb24ToUV)(formatConvBuffer, formatConvBuffer+VOFW, src1, src2, srcW);
         src1= formatConvBuffer;
-        src2= formatConvBuffer+2048;
+        src2= formatConvBuffer+VOFW;
     }
     else if (srcFormat==PIX_FMT_RGB565)
     {
-        RENAME(rgb16ToUV)(formatConvBuffer, formatConvBuffer+2048, src1, src2, srcW);
+        RENAME(rgb16ToUV)(formatConvBuffer, formatConvBuffer+VOFW, src1, src2, srcW);
         src1= formatConvBuffer;
-        src2= formatConvBuffer+2048;
+        src2= formatConvBuffer+VOFW;
     }
     else if (srcFormat==PIX_FMT_RGB555)
     {
-        RENAME(rgb15ToUV)(formatConvBuffer, formatConvBuffer+2048, src1, src2, srcW);
+        RENAME(rgb15ToUV)(formatConvBuffer, formatConvBuffer+VOFW, src1, src2, srcW);
         src1= formatConvBuffer;
-        src2= formatConvBuffer+2048;
+        src2= formatConvBuffer+VOFW;
     }
     else if (isGray(srcFormat))
     {
@@ -2754,9 +2754,9 @@
     }
     else if (srcFormat==PIX_FMT_RGB8 || srcFormat==PIX_FMT_BGR8 || srcFormat==PIX_FMT_PAL8 || srcFormat==PIX_FMT_BGR4_BYTE  || srcFormat==PIX_FMT_RGB4_BYTE)
     {
-        RENAME(palToUV)(formatConvBuffer, formatConvBuffer+2048, src1, src2, srcW, pal);
+        RENAME(palToUV)(formatConvBuffer, formatConvBuffer+VOFW, src1, src2, srcW, pal);
         src1= formatConvBuffer;
-        src2= formatConvBuffer+2048;
+        src2= formatConvBuffer+VOFW;
     }
 
 #ifdef HAVE_MMX
@@ -2767,7 +2767,7 @@
 #endif
     {
         RENAME(hScale)(dst     , dstWidth, src1, srcW, xInc, hChrFilter, hChrFilterPos, hChrFilterSize);
-        RENAME(hScale)(dst+2048, dstWidth, src2, srcW, xInc, hChrFilter, hChrFilterPos, hChrFilterSize);
+        RENAME(hScale)(dst+VOFW, dstWidth, src2, srcW, xInc, hChrFilter, hChrFilterPos, hChrFilterSize);
     }
     else // Fast Bilinear upscale / crap downscale
     {
@@ -2812,7 +2812,7 @@
             &quot;add          %%&quot;REG_a&quot;, %%&quot;REG_D&quot;  \n\t&quot;\
             &quot;xor          %%&quot;REG_a&quot;, %%&quot;REG_a&quot;  \n\t&quot;\
 
-#endif
+#endif /* ARCH_X86_64 */
 
 FUNNY_UV_CODE
 FUNNY_UV_CODE
@@ -2821,7 +2821,7 @@
             &quot;xor          %%&quot;REG_a&quot;, %%&quot;REG_a&quot;  \n\t&quot; // i
             &quot;mov                 %5, %%&quot;REG_c&quot;  \n\t&quot; // src
             &quot;mov                 %1, %%&quot;REG_D&quot;  \n\t&quot; // buf1
-            &quot;add              $4096, %%&quot;REG_D&quot;  \n\t&quot;
+            &quot;add              $&quot;AV_STRINGIFY(VOF)&quot;, %%&quot;REG_D&quot;  \n\t&quot;
             PREFETCH&quot;   (%%&quot;REG_c&quot;)             \n\t&quot;
             PREFETCH&quot; 32(%%&quot;REG_c&quot;)             \n\t&quot;
             PREFETCH&quot; 64(%%&quot;REG_c&quot;)             \n\t&quot;
@@ -2848,12 +2848,12 @@
             {
                 //printf(&quot;%d %d %d\n&quot;, dstWidth, i, srcW);
                 dst[i] = src1[srcW-1]*128;
-                dst[i+2048] = src2[srcW-1]*128;
+                dst[i+VOFW] = src2[srcW-1]*128;
             }
         }
         else
         {
-#endif
+#endif /* HAVE_MMX2 */
             long xInc_shr16 = (long) (xInc &gt;&gt; 16);
             uint16_t xInc_mask = xInc &amp; 0xffff;
             asm volatile(
@@ -2881,7 +2881,7 @@
             &quot;addl    %%edi, %%esi                   \n\t&quot; //src[xx+1]*2*xalpha + src[xx]*(1-2*xalpha)
             &quot;mov        %1, %%&quot;REG_D&quot;               \n\t&quot;
             &quot;shrl       $9, %%esi                   \n\t&quot;
-            &quot;movw     %%si, 4096(%%&quot;REG_D&quot;, %%&quot;REG_a&quot;, 2)   \n\t&quot;
+            &quot;movw     %%si, &quot;AV_STRINGIFY(VOF)&quot;(%%&quot;REG_D&quot;, %%&quot;REG_a&quot;, 2)   \n\t&quot;
 
             &quot;addw       %4, %%cx                    \n\t&quot; //2*xalpha += xInc&amp;0xFF
             &quot;adc        %3, %%&quot;REG_d&quot;               \n\t&quot; //xx+= xInc&gt;&gt;8 + carry
@@ -2891,7 +2891,7 @@
 
 /* GCC-3.3 makes MPlayer crash on IA-32 machines when using &quot;g&quot; operand here,
    which is needed to support GCC-4.0 */
-#if defined(ARCH_X86_64) &amp;&amp; ((__GNUC__ &gt; 3) || ( __GNUC__ == 3 &amp;&amp; __GNUC_MINOR__ &gt;= 4))
+#if defined(ARCH_X86_64) &amp;&amp; ((__GNUC__ &gt; 3) || (__GNUC__ == 3 &amp;&amp; __GNUC_MINOR__ &gt;= 4))
             :: &quot;m&quot; (src1), &quot;m&quot; (dst), &quot;g&quot; ((long)dstWidth), &quot;m&quot; (xInc_shr16), &quot;m&quot; (xInc_mask),
 #else
             :: &quot;m&quot; (src1), &quot;m&quot; (dst), &quot;m&quot; ((long)dstWidth), &quot;m&quot; (xInc_shr16), &quot;m&quot; (xInc_mask),
@@ -2910,14 +2910,14 @@
             register unsigned int xx=xpos&gt;&gt;16;
             register unsigned int xalpha=(xpos&amp;0xFFFF)&gt;&gt;9;
             dst[i]=(src1[xx]*(xalpha^127)+src1[xx+1]*xalpha);
-            dst[i+2048]=(src2[xx]*(xalpha^127)+src2[xx+1]*xalpha);
+            dst[i+VOFW]=(src2[xx]*(xalpha^127)+src2[xx+1]*xalpha);
             /* slower
             dst[i]= (src1[xx]&lt;&lt;7) + (src1[xx+1] - src1[xx])*xalpha;
-            dst[i+2048]=(src2[xx]&lt;&lt;7) + (src2[xx+1] - src2[xx])*xalpha;
+            dst[i+VOFW]=(src2[xx]&lt;&lt;7) + (src2[xx+1] - src2[xx])*xalpha;
             */
             xpos+=xInc;
         }
-#endif
+#endif /* defined(ARCH_X86) */
     }
 }
 
@@ -3002,8 +3002,8 @@
         static int firstTime=1; //FIXME move this into the context perhaps
         if (flags &amp; SWS_PRINT_INFO &amp;&amp; firstTime)
         {
-            av_log(c, AV_LOG_WARNING, &quot;SwScaler: Warning: dstStride is not aligned!\n&quot;
-                   &quot;SwScaler:          -&gt;cannot do aligned memory acesses anymore\n&quot;);
+            av_log(c, AV_LOG_WARNING, &quot;Warning: dstStride is not aligned!\n&quot;
+                   &quot;         -&gt;cannot do aligned memory acesses anymore\n&quot;);
             firstTime=0;
         }
     }
@@ -3077,8 +3077,8 @@
                 lastInChrBuf++;
             }
             //wrap buf index around to stay inside the ring buffer
-            if (lumBufIndex &gt;= vLumBufSize ) lumBufIndex-= vLumBufSize;
-            if (chrBufIndex &gt;= vChrBufSize ) chrBufIndex-= vChrBufSize;
+            if (lumBufIndex &gt;= vLumBufSize) lumBufIndex-= vLumBufSize;
+            if (chrBufIndex &gt;= vChrBufSize) chrBufIndex-= vChrBufSize;
         }
         else // not enough lines left in this slice -&gt; load the rest in the buffer
         {
@@ -3118,16 +3118,16 @@
                 lastInChrBuf++;
             }
             //wrap buf index around to stay inside the ring buffer
-            if (lumBufIndex &gt;= vLumBufSize ) lumBufIndex-= vLumBufSize;
-            if (chrBufIndex &gt;= vChrBufSize ) chrBufIndex-= vChrBufSize;
+            if (lumBufIndex &gt;= vLumBufSize) lumBufIndex-= vLumBufSize;
+            if (chrBufIndex &gt;= vChrBufSize) chrBufIndex-= vChrBufSize;
             break; //we can't output a dstY line so let's try with the next slice
         }
 
 #ifdef HAVE_MMX
-        b5Dither= dither8[dstY&amp;1];
-        g6Dither= dither4[dstY&amp;1];
-        g5Dither= dither8[dstY&amp;1];
-        r5Dither= dither8[(dstY+1)&amp;1];
+        b5Dither= ff_dither8[dstY&amp;1];
+        g6Dither= ff_dither4[dstY&amp;1];
+        g5Dither= ff_dither8[dstY&amp;1];
+        r5Dither= ff_dither8[(dstY+1)&amp;1];
 #endif
         if (dstY &lt; dstH-2)
         {
@@ -3259,8 +3259,8 @@
     }
 
 #ifdef HAVE_MMX
-    __asm __volatile(SFENCE:::&quot;memory&quot;);
-    __asm __volatile(EMMS:::&quot;memory&quot;);
+    asm volatile(SFENCE:::&quot;memory&quot;);
+    asm volatile(EMMS:::&quot;memory&quot;);
 #endif
     /* store changed local vars back in the context */
     c-&gt;dstY= dstY;

Modified: branches/avidemux_2.5_branch_gruntster/avidemux/ADM_libraries/ADM_ffmpeg/ADM_libswscale/yuv2rgb.c
===================================================================
--- branches/avidemux_2.5_branch_gruntster/avidemux/ADM_libraries/ADM_ffmpeg/ADM_libswscale/yuv2rgb.c	2008-03-21 06:48:49 UTC (rev 3911)
+++ branches/avidemux_2.5_branch_gruntster/avidemux/ADM_libraries/ADM_ffmpeg/ADM_libswscale/yuv2rgb.c	2008-03-21 06:51:32 UTC (rev 3912)
@@ -2,7 +2,6 @@
  * yuv2rgb.c, Software YUV to RGB converter
  *
  *  Copyright (C) 1999, Aaron Holtzman &lt;<A HREF="https://lists.berlios.de/mailman/listinfo/avidemux-svn-commit">aholtzma at ess.engr.uvic.ca</A>&gt;
- *  All Rights Reserved.
  *
  *  Functions broken out from display_x11.c and several new modes
  *  added by H&#229;kan Hjort &lt;<A HREF="https://lists.berlios.de/mailman/listinfo/avidemux-svn-commit">d95hjort at dtek.chalmers.se</A>&gt;
@@ -29,9 +28,6 @@
  *  along with mpeg2dec; if not, write to the Free Software
  *  Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA
  */
-// MEANX
-#include &quot;config.h&quot;
-// /MEANX
 
 #include &lt;stdio.h&gt;
 #include &lt;stdlib.h&gt;
@@ -43,15 +39,14 @@
 #include &quot;swscale.h&quot;
 #include &quot;swscale_internal.h&quot;
 
+#ifdef HAVE_VIS
+#include &quot;yuv2rgb_vis.c&quot;
+#endif
+
 #ifdef HAVE_MLIB
 #include &quot;yuv2rgb_mlib.c&quot;
 #endif
-// MEANX
-#include &quot;wrapper.h&quot;
 
-// /MEANX
-
-
 #define DITHER1XBPP // only for mmx
 
 const uint8_t  __attribute__((aligned(8))) dither_2x2_4[2][8]={
@@ -164,14 +159,10 @@
 #ifdef HAVE_MMX
 
 /* hope these constant values are cache line aligned */
-static uint64_t attribute_used __attribute__((aligned(8))) mmx_00ffw   = 0x00ff00ff00ff00ffULL;
-static uint64_t attribute_used __attribute__((aligned(8))) mmx_redmask = 0xf8f8f8f8f8f8f8f8ULL;
-static uint64_t attribute_used __attribute__((aligned(8))) mmx_grnmask = 0xfcfcfcfcfcfcfcfcULL;
+DECLARE_ASM_CONST(8, uint64_t, mmx_00ffw)   = 0x00ff00ff00ff00ffULL;
+DECLARE_ASM_CONST(8, uint64_t, mmx_redmask) = 0xf8f8f8f8f8f8f8f8ULL;
+DECLARE_ASM_CONST(8, uint64_t, mmx_grnmask) = 0xfcfcfcfcfcfcfcfcULL;
 
-static uint64_t attribute_used __attribute__((aligned(8))) M24A=   0x00FF0000FF0000FFULL;
-static uint64_t attribute_used __attribute__((aligned(8))) M24B=   0xFF0000FF0000FF00ULL;
-static uint64_t attribute_used __attribute__((aligned(8))) M24C=   0x0000FF0000FF0000ULL;
-
 // the volatile is required because gcc otherwise optimizes some writes away not knowing that these
 // are read in the asm block
 static volatile uint64_t attribute_used __attribute__((aligned(8))) b5Dither;
@@ -179,14 +170,6 @@
 static volatile uint64_t attribute_used __attribute__((aligned(8))) g6Dither;
 static volatile uint64_t attribute_used __attribute__((aligned(8))) r5Dither;
 
-static uint64_t __attribute__((aligned(8))) dither4[2]={
-    0x0103010301030103LL,
-    0x0200020002000200LL,};
-
-static uint64_t __attribute__((aligned(8))) dither8[2]={
-    0x0602060206020602LL,
-    0x0004000400040004LL,};
-
 #undef HAVE_MMX
 
 //MMX versions
@@ -293,8 +276,7 @@
             dst_2 += dst_delta;\
         }\
         if (c-&gt;dstW &amp; 4) {\
-            int av_unused U, V;\
-            int Y;\
+            int av_unused Y, U, V;\
 
 #define EPILOG2()\
         }\
@@ -639,6 +621,12 @@
         }
     }
 #endif
+#ifdef HAVE_VIS
+    {
+        SwsFunc t= yuv2rgb_init_vis(c);
+        if (t) return t;
+    }
+#endif
 #ifdef HAVE_MLIB
     {
         SwsFunc t= yuv2rgb_init_mlib(c);

Modified: branches/avidemux_2.5_branch_gruntster/avidemux/ADM_libraries/ADM_ffmpeg/ADM_libswscale/yuv2rgb_altivec.c
===================================================================
--- branches/avidemux_2.5_branch_gruntster/avidemux/ADM_libraries/ADM_ffmpeg/ADM_libswscale/yuv2rgb_altivec.c	2008-03-21 06:48:49 UTC (rev 3911)
+++ branches/avidemux_2.5_branch_gruntster/avidemux/ADM_libraries/ADM_ffmpeg/ADM_libswscale/yuv2rgb_altivec.c	2008-03-21 06:51:32 UTC (rev 3912)
@@ -1,968 +1,960 @@
-/*
-  <A HREF="https://lists.berlios.de/mailman/listinfo/avidemux-svn-commit">marc.hoffman at analog.com</A>    March 8, 2004
-
-  Altivec Acceleration for Color Space Conversion revision 0.2
-
-  convert I420 YV12 to RGB in various formats,
-    it rejects images that are not in 420 formats
-    it rejects images that don't have widths of multiples of 16
-    it rejects images that don't have heights of multiples of 2
-  reject defers to C simulation codes.
-
-  lots of optimizations to be done here
-
-  1. need to fix saturation code, I just couldn't get it to fly with packs and adds.
-     so we currently use max min to clip
-
-  2. the inefficient use of chroma loading needs a bit of brushing up
-
-  3. analysis of pipeline stalls needs to be done, use shark to identify pipeline stalls
-
-
-  MODIFIED to calculate coeffs from currently selected color space.
-  MODIFIED core to be a macro which you spec the output format.
-  ADDED UYVY conversion which is never called due to some thing in SWSCALE.
-  CORRECTED algorithim selection to be strict on input formats.
-  ADDED runtime detection of altivec.
-
-  ADDED altivec_yuv2packedX vertical scl + RGB converter
-
-  March 27,2004
-  PERFORMANCE ANALYSIS
-
-  The C version use 25% of the processor or ~250Mips for D1 video rawvideo used as test
-  The ALTIVEC version uses 10% of the processor or ~100Mips for D1 video same sequence
-
-  720*480*30  ~10MPS
-
-  so we have roughly 10clocks per pixel this is too high something has to be wrong.
-
-  OPTIMIZED clip codes to utilize vec_max and vec_packs removing the need for vec_min.
-
-  OPTIMIZED DST OUTPUT cache/dma controls. we are pretty much
-  guaranteed to have the input video frame it was just decompressed so
-  it probably resides in L1 caches.  However we are creating the
-  output video stream this needs to use the DSTST instruction to
-  optimize for the cache.  We couple this with the fact that we are
-  not going to be visiting the input buffer again so we mark it Least
-  Recently Used.  This shaves 25% of the processor cycles off.
-
-  Now MEMCPY is the largest mips consumer in the system, probably due
-  to the inefficient X11 stuff.
-
-  GL libraries seem to be very slow on this machine 1.33Ghz PB running
-  Jaguar, this is not the case for my 1Ghz PB.  I thought it might be
-  a versioning issues, however i have libGL.1.2.dylib for both
-  machines. ((We need to figure this out now))
-
-  GL2 libraries work now with patch for RGB32
-
-  NOTE quartz vo driver ARGB32_to_RGB24 consumes 30% of the processor
-
-  Integrated luma prescaling adjustment for saturation/contrast/brightness adjustment.
-*/
-
-/*
- * This file is part of FFmpeg.
- *
- * FFmpeg is free software; you can redistribute it and/or modify
- * it under the terms of the GNU General Public License as published by
- * the Free Software Foundation; either version 2 of the License, or
- * (at your option) any later version.
- *
- * FFmpeg is distributed in the hope that it will be useful,
- * but WITHOUT ANY WARRANTY; without even the implied warranty of
- * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
- * GNU General Public License for more details.
- *
- * You should have received a copy of the GNU General Public License
- * along with FFmpeg; if not, write to the Free Software
- * Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA
- */
-#include &quot;config.h&quot;
-
-#include &lt;stdio.h&gt;
-#include &lt;stdlib.h&gt;
-#include &lt;string.h&gt;
-#include &lt;inttypes.h&gt;
-#include &lt;assert.h&gt;
-#include &quot;config.h&quot;
-#ifdef HAVE_MALLOC_H
-#include &lt;malloc.h&gt;
-#endif
-#include &quot;rgb2rgb.h&quot;
-#include &quot;swscale.h&quot;
-#include &quot;swscale_internal.h&quot;
-
-#undef PROFILE_THE_BEAST
-#undef INC_SCALING
-
-typedef unsigned char ubyte;
-typedef signed char   sbyte;
-
-
-/* RGB interleaver, 16 planar pels 8-bit samples per channel in
-   homogeneous vector registers x0,x1,x2 are interleaved with the
-   following technique:
-
-      o0 = vec_mergeh (x0,x1);
-      o1 = vec_perm (o0, x2, perm_rgb_0);
-      o2 = vec_perm (o0, x2, perm_rgb_1);
-      o3 = vec_mergel (x0,x1);
-      o4 = vec_perm (o3,o2,perm_rgb_2);
-      o5 = vec_perm (o3,o2,perm_rgb_3);
-
-  perm_rgb_0:   o0(RG).h v1(B) --&gt; o1*
-              0   1  2   3   4
-             rgbr|gbrg|brgb|rgbr
-             0010 0100 1001 0010
-             0102 3145 2673 894A
-
-  perm_rgb_1:   o0(RG).h v1(B) --&gt; o2
-              0   1  2   3   4
-             gbrg|brgb|bbbb|bbbb
-             0100 1001 1111 1111
-             B5CD 6EF7 89AB CDEF
-
-  perm_rgb_2:   o3(RG).l o2(rgbB.l) --&gt; o4*
-              0   1  2   3   4
-             gbrg|brgb|rgbr|gbrg
-             1111 1111 0010 0100
-             89AB CDEF 0182 3945
-
-  perm_rgb_2:   o3(RG).l o2(rgbB.l) ---&gt; o5*
-              0   1  2   3   4
-             brgb|rgbr|gbrg|brgb
-             1001 0010 0100 1001
-             a67b 89cA BdCD eEFf
-
-*/
-static
-const vector unsigned char
-  perm_rgb_0 = (const vector unsigned char)AVV(0x00,0x01,0x10,0x02,0x03,0x11,0x04,0x05,
-                                               0x12,0x06,0x07,0x13,0x08,0x09,0x14,0x0a),
-  perm_rgb_1 = (const vector unsigned char)AVV(0x0b,0x15,0x0c,0x0d,0x16,0x0e,0x0f,0x17,
-                                               0x18,0x19,0x1a,0x1b,0x1c,0x1d,0x1e,0x1f),
-  perm_rgb_2 = (const vector unsigned char)AVV(0x10,0x11,0x12,0x13,0x14,0x15,0x16,0x17,
-                                               0x00,0x01,0x18,0x02,0x03,0x19,0x04,0x05),
-  perm_rgb_3 = (const vector unsigned char)AVV(0x1a,0x06,0x07,0x1b,0x08,0x09,0x1c,0x0a,
-                                               0x0b,0x1d,0x0c,0x0d,0x1e,0x0e,0x0f,0x1f);
-
-#define vec_merge3(x2,x1,x0,y0,y1,y2)       \
-do {                                        \
-    typeof(x0) o0,o2,o3;                    \
-        o0 = vec_mergeh (x0,x1);            \
-        y0 = vec_perm (o0, x2, perm_rgb_0); \
-        o2 = vec_perm (o0, x2, perm_rgb_1); \
-        o3 = vec_mergel (x0,x1);            \
-        y1 = vec_perm (o3,o2,perm_rgb_2);   \
-        y2 = vec_perm (o3,o2,perm_rgb_3);   \
-} while(0)
-
-#define vec_mstbgr24(x0,x1,x2,ptr)      \
-do {                                    \
-    typeof(x0) _0,_1,_2;                \
-    vec_merge3 (x0,x1,x2,_0,_1,_2);     \
-    vec_st (_0, 0, ptr++);              \
-    vec_st (_1, 0, ptr++);              \
-    vec_st (_2, 0, ptr++);              \
-}  while (0);
-
-#define vec_mstrgb24(x0,x1,x2,ptr)      \
-do {                                    \
-    typeof(x0) _0,_1,_2;                \
-    vec_merge3 (x2,x1,x0,_0,_1,_2);     \
-    vec_st (_0, 0, ptr++);              \
-    vec_st (_1, 0, ptr++);              \
-    vec_st (_2, 0, ptr++);              \
-}  while (0);
-
-/* pack the pixels in rgb0 format
-   msb R
-   lsb 0
-*/
-#define vec_mstrgb32(T,x0,x1,x2,x3,ptr)                                       \
-do {                                                                          \
-    T _0,_1,_2,_3;                                                            \
-    _0 = vec_mergeh (x0,x1);                                                  \
-    _1 = vec_mergeh (x2,x3);                                                  \
-    _2 = (T)vec_mergeh ((vector unsigned short)_0,(vector unsigned short)_1); \
-    _3 = (T)vec_mergel ((vector unsigned short)_0,(vector unsigned short)_1); \
-    vec_st (_2, 0*16, (T *)ptr);                                              \
-    vec_st (_3, 1*16, (T *)ptr);                                              \
-    _0 = vec_mergel (x0,x1);                                                  \
-    _1 = vec_mergel (x2,x3);                                                  \
-    _2 = (T)vec_mergeh ((vector unsigned short)_0,(vector unsigned short)_1); \
-    _3 = (T)vec_mergel ((vector unsigned short)_0,(vector unsigned short)_1); \
-    vec_st (_2, 2*16, (T *)ptr);                                              \
-    vec_st (_3, 3*16, (T *)ptr);                                              \
-    ptr += 4;                                                                 \
-}  while (0);
-
-/*
-
-  | 1     0       1.4021   | | Y |
-  | 1    -0.3441 -0.7142   |x| Cb|
-  | 1     1.7718  0        | | Cr|
-
-
-  Y:      [-128 127]
-  Cb/Cr : [-128 127]
-
-  typical yuv conversion work on Y: 0-255 this version has been optimized for jpeg decode.
-
-*/
-
-
-
-
-#define vec_unh(x) \
-    (vector signed short) \
-        vec_perm(x,(typeof(x))AVV(0),\
-                 (vector unsigned char)AVV(0x10,0x00,0x10,0x01,0x10,0x02,0x10,0x03,\
-                                           0x10,0x04,0x10,0x05,0x10,0x06,0x10,0x07))
-#define vec_unl(x) \
-    (vector signed short) \
-        vec_perm(x,(typeof(x))AVV(0),\
-                 (vector unsigned char)AVV(0x10,0x08,0x10,0x09,0x10,0x0A,0x10,0x0B,\
-                                           0x10,0x0C,0x10,0x0D,0x10,0x0E,0x10,0x0F))
-
-#define vec_clip_s16(x) \
-    vec_max (vec_min (x, (vector signed short)AVV(235,235,235,235,235,235,235,235)),\
-                         (vector signed short)AVV( 16, 16, 16, 16, 16, 16, 16, 16))
-
-#define vec_packclp(x,y) \
-    (vector unsigned char)vec_packs \
-        ((vector unsigned short)vec_max (x,(vector signed short) AVV(0)), \
-         (vector unsigned short)vec_max (y,(vector signed short) AVV(0)))
-
-//#define out_pixels(a,b,c,ptr) vec_mstrgb32(typeof(a),((typeof (a))AVV(0)),a,a,a,ptr)
-
-
-static inline void cvtyuvtoRGB (SwsContext *c,
-                                vector signed short Y, vector signed short U, vector signed short V,
-                                vector signed short *R, vector signed short *G, vector signed short *B)
-{
-    vector signed   short vx,ux,uvx;
-
-    Y = vec_mradds (Y, c-&gt;CY, c-&gt;OY);
-    U  = vec_sub (U,(vector signed short)
-                    vec_splat((vector signed short)AVV(128),0));
-    V  = vec_sub (V,(vector signed short)
-                    vec_splat((vector signed short)AVV(128),0));
-
-    //   ux  = (CBU*(u&lt;&lt;c-&gt;CSHIFT)+0x4000)&gt;&gt;15;
-    ux = vec_sl (U, c-&gt;CSHIFT);
-    *B = vec_mradds (ux, c-&gt;CBU, Y);
-
-    // vx  = (CRV*(v&lt;&lt;c-&gt;CSHIFT)+0x4000)&gt;&gt;15;
-    vx = vec_sl (V, c-&gt;CSHIFT);
-    *R = vec_mradds (vx, c-&gt;CRV, Y);
-
-    // uvx = ((CGU*u) + (CGV*v))&gt;&gt;15;
-    uvx = vec_mradds (U, c-&gt;CGU, Y);
-    *G  = vec_mradds (V, c-&gt;CGV, uvx);
-}
-
-
-/*
-  ------------------------------------------------------------------------------
-  CS converters
-  ------------------------------------------------------------------------------
-*/
-
-
-#define DEFCSP420_CVT(name,out_pixels)                                  \
-static int altivec_##name (SwsContext *c,                               \
-                           unsigned char **in, int *instrides,          \
-                           int srcSliceY,        int srcSliceH,         \
-                           unsigned char **oplanes, int *outstrides)    \
-{                                                                       \
-    int w = c-&gt;srcW;                                                    \
-    int h = srcSliceH;                                                  \
-    int i,j;                                                            \
-    int instrides_scl[3];                                               \
-    vector unsigned char y0,y1;                                         \
-                                                                        \
-    vector signed char  u,v;                                            \
-                                                                        \
-    vector signed short Y0,Y1,Y2,Y3;                                    \
-    vector signed short U,V;                                            \
-    vector signed short vx,ux,uvx;                                      \
-    vector signed short vx0,ux0,uvx0;                                   \
-    vector signed short vx1,ux1,uvx1;                                   \
-    vector signed short R0,G0,B0;                                       \
-    vector signed short R1,G1,B1;                                       \
-    vector unsigned char R,G,B;                                         \
-                                                                        \
-    vector unsigned char *y1ivP, *y2ivP, *uivP, *vivP;                  \
-    vector unsigned char align_perm;                                    \
-                                                                        \
-    vector signed short                                                 \
-        lCY  = c-&gt;CY,                                                   \
-        lOY  = c-&gt;OY,                                                   \
-        lCRV = c-&gt;CRV,                                                  \
-        lCBU = c-&gt;CBU,                                                  \
-        lCGU = c-&gt;CGU,                                                  \
-        lCGV = c-&gt;CGV;                                                  \
-                                                                        \
-    vector unsigned short lCSHIFT = c-&gt;CSHIFT;                          \
-                                                                        \
-    ubyte *y1i   = in[0];                                               \
-    ubyte *y2i   = in[0]+instrides[0];                                  \
-    ubyte *ui    = in[1];                                               \
-    ubyte *vi    = in[2];                                               \
-                                                                        \
-    vector unsigned char *oute                                          \
-        = (vector unsigned char *)                                      \
-            (oplanes[0]+srcSliceY*outstrides[0]);                       \
-    vector unsigned char *outo                                          \
-        = (vector unsigned char *)                                      \
-            (oplanes[0]+srcSliceY*outstrides[0]+outstrides[0]);         \
-                                                                        \
-                                                                        \
-    instrides_scl[0] = instrides[0]*2-w;  /* the loop moves y{1,2}i by w */ \
-    instrides_scl[1] = instrides[1]-w/2;  /* the loop moves ui by w/2 */    \
-    instrides_scl[2] = instrides[2]-w/2;  /* the loop moves vi by w/2 */    \
-                                                                        \
-                                                                        \
-    for (i=0;i&lt;h/2;i++) {                                               \
-        vec_dstst (outo, (0x02000002|(((w*3+32)/32)&lt;&lt;16)), 0);          \
-        vec_dstst (oute, (0x02000002|(((w*3+32)/32)&lt;&lt;16)), 1);          \
-                                                                        \
-        for (j=0;j&lt;w/16;j++) {                                          \
-                                                                        \
-            y1ivP = (vector unsigned char *)y1i;                        \
-            y2ivP = (vector unsigned char *)y2i;                        \
-            uivP  = (vector unsigned char *)ui;                         \
-            vivP  = (vector unsigned char *)vi;                         \
-                                                                        \
-            align_perm = vec_lvsl (0, y1i);                             \
-            y0 = (vector unsigned char)                                 \
-                 vec_perm (y1ivP[0], y1ivP[1], align_perm);             \
-                                                                        \
-            align_perm = vec_lvsl (0, y2i);                             \
-            y1 = (vector unsigned char)                                 \
-                 vec_perm (y2ivP[0], y2ivP[1], align_perm);             \
-                                                                        \
-            align_perm = vec_lvsl (0, ui);                              \
-            u = (vector signed char)                                    \
-                vec_perm (uivP[0], uivP[1], align_perm);                \
-                                                                        \
-            align_perm = vec_lvsl (0, vi);                              \
-            v = (vector signed char)                                    \
-                vec_perm (vivP[0], vivP[1], align_perm);                \
-                                                                        \
-            u  = (vector signed char)                                   \
-                 vec_sub (u,(vector signed char)                        \
-                          vec_splat((vector signed char)AVV(128),0));   \
-            v  = (vector signed char)                                   \
-                 vec_sub (v,(vector signed char)                        \
-                          vec_splat((vector signed char)AVV(128),0));   \
-                                                                        \
-            U  = vec_unpackh (u);                                       \
-            V  = vec_unpackh (v);                                       \
-                                                                        \
-                                                                        \
-            Y0 = vec_unh (y0);                                          \
-            Y1 = vec_unl (y0);                                          \
-            Y2 = vec_unh (y1);                                          \
-            Y3 = vec_unl (y1);                                          \
-                                                                        \
-            Y0 = vec_mradds (Y0, lCY, lOY);                             \
-            Y1 = vec_mradds (Y1, lCY, lOY);                             \
-            Y2 = vec_mradds (Y2, lCY, lOY);                             \
-            Y3 = vec_mradds (Y3, lCY, lOY);                             \
-                                                                        \
-            /*   ux  = (CBU*(u&lt;&lt;CSHIFT)+0x4000)&gt;&gt;15 */                  \
-            ux = vec_sl (U, lCSHIFT);                                   \
-            ux = vec_mradds (ux, lCBU, (vector signed short)AVV(0));    \
-            ux0  = vec_mergeh (ux,ux);                                  \
-            ux1  = vec_mergel (ux,ux);                                  \
-                                                                        \
-            /* vx  = (CRV*(v&lt;&lt;CSHIFT)+0x4000)&gt;&gt;15;        */            \
-            vx = vec_sl (V, lCSHIFT);                                   \
-            vx = vec_mradds (vx, lCRV, (vector signed short)AVV(0));    \
-            vx0  = vec_mergeh (vx,vx);                                  \
-            vx1  = vec_mergel (vx,vx);                                  \
-                                                                        \
-            /* uvx = ((CGU*u) + (CGV*v))&gt;&gt;15 */                         \
-            uvx = vec_mradds (U, lCGU, (vector signed short)AVV(0));    \
-            uvx = vec_mradds (V, lCGV, uvx);                            \
-            uvx0 = vec_mergeh (uvx,uvx);                                \
-            uvx1 = vec_mergel (uvx,uvx);                                \
-                                                                        \
-            R0 = vec_add (Y0,vx0);                                      \
-            G0 = vec_add (Y0,uvx0);                                     \
-            B0 = vec_add (Y0,ux0);                                      \
-            R1 = vec_add (Y1,vx1);                                      \
-            G1 = vec_add (Y1,uvx1);                                     \
-            B1 = vec_add (Y1,ux1);                                      \
-                                                                        \
-            R  = vec_packclp (R0,R1);                                   \
-            G  = vec_packclp (G0,G1);                                   \
-            B  = vec_packclp (B0,B1);                                   \
-                                                                        \
-            out_pixels(R,G,B,oute);                                     \
-                                                                        \
-            R0 = vec_add (Y2,vx0);                                      \
-            G0 = vec_add (Y2,uvx0);                                     \
-            B0 = vec_add (Y2,ux0);                                      \
-            R1 = vec_add (Y3,vx1);                                      \
-            G1 = vec_add (Y3,uvx1);                                     \
-            B1 = vec_add (Y3,ux1);                                      \
-            R  = vec_packclp (R0,R1);                                   \
-            G  = vec_packclp (G0,G1);                                   \
-            B  = vec_packclp (B0,B1);                                   \
-                                                                        \
-                                                                        \
-            out_pixels(R,G,B,outo);                                     \
-                                                                        \
-            y1i  += 16;                                                 \
-            y2i  += 16;                                                 \
-            ui   += 8;                                                  \
-            vi   += 8;                                                  \
-                                                                        \
-        }                                                               \
-                                                                        \
-        outo  += (outstrides[0])&gt;&gt;4;                                    \
-        oute  += (outstrides[0])&gt;&gt;4;                                    \
-                                                                        \
-        ui    += instrides_scl[1];                                      \
-        vi    += instrides_scl[2];                                      \
-        y1i   += instrides_scl[0];                                      \
-        y2i   += instrides_scl[0];                                      \
-    }                                                                   \
-    return srcSliceH;                                                   \
-}
-
-
-#define out_abgr(a,b,c,ptr)  vec_mstrgb32(typeof(a),((typeof (a))AVV(0)),c,b,a,ptr)
-#define out_bgra(a,b,c,ptr)  vec_mstrgb32(typeof(a),c,b,a,((typeof (a))AVV(0)),ptr)
-#define out_rgba(a,b,c,ptr)  vec_mstrgb32(typeof(a),a,b,c,((typeof (a))AVV(0)),ptr)
-#define out_argb(a,b,c,ptr)  vec_mstrgb32(typeof(a),((typeof (a))AVV(0)),a,b,c,ptr)
-#define out_rgb24(a,b,c,ptr) vec_mstrgb24(a,b,c,ptr)
-#define out_bgr24(a,b,c,ptr) vec_mstbgr24(a,b,c,ptr)
-
-DEFCSP420_CVT (yuv2_abgr, out_abgr)
-#if 1
-DEFCSP420_CVT (yuv2_bgra, out_bgra)
-#else
-static int altivec_yuv2_bgra32 (SwsContext *c,
-                                unsigned char **in, int *instrides,
-                                int srcSliceY,        int srcSliceH,
-                                unsigned char **oplanes, int *outstrides)
-{
-    int w = c-&gt;srcW;
-    int h = srcSliceH;
-    int i,j;
-    int instrides_scl[3];
-    vector unsigned char y0,y1;
-
-    vector signed char  u,v;
-
-    vector signed short Y0,Y1,Y2,Y3;
-    vector signed short U,V;
-    vector signed short vx,ux,uvx;
-    vector signed short vx0,ux0,uvx0;
-    vector signed short vx1,ux1,uvx1;
-    vector signed short R0,G0,B0;
-    vector signed short R1,G1,B1;
-    vector unsigned char R,G,B;
-
-    vector unsigned char *uivP, *vivP;
-    vector unsigned char align_perm;
-
-    vector signed short
-        lCY  = c-&gt;CY,
-        lOY  = c-&gt;OY,
-        lCRV = c-&gt;CRV,
-        lCBU = c-&gt;CBU,
-        lCGU = c-&gt;CGU,
-        lCGV = c-&gt;CGV;
-
-    vector unsigned short lCSHIFT = c-&gt;CSHIFT;
-
-    ubyte *y1i   = in[0];
-    ubyte *y2i   = in[0]+w;
-    ubyte *ui    = in[1];
-    ubyte *vi    = in[2];
-
-    vector unsigned char *oute
-        = (vector unsigned char *)
-          (oplanes[0]+srcSliceY*outstrides[0]);
-    vector unsigned char *outo
-        = (vector unsigned char *)
-          (oplanes[0]+srcSliceY*outstrides[0]+outstrides[0]);
-
-
-    instrides_scl[0] = instrides[0];
-    instrides_scl[1] = instrides[1]-w/2;  /* the loop moves ui by w/2 */
-    instrides_scl[2] = instrides[2]-w/2;  /* the loop moves vi by w/2 */
-
-
-    for (i=0;i&lt;h/2;i++) {
-        vec_dstst (outo, (0x02000002|(((w*3+32)/32)&lt;&lt;16)), 0);
-        vec_dstst (oute, (0x02000002|(((w*3+32)/32)&lt;&lt;16)), 1);
-
-        for (j=0;j&lt;w/16;j++) {
-
-            y0 = vec_ldl (0,y1i);
-            y1 = vec_ldl (0,y2i);
-            uivP = (vector unsigned char *)ui;
-            vivP = (vector unsigned char *)vi;
-
-            align_perm = vec_lvsl (0, ui);
-            u  = (vector signed char)vec_perm (uivP[0], uivP[1], align_perm);
-
-            align_perm = vec_lvsl (0, vi);
-            v  = (vector signed char)vec_perm (vivP[0], vivP[1], align_perm);
-            u  = (vector signed char)
-                 vec_sub (u,(vector signed char)
-                          vec_splat((vector signed char)AVV(128),0));
-
-            v  = (vector signed char)
-                 vec_sub (v, (vector signed char)
-                          vec_splat((vector signed char)AVV(128),0));
-
-            U  = vec_unpackh (u);
-            V  = vec_unpackh (v);
-
-
-            Y0 = vec_unh (y0);
-            Y1 = vec_unl (y0);
-            Y2 = vec_unh (y1);
-            Y3 = vec_unl (y1);
-
-            Y0 = vec_mradds (Y0, lCY, lOY);
-            Y1 = vec_mradds (Y1, lCY, lOY);
-            Y2 = vec_mradds (Y2, lCY, lOY);
-            Y3 = vec_mradds (Y3, lCY, lOY);
-
-            /*   ux  = (CBU*(u&lt;&lt;CSHIFT)+0x4000)&gt;&gt;15 */
-            ux = vec_sl (U, lCSHIFT);
-            ux = vec_mradds (ux, lCBU, (vector signed short)AVV(0));
-            ux0  = vec_mergeh (ux,ux);
-            ux1  = vec_mergel (ux,ux);
-
-            /* vx  = (CRV*(v&lt;&lt;CSHIFT)+0x4000)&gt;&gt;15;        */
-            vx = vec_sl (V, lCSHIFT);
-            vx = vec_mradds (vx, lCRV, (vector signed short)AVV(0));
-            vx0  = vec_mergeh (vx,vx);
-            vx1  = vec_mergel (vx,vx);
-            /* uvx = ((CGU*u) + (CGV*v))&gt;&gt;15 */
-            uvx = vec_mradds (U, lCGU, (vector signed short)AVV(0));
-            uvx = vec_mradds (V, lCGV, uvx);
-            uvx0 = vec_mergeh (uvx,uvx);
-            uvx1 = vec_mergel (uvx,uvx);
-            R0 = vec_add (Y0,vx0);
-            G0 = vec_add (Y0,uvx0);
-            B0 = vec_add (Y0,ux0);
-            R1 = vec_add (Y1,vx1);
-            G1 = vec_add (Y1,uvx1);
-            B1 = vec_add (Y1,ux1);
-            R  = vec_packclp (R0,R1);
-            G  = vec_packclp (G0,G1);
-            B  = vec_packclp (B0,B1);
-
-            out_argb(R,G,B,oute);
-            R0 = vec_add (Y2,vx0);
-            G0 = vec_add (Y2,uvx0);
-            B0 = vec_add (Y2,ux0);
-            R1 = vec_add (Y3,vx1);
-            G1 = vec_add (Y3,uvx1);
-            B1 = vec_add (Y3,ux1);
-            R  = vec_packclp (R0,R1);
-            G  = vec_packclp (G0,G1);
-            B  = vec_packclp (B0,B1);
-
-            out_argb(R,G,B,outo);
-            y1i  += 16;
-            y2i  += 16;
-            ui   += 8;
-            vi   += 8;
-
-        }
-
-        outo  += (outstrides[0])&gt;&gt;4;
-        oute  += (outstrides[0])&gt;&gt;4;
-
-        ui    += instrides_scl[1];
-        vi    += instrides_scl[2];
-        y1i   += instrides_scl[0];
-        y2i   += instrides_scl[0];
-    }
-    return srcSliceH;
-}
-
-#endif
-
-
-DEFCSP420_CVT (yuv2_rgba, out_rgba)
-DEFCSP420_CVT (yuv2_argb, out_argb)
-DEFCSP420_CVT (yuv2_rgb24,  out_rgb24)
-DEFCSP420_CVT (yuv2_bgr24,  out_bgr24)
-
-
-// uyvy|uyvy|uyvy|uyvy
-// 0123 4567 89ab cdef
-static
-const vector unsigned char
-    demux_u = (const vector unsigned char)AVV(0x10,0x00,0x10,0x00,
-                                              0x10,0x04,0x10,0x04,
-                                              0x10,0x08,0x10,0x08,
-                                              0x10,0x0c,0x10,0x0c),
-    demux_v = (const vector unsigned char)AVV(0x10,0x02,0x10,0x02,
-                                              0x10,0x06,0x10,0x06,
-                                              0x10,0x0A,0x10,0x0A,
-                                              0x10,0x0E,0x10,0x0E),
-    demux_y = (const vector unsigned char)AVV(0x10,0x01,0x10,0x03,
-                                              0x10,0x05,0x10,0x07,
-                                              0x10,0x09,0x10,0x0B,
-                                              0x10,0x0D,0x10,0x0F);
-
-/*
-  this is so I can play live CCIR raw video
-*/
-static int altivec_uyvy_rgb32 (SwsContext *c,
-                               unsigned char **in, int *instrides,
-                               int srcSliceY,        int srcSliceH,
-                               unsigned char **oplanes, int *outstrides)
-{
-    int w = c-&gt;srcW;
-    int h = srcSliceH;
-    int i,j;
-    vector unsigned char uyvy;
-    vector signed   short Y,U,V;
-    vector signed   short R0,G0,B0,R1,G1,B1;
-    vector unsigned char  R,G,B;
-    vector unsigned char *out;
-    ubyte *img;
-
-    img = in[0];
-    out = (vector unsigned char *)(oplanes[0]+srcSliceY*outstrides[0]);
-
-    for (i=0;i&lt;h;i++) {
-        for (j=0;j&lt;w/16;j++) {
-            uyvy = vec_ld (0, img);
-            U = (vector signed short)
-                vec_perm (uyvy, (vector unsigned char)AVV(0), demux_u);
-
-            V = (vector signed short)
-                vec_perm (uyvy, (vector unsigned char)AVV(0), demux_v);
-
-            Y = (vector signed short)
-                vec_perm (uyvy, (vector unsigned char)AVV(0), demux_y);
-
-            cvtyuvtoRGB (c, Y,U,V,&amp;R0,&amp;G0,&amp;B0);
-
-            uyvy = vec_ld (16, img);
-            U = (vector signed short)
-                vec_perm (uyvy, (vector unsigned char)AVV(0), demux_u);
-
-            V = (vector signed short)
-                vec_perm (uyvy, (vector unsigned char)AVV(0), demux_v);
-
-            Y = (vector signed short)
-                vec_perm (uyvy, (vector unsigned char)AVV(0), demux_y);
-
-            cvtyuvtoRGB (c, Y,U,V,&amp;R1,&amp;G1,&amp;B1);
-
-            R  = vec_packclp (R0,R1);
-            G  = vec_packclp (G0,G1);
-            B  = vec_packclp (B0,B1);
-
-            //      vec_mstbgr24 (R,G,B, out);
-            out_rgba (R,G,B,out);
-
-            img += 32;
-        }
-    }
-    return srcSliceH;
-}
-
-
-
-/* Ok currently the acceleration routine only supports
-   inputs of widths a multiple of 16
-   and heights a multiple 2
-
-   So we just fall back to the C codes for this.
-*/
-SwsFunc yuv2rgb_init_altivec (SwsContext *c)
-{
-    if (!(c-&gt;flags &amp; SWS_CPU_CAPS_ALTIVEC))
-        return NULL;
-
-    /*
-      and this seems not to matter too much I tried a bunch of
-      videos with abnormal widths and mplayer crashes else where.
-      mplayer -vo x11 -rawvideo on:w=350:h=240 raw-350x240.eyuv
-      boom with X11 bad match.
-
-    */
-    if ((c-&gt;srcW &amp; 0xf) != 0)    return NULL;
-
-    switch (c-&gt;srcFormat) {
-    case PIX_FMT_YUV410P:
-    case PIX_FMT_YUV420P:
-    /*case IMGFMT_CLPL:        ??? */
-    case PIX_FMT_GRAY8:
-    case PIX_FMT_NV12:
-    case PIX_FMT_NV21:
-        if ((c-&gt;srcH &amp; 0x1) != 0)
-            return NULL;
-
-        switch(c-&gt;dstFormat){
-        case PIX_FMT_RGB24:
-            av_log(c, AV_LOG_WARNING, &quot;ALTIVEC: Color Space RGB24\n&quot;);
-            return altivec_yuv2_rgb24;
-        case PIX_FMT_BGR24:
-            av_log(c, AV_LOG_WARNING, &quot;ALTIVEC: Color Space BGR24\n&quot;);
-            return altivec_yuv2_bgr24;
-        case PIX_FMT_ARGB:
-            av_log(c, AV_LOG_WARNING, &quot;ALTIVEC: Color Space ARGB\n&quot;);
-            return altivec_yuv2_argb;
-        case PIX_FMT_ABGR:
-            av_log(c, AV_LOG_WARNING, &quot;ALTIVEC: Color Space ABGR\n&quot;);
-            return altivec_yuv2_abgr;
-        case PIX_FMT_RGBA:
-            av_log(c, AV_LOG_WARNING, &quot;ALTIVEC: Color Space RGBA\n&quot;);
-            return altivec_yuv2_rgba;
-        case PIX_FMT_BGRA:
-            av_log(c, AV_LOG_WARNING, &quot;ALTIVEC: Color Space BGRA\n&quot;);
-            return altivec_yuv2_bgra;
-        default: return NULL;
-        }
-        break;
-
-    case PIX_FMT_UYVY422:
-        switch(c-&gt;dstFormat){
-        case PIX_FMT_BGR32:
-            av_log(c, AV_LOG_WARNING, &quot;ALTIVEC: Color Space UYVY -&gt; RGB32\n&quot;);
-            return altivec_uyvy_rgb32;
-        default: return NULL;
-        }
-        break;
-
-    }
-    return NULL;
-}
-
-static uint16_t roundToInt16(int64_t f){
-    int r= (f + (1&lt;&lt;15))&gt;&gt;16;
-         if (r&lt;-0x7FFF) return 0x8000;
-    else if (r&gt; 0x7FFF) return 0x7FFF;
-    else                return r;
-}
-
-void yuv2rgb_altivec_init_tables (SwsContext *c, const int inv_table[4],int brightness,int contrast, int saturation)
-{
-    union {
-        signed short tmp[8] __attribute__ ((aligned(16)));
-        vector signed short vec;
-    } buf;
-
-    buf.tmp[0] =  ( (0xffffLL) * contrast&gt;&gt;8 )&gt;&gt;9;                      //cy
-    buf.tmp[1] =  -256*brightness;                                      //oy
-    buf.tmp[2] =  (inv_table[0]&gt;&gt;3) *(contrast&gt;&gt;16)*(saturation&gt;&gt;16);   //crv
-    buf.tmp[3] =  (inv_table[1]&gt;&gt;3) *(contrast&gt;&gt;16)*(saturation&gt;&gt;16);   //cbu
-    buf.tmp[4] = -((inv_table[2]&gt;&gt;1)*(contrast&gt;&gt;16)*(saturation&gt;&gt;16));  //cgu
-    buf.tmp[5] = -((inv_table[3]&gt;&gt;1)*(contrast&gt;&gt;16)*(saturation&gt;&gt;16));  //cgv
-
-
-    c-&gt;CSHIFT = (vector unsigned short)vec_splat_u16(2);
-    c-&gt;CY   = vec_splat ((vector signed short)buf.vec, 0);
-    c-&gt;OY   = vec_splat ((vector signed short)buf.vec, 1);
-    c-&gt;CRV  = vec_splat ((vector signed short)buf.vec, 2);
-    c-&gt;CBU  = vec_splat ((vector signed short)buf.vec, 3);
-    c-&gt;CGU  = vec_splat ((vector signed short)buf.vec, 4);
-    c-&gt;CGV  = vec_splat ((vector signed short)buf.vec, 5);
-#if 0
-    {
-    int i;
-    char *v[6]={&quot;cy&quot;,&quot;oy&quot;,&quot;crv&quot;,&quot;cbu&quot;,&quot;cgu&quot;,&quot;cgv&quot;};
-    for (i=0; i&lt;6; i++)
-        printf(&quot;%s %d &quot;, v[i],buf.tmp[i] );
-        printf(&quot;\n&quot;);
-    }
-#endif
-    return;
-}
-
-
-void
-altivec_yuv2packedX (SwsContext *c,
-                     int16_t *lumFilter, int16_t **lumSrc, int lumFilterSize,
-                     int16_t *chrFilter, int16_t **chrSrc, int chrFilterSize,
-                     uint8_t *dest, int dstW, int dstY)
-{
-    int i,j;
-    vector signed short X,X0,X1,Y0,U0,V0,Y1,U1,V1,U,V;
-    vector signed short R0,G0,B0,R1,G1,B1;
-
-    vector unsigned char R,G,B;
-    vector unsigned char *out,*nout;
-
-    vector signed short   RND = vec_splat_s16(1&lt;&lt;3);
-    vector unsigned short SCL = vec_splat_u16(4);
-    unsigned long scratch[16] __attribute__ ((aligned (16)));
-
-    vector signed short *YCoeffs, *CCoeffs;
-
-    YCoeffs = c-&gt;vYCoeffsBank+dstY*lumFilterSize;
-    CCoeffs = c-&gt;vCCoeffsBank+dstY*chrFilterSize;
-
-    out = (vector unsigned char *)dest;
-
-    for (i=0; i&lt;dstW; i+=16){
-        Y0 = RND;
-        Y1 = RND;
-        /* extract 16 coeffs from lumSrc */
-        for (j=0; j&lt;lumFilterSize; j++) {
-            X0 = vec_ld (0,  &amp;lumSrc[j][i]);
-            X1 = vec_ld (16, &amp;lumSrc[j][i]);
-            Y0 = vec_mradds (X0, YCoeffs[j], Y0);
-            Y1 = vec_mradds (X1, YCoeffs[j], Y1);
-        }
-
-        U = RND;
-        V = RND;
-        /* extract 8 coeffs from U,V */
-        for (j=0; j&lt;chrFilterSize; j++) {
-            X  = vec_ld (0, &amp;chrSrc[j][i/2]);
-            U  = vec_mradds (X, CCoeffs[j], U);
-            X  = vec_ld (0, &amp;chrSrc[j][i/2+2048]);
-            V  = vec_mradds (X, CCoeffs[j], V);
-        }
-
-        /* scale and clip signals */
-        Y0 = vec_sra (Y0, SCL);
-        Y1 = vec_sra (Y1, SCL);
-        U  = vec_sra (U,  SCL);
-        V  = vec_sra (V,  SCL);
-
-        Y0 = vec_clip_s16 (Y0);
-        Y1 = vec_clip_s16 (Y1);
-        U  = vec_clip_s16 (U);
-        V  = vec_clip_s16 (V);
-
-        /* now we have
-          Y0= y0 y1 y2 y3 y4 y5 y6 y7     Y1= y8 y9 y10 y11 y12 y13 y14 y15
-          U= u0 u1 u2 u3 u4 u5 u6 u7      V= v0 v1 v2 v3 v4 v5 v6 v7
-
-          Y0= y0 y1 y2 y3 y4 y5 y6 y7    Y1= y8 y9 y10 y11 y12 y13 y14 y15
-          U0= u0 u0 u1 u1 u2 u2 u3 u3    U1= u4 u4 u5 u5 u6 u6 u7 u7
-          V0= v0 v0 v1 v1 v2 v2 v3 v3    V1= v4 v4 v5 v5 v6 v6 v7 v7
-        */
-
-        U0 = vec_mergeh (U,U);
-        V0 = vec_mergeh (V,V);
-
-        U1 = vec_mergel (U,U);
-        V1 = vec_mergel (V,V);
-
-        cvtyuvtoRGB (c, Y0,U0,V0,&amp;R0,&amp;G0,&amp;B0);
-        cvtyuvtoRGB (c, Y1,U1,V1,&amp;R1,&amp;G1,&amp;B1);
-
-        R  = vec_packclp (R0,R1);
-        G  = vec_packclp (G0,G1);
-        B  = vec_packclp (B0,B1);
-
-        switch(c-&gt;dstFormat) {
-            case PIX_FMT_ABGR:  out_abgr  (R,G,B,out); break;
-            case PIX_FMT_BGRA:  out_bgra  (R,G,B,out); break;
-            case PIX_FMT_RGBA:  out_rgba  (R,G,B,out); break;
-            case PIX_FMT_ARGB:  out_argb  (R,G,B,out); break;
-            case PIX_FMT_RGB24: out_rgb24 (R,G,B,out); break;
-            case PIX_FMT_BGR24: out_bgr24 (R,G,B,out); break;
-            default:
-            {
-                /* If this is reached, the caller should have called yuv2packedXinC
-                   instead. */
-                static int printed_error_message;
-                if (!printed_error_message) {
-                    av_log(c, AV_LOG_ERROR, &quot;altivec_yuv2packedX doesn't support %s output\n&quot;,
-                           sws_format_name(c-&gt;dstFormat));
-                    printed_error_message=1;
-                }
-                return;
-            }
-        }
-    }
-
-    if (i &lt; dstW) {
-        i -= 16;
-
-        Y0 = RND;
-        Y1 = RND;
-        /* extract 16 coeffs from lumSrc */
-        for (j=0; j&lt;lumFilterSize; j++) {
-            X0 = vec_ld (0,  &amp;lumSrc[j][i]);
-            X1 = vec_ld (16, &amp;lumSrc[j][i]);
-            Y0 = vec_mradds (X0, YCoeffs[j], Y0);
-            Y1 = vec_mradds (X1, YCoeffs[j], Y1);
-        }
-
-        U = RND;
-        V = RND;
-        /* extract 8 coeffs from U,V */
-        for (j=0; j&lt;chrFilterSize; j++) {
-            X  = vec_ld (0, &amp;chrSrc[j][i/2]);
-            U  = vec_mradds (X, CCoeffs[j], U);
-            X  = vec_ld (0, &amp;chrSrc[j][i/2+2048]);
-            V  = vec_mradds (X, CCoeffs[j], V);
-        }
-
-        /* scale and clip signals */
-        Y0 = vec_sra (Y0, SCL);
-        Y1 = vec_sra (Y1, SCL);
-        U  = vec_sra (U,  SCL);
-        V  = vec_sra (V,  SCL);
-
-        Y0 = vec_clip_s16 (Y0);
-        Y1 = vec_clip_s16 (Y1);
-        U  = vec_clip_s16 (U);
-        V  = vec_clip_s16 (V);
-
-        /* now we have
-           Y0= y0 y1 y2 y3 y4 y5 y6 y7     Y1= y8 y9 y10 y11 y12 y13 y14 y15
-           U = u0 u1 u2 u3 u4 u5 u6 u7     V = v0 v1 v2 v3 v4 v5 v6 v7
-
-           Y0= y0 y1 y2 y3 y4 y5 y6 y7    Y1= y8 y9 y10 y11 y12 y13 y14 y15
-           U0= u0 u0 u1 u1 u2 u2 u3 u3    U1= u4 u4 u5 u5 u6 u6 u7 u7
-           V0= v0 v0 v1 v1 v2 v2 v3 v3    V1= v4 v4 v5 v5 v6 v6 v7 v7
-        */
-
-        U0 = vec_mergeh (U,U);
-        V0 = vec_mergeh (V,V);
-
-        U1 = vec_mergel (U,U);
-        V1 = vec_mergel (V,V);
-
-        cvtyuvtoRGB (c, Y0,U0,V0,&amp;R0,&amp;G0,&amp;B0);
-        cvtyuvtoRGB (c, Y1,U1,V1,&amp;R1,&amp;G1,&amp;B1);
-
-        R  = vec_packclp (R0,R1);
-        G  = vec_packclp (G0,G1);
-        B  = vec_packclp (B0,B1);
-
-        nout = (vector unsigned char *)scratch;
-        switch(c-&gt;dstFormat) {
-            case PIX_FMT_ABGR:  out_abgr  (R,G,B,nout); break;
-            case PIX_FMT_BGRA:  out_bgra  (R,G,B,nout); break;
-            case PIX_FMT_RGBA:  out_rgba  (R,G,B,nout); break;
-            case PIX_FMT_ARGB:  out_argb  (R,G,B,nout); break;
-            case PIX_FMT_RGB24: out_rgb24 (R,G,B,nout); break;
-            case PIX_FMT_BGR24: out_bgr24 (R,G,B,nout); break;
-            default:
-                /* Unreachable, I think. */
-                av_log(c, AV_LOG_ERROR, &quot;altivec_yuv2packedX doesn't support %s output\n&quot;,
-                       sws_format_name(c-&gt;dstFormat));
-                return;
-        }
-
-        memcpy (&amp;((uint32_t*)dest)[i], scratch, (dstW-i)/4);
-    }
-
-}
+/*
+  <A HREF="https://lists.berlios.de/mailman/listinfo/avidemux-svn-commit">marc.hoffman at analog.com</A>    March 8, 2004
+
+  AltiVec acceleration for colorspace conversion revision 0.2
+
+  convert I420 YV12 to RGB in various formats,
+    it rejects images that are not in 420 formats
+    it rejects images that don't have widths of multiples of 16
+    it rejects images that don't have heights of multiples of 2
+  reject defers to C simulation codes.
+
+  lots of optimizations to be done here
+
+  1. need to fix saturation code, I just couldn't get it to fly with packs and adds.
+     so we currently use max min to clip
+
+  2. the inefficient use of chroma loading needs a bit of brushing up
+
+  3. analysis of pipeline stalls needs to be done, use shark to identify pipeline stalls
+
+
+  MODIFIED to calculate coeffs from currently selected color space.
+  MODIFIED core to be a macro which you spec the output format.
+  ADDED UYVY conversion which is never called due to some thing in SWSCALE.
+  CORRECTED algorithim selection to be strict on input formats.
+  ADDED runtime detection of altivec.
+
+  ADDED altivec_yuv2packedX vertical scl + RGB converter
+
+  March 27,2004
+  PERFORMANCE ANALYSIS
+
+  The C version use 25% of the processor or ~250Mips for D1 video rawvideo used as test
+  The ALTIVEC version uses 10% of the processor or ~100Mips for D1 video same sequence
+
+  720*480*30  ~10MPS
+
+  so we have roughly 10clocks per pixel this is too high something has to be wrong.
+
+  OPTIMIZED clip codes to utilize vec_max and vec_packs removing the need for vec_min.
+
+  OPTIMIZED DST OUTPUT cache/dma controls. we are pretty much
+  guaranteed to have the input video frame it was just decompressed so
+  it probably resides in L1 caches.  However we are creating the
+  output video stream this needs to use the DSTST instruction to
+  optimize for the cache.  We couple this with the fact that we are
+  not going to be visiting the input buffer again so we mark it Least
+  Recently Used.  This shaves 25% of the processor cycles off.
+
+  Now MEMCPY is the largest mips consumer in the system, probably due
+  to the inefficient X11 stuff.
+
+  GL libraries seem to be very slow on this machine 1.33Ghz PB running
+  Jaguar, this is not the case for my 1Ghz PB.  I thought it might be
+  a versioning issues, however I have libGL.1.2.dylib for both
+  machines. ((We need to figure this out now))
+
+  GL2 libraries work now with patch for RGB32
+
+  NOTE quartz vo driver ARGB32_to_RGB24 consumes 30% of the processor
+
+  Integrated luma prescaling adjustment for saturation/contrast/brightness adjustment.
+*/
+
+/*
+ * This file is part of FFmpeg.
+ *
+ * FFmpeg is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License as published by
+ * the Free Software Foundation; either version 2 of the License, or
+ * (at your option) any later version.
+ *
+ * FFmpeg is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ * GNU General Public License for more details.
+ *
+ * You should have received a copy of the GNU General Public License
+ * along with FFmpeg; if not, write to the Free Software
+ * Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA
+ */
+
+#include &lt;stdio.h&gt;
+#include &lt;stdlib.h&gt;
+#include &lt;string.h&gt;
+#include &lt;inttypes.h&gt;
+#include &lt;assert.h&gt;
+#include &quot;config.h&quot;
+#ifdef HAVE_MALLOC_H
+#include &lt;malloc.h&gt;
+#endif
+#include &quot;rgb2rgb.h&quot;
+#include &quot;swscale.h&quot;
+#include &quot;swscale_internal.h&quot;
+
+#undef PROFILE_THE_BEAST
+#undef INC_SCALING
+
+typedef unsigned char ubyte;
+typedef signed char   sbyte;
+
+
+/* RGB interleaver, 16 planar pels 8-bit samples per channel in
+   homogeneous vector registers x0,x1,x2 are interleaved with the
+   following technique:
+
+      o0 = vec_mergeh (x0,x1);
+      o1 = vec_perm (o0, x2, perm_rgb_0);
+      o2 = vec_perm (o0, x2, perm_rgb_1);
+      o3 = vec_mergel (x0,x1);
+      o4 = vec_perm (o3,o2,perm_rgb_2);
+      o5 = vec_perm (o3,o2,perm_rgb_3);
+
+  perm_rgb_0:   o0(RG).h v1(B) --&gt; o1*
+              0   1  2   3   4
+             rgbr|gbrg|brgb|rgbr
+             0010 0100 1001 0010
+             0102 3145 2673 894A
+
+  perm_rgb_1:   o0(RG).h v1(B) --&gt; o2
+              0   1  2   3   4
+             gbrg|brgb|bbbb|bbbb
+             0100 1001 1111 1111
+             B5CD 6EF7 89AB CDEF
+
+  perm_rgb_2:   o3(RG).l o2(rgbB.l) --&gt; o4*
+              0   1  2   3   4
+             gbrg|brgb|rgbr|gbrg
+             1111 1111 0010 0100
+             89AB CDEF 0182 3945
+
+  perm_rgb_2:   o3(RG).l o2(rgbB.l) ---&gt; o5*
+              0   1  2   3   4
+             brgb|rgbr|gbrg|brgb
+             1001 0010 0100 1001
+             a67b 89cA BdCD eEFf
+
+*/
+static
+const vector unsigned char
+  perm_rgb_0 = (const vector unsigned char)AVV(0x00,0x01,0x10,0x02,0x03,0x11,0x04,0x05,
+                                               0x12,0x06,0x07,0x13,0x08,0x09,0x14,0x0a),
+  perm_rgb_1 = (const vector unsigned char)AVV(0x0b,0x15,0x0c,0x0d,0x16,0x0e,0x0f,0x17,
+                                               0x18,0x19,0x1a,0x1b,0x1c,0x1d,0x1e,0x1f),
+  perm_rgb_2 = (const vector unsigned char)AVV(0x10,0x11,0x12,0x13,0x14,0x15,0x16,0x17,
+                                               0x00,0x01,0x18,0x02,0x03,0x19,0x04,0x05),
+  perm_rgb_3 = (const vector unsigned char)AVV(0x1a,0x06,0x07,0x1b,0x08,0x09,0x1c,0x0a,
+                                               0x0b,0x1d,0x0c,0x0d,0x1e,0x0e,0x0f,0x1f);
+
+#define vec_merge3(x2,x1,x0,y0,y1,y2)       \
+do {                                        \
+    typeof(x0) o0,o2,o3;                    \
+        o0 = vec_mergeh (x0,x1);            \
+        y0 = vec_perm (o0, x2, perm_rgb_0); \
+        o2 = vec_perm (o0, x2, perm_rgb_1); \
+        o3 = vec_mergel (x0,x1);            \
+        y1 = vec_perm (o3,o2,perm_rgb_2);   \
+        y2 = vec_perm (o3,o2,perm_rgb_3);   \
+} while(0)
+
+#define vec_mstbgr24(x0,x1,x2,ptr)      \
+do {                                    \
+    typeof(x0) _0,_1,_2;                \
+    vec_merge3 (x0,x1,x2,_0,_1,_2);     \
+    vec_st (_0, 0, ptr++);              \
+    vec_st (_1, 0, ptr++);              \
+    vec_st (_2, 0, ptr++);              \
+}  while (0);
+
+#define vec_mstrgb24(x0,x1,x2,ptr)      \
+do {                                    \
+    typeof(x0) _0,_1,_2;                \
+    vec_merge3 (x2,x1,x0,_0,_1,_2);     \
+    vec_st (_0, 0, ptr++);              \
+    vec_st (_1, 0, ptr++);              \
+    vec_st (_2, 0, ptr++);              \
+}  while (0);
+
+/* pack the pixels in rgb0 format
+   msb R
+   lsb 0
+*/
+#define vec_mstrgb32(T,x0,x1,x2,x3,ptr)                                       \
+do {                                                                          \
+    T _0,_1,_2,_3;                                                            \
+    _0 = vec_mergeh (x0,x1);                                                  \
+    _1 = vec_mergeh (x2,x3);                                                  \
+    _2 = (T)vec_mergeh ((vector unsigned short)_0,(vector unsigned short)_1); \
+    _3 = (T)vec_mergel ((vector unsigned short)_0,(vector unsigned short)_1); \
+    vec_st (_2, 0*16, (T *)ptr);                                              \
+    vec_st (_3, 1*16, (T *)ptr);                                              \
+    _0 = vec_mergel (x0,x1);                                                  \
+    _1 = vec_mergel (x2,x3);                                                  \
+    _2 = (T)vec_mergeh ((vector unsigned short)_0,(vector unsigned short)_1); \
+    _3 = (T)vec_mergel ((vector unsigned short)_0,(vector unsigned short)_1); \
+    vec_st (_2, 2*16, (T *)ptr);                                              \
+    vec_st (_3, 3*16, (T *)ptr);                                              \
+    ptr += 4;                                                                 \
+}  while (0);
+
+/*
+
+  | 1     0       1.4021   | | Y |
+  | 1    -0.3441 -0.7142   |x| Cb|
+  | 1     1.7718  0        | | Cr|
+
+
+  Y:      [-128 127]
+  Cb/Cr : [-128 127]
+
+  typical yuv conversion work on Y: 0-255 this version has been optimized for jpeg decode.
+
+*/
+
+
+
+
+#define vec_unh(x) \
+    (vector signed short) \
+        vec_perm(x,(typeof(x))AVV(0),\
+                 (vector unsigned char)AVV(0x10,0x00,0x10,0x01,0x10,0x02,0x10,0x03,\
+                                           0x10,0x04,0x10,0x05,0x10,0x06,0x10,0x07))
+#define vec_unl(x) \
+    (vector signed short) \
+        vec_perm(x,(typeof(x))AVV(0),\
+                 (vector unsigned char)AVV(0x10,0x08,0x10,0x09,0x10,0x0A,0x10,0x0B,\
+                                           0x10,0x0C,0x10,0x0D,0x10,0x0E,0x10,0x0F))
+
+#define vec_clip_s16(x) \
+    vec_max (vec_min (x, (vector signed short)AVV(235,235,235,235,235,235,235,235)),\
+                         (vector signed short)AVV( 16, 16, 16, 16, 16, 16, 16, 16))
+
+#define vec_packclp(x,y) \
+    (vector unsigned char)vec_packs \
+        ((vector unsigned short)vec_max (x,(vector signed short) AVV(0)), \
+         (vector unsigned short)vec_max (y,(vector signed short) AVV(0)))
+
+//#define out_pixels(a,b,c,ptr) vec_mstrgb32(typeof(a),((typeof (a))AVV(0)),a,a,a,ptr)
+
+
+static inline void cvtyuvtoRGB (SwsContext *c,
+                                vector signed short Y, vector signed short U, vector signed short V,
+                                vector signed short *R, vector signed short *G, vector signed short *B)
+{
+    vector signed   short vx,ux,uvx;
+
+    Y = vec_mradds (Y, c-&gt;CY, c-&gt;OY);
+    U  = vec_sub (U,(vector signed short)
+                    vec_splat((vector signed short)AVV(128),0));
+    V  = vec_sub (V,(vector signed short)
+                    vec_splat((vector signed short)AVV(128),0));
+
+    //   ux  = (CBU*(u&lt;&lt;c-&gt;CSHIFT)+0x4000)&gt;&gt;15;
+    ux = vec_sl (U, c-&gt;CSHIFT);
+    *B = vec_mradds (ux, c-&gt;CBU, Y);
+
+    // vx  = (CRV*(v&lt;&lt;c-&gt;CSHIFT)+0x4000)&gt;&gt;15;
+    vx = vec_sl (V, c-&gt;CSHIFT);
+    *R = vec_mradds (vx, c-&gt;CRV, Y);
+
+    // uvx = ((CGU*u) + (CGV*v))&gt;&gt;15;
+    uvx = vec_mradds (U, c-&gt;CGU, Y);
+    *G  = vec_mradds (V, c-&gt;CGV, uvx);
+}
+
+
+/*
+  ------------------------------------------------------------------------------
+  CS converters
+  ------------------------------------------------------------------------------
+*/
+
+
+#define DEFCSP420_CVT(name,out_pixels)                                  \
+static int altivec_##name (SwsContext *c,                               \
+                           unsigned char **in, int *instrides,          \
+                           int srcSliceY,        int srcSliceH,         \
+                           unsigned char **oplanes, int *outstrides)    \
+{                                                                       \
+    int w = c-&gt;srcW;                                                    \
+    int h = srcSliceH;                                                  \
+    int i,j;                                                            \
+    int instrides_scl[3];                                               \
+    vector unsigned char y0,y1;                                         \
+                                                                        \
+    vector signed char  u,v;                                            \
+                                                                        \
+    vector signed short Y0,Y1,Y2,Y3;                                    \
+    vector signed short U,V;                                            \
+    vector signed short vx,ux,uvx;                                      \
+    vector signed short vx0,ux0,uvx0;                                   \
+    vector signed short vx1,ux1,uvx1;                                   \
+    vector signed short R0,G0,B0;                                       \
+    vector signed short R1,G1,B1;                                       \
+    vector unsigned char R,G,B;                                         \
+                                                                        \
+    vector unsigned char *y1ivP, *y2ivP, *uivP, *vivP;                  \
+    vector unsigned char align_perm;                                    \
+                                                                        \
+    vector signed short                                                 \
+        lCY  = c-&gt;CY,                                                   \
+        lOY  = c-&gt;OY,                                                   \
+        lCRV = c-&gt;CRV,                                                  \
+        lCBU = c-&gt;CBU,                                                  \
+        lCGU = c-&gt;CGU,                                                  \
+        lCGV = c-&gt;CGV;                                                  \
+                                                                        \
+    vector unsigned short lCSHIFT = c-&gt;CSHIFT;                          \
+                                                                        \
+    ubyte *y1i   = in[0];                                               \
+    ubyte *y2i   = in[0]+instrides[0];                                  \
+    ubyte *ui    = in[1];                                               \
+    ubyte *vi    = in[2];                                               \
+                                                                        \
+    vector unsigned char *oute                                          \
+        = (vector unsigned char *)                                      \
+            (oplanes[0]+srcSliceY*outstrides[0]);                       \
+    vector unsigned char *outo                                          \
+        = (vector unsigned char *)                                      \
+            (oplanes[0]+srcSliceY*outstrides[0]+outstrides[0]);         \
+                                                                        \
+                                                                        \
+    instrides_scl[0] = instrides[0]*2-w;  /* the loop moves y{1,2}i by w */ \
+    instrides_scl[1] = instrides[1]-w/2;  /* the loop moves ui by w/2 */    \
+    instrides_scl[2] = instrides[2]-w/2;  /* the loop moves vi by w/2 */    \
+                                                                        \
+                                                                        \
+    for (i=0;i&lt;h/2;i++) {                                               \
+        vec_dstst (outo, (0x02000002|(((w*3+32)/32)&lt;&lt;16)), 0);          \
+        vec_dstst (oute, (0x02000002|(((w*3+32)/32)&lt;&lt;16)), 1);          \
+                                                                        \
+        for (j=0;j&lt;w/16;j++) {                                          \
+                                                                        \
+            y1ivP = (vector unsigned char *)y1i;                        \
+            y2ivP = (vector unsigned char *)y2i;                        \
+            uivP  = (vector unsigned char *)ui;                         \
+            vivP  = (vector unsigned char *)vi;                         \
+                                                                        \
+            align_perm = vec_lvsl (0, y1i);                             \
+            y0 = (vector unsigned char)                                 \
+                 vec_perm (y1ivP[0], y1ivP[1], align_perm);             \
+                                                                        \
+            align_perm = vec_lvsl (0, y2i);                             \
+            y1 = (vector unsigned char)                                 \
+                 vec_perm (y2ivP[0], y2ivP[1], align_perm);             \
+                                                                        \
+            align_perm = vec_lvsl (0, ui);                              \
+            u = (vector signed char)                                    \
+                vec_perm (uivP[0], uivP[1], align_perm);                \
+                                                                        \
+            align_perm = vec_lvsl (0, vi);                              \
+            v = (vector signed char)                                    \
+                vec_perm (vivP[0], vivP[1], align_perm);                \
+                                                                        \
+            u  = (vector signed char)                                   \
+                 vec_sub (u,(vector signed char)                        \
+                          vec_splat((vector signed char)AVV(128),0));   \
+            v  = (vector signed char)                                   \
+                 vec_sub (v,(vector signed char)                        \
+                          vec_splat((vector signed char)AVV(128),0));   \
+                                                                        \
+            U  = vec_unpackh (u);                                       \
+            V  = vec_unpackh (v);                                       \
+                                                                        \
+                                                                        \
+            Y0 = vec_unh (y0);                                          \
+            Y1 = vec_unl (y0);                                          \
+            Y2 = vec_unh (y1);                                          \
+            Y3 = vec_unl (y1);                                          \
+                                                                        \
+            Y0 = vec_mradds (Y0, lCY, lOY);                             \
+            Y1 = vec_mradds (Y1, lCY, lOY);                             \
+            Y2 = vec_mradds (Y2, lCY, lOY);                             \
+            Y3 = vec_mradds (Y3, lCY, lOY);                             \
+                                                                        \
+            /*   ux  = (CBU*(u&lt;&lt;CSHIFT)+0x4000)&gt;&gt;15 */                  \
+            ux = vec_sl (U, lCSHIFT);                                   \
+            ux = vec_mradds (ux, lCBU, (vector signed short)AVV(0));    \
+            ux0  = vec_mergeh (ux,ux);                                  \
+            ux1  = vec_mergel (ux,ux);                                  \
+                                                                        \
+            /* vx  = (CRV*(v&lt;&lt;CSHIFT)+0x4000)&gt;&gt;15;        */            \
+            vx = vec_sl (V, lCSHIFT);                                   \
+            vx = vec_mradds (vx, lCRV, (vector signed short)AVV(0));    \
+            vx0  = vec_mergeh (vx,vx);                                  \
+            vx1  = vec_mergel (vx,vx);                                  \
+                                                                        \
+            /* uvx = ((CGU*u) + (CGV*v))&gt;&gt;15 */                         \
+            uvx = vec_mradds (U, lCGU, (vector signed short)AVV(0));    \
+            uvx = vec_mradds (V, lCGV, uvx);                            \
+            uvx0 = vec_mergeh (uvx,uvx);                                \
+            uvx1 = vec_mergel (uvx,uvx);                                \
+                                                                        \
+            R0 = vec_add (Y0,vx0);                                      \
+            G0 = vec_add (Y0,uvx0);                                     \
+            B0 = vec_add (Y0,ux0);                                      \
+            R1 = vec_add (Y1,vx1);                                      \
+            G1 = vec_add (Y1,uvx1);                                     \
+            B1 = vec_add (Y1,ux1);                                      \
+                                                                        \
+            R  = vec_packclp (R0,R1);                                   \
+            G  = vec_packclp (G0,G1);                                   \
+            B  = vec_packclp (B0,B1);                                   \
+                                                                        \
+            out_pixels(R,G,B,oute);                                     \
+                                                                        \
+            R0 = vec_add (Y2,vx0);                                      \
+            G0 = vec_add (Y2,uvx0);                                     \
+            B0 = vec_add (Y2,ux0);                                      \
+            R1 = vec_add (Y3,vx1);                                      \
+            G1 = vec_add (Y3,uvx1);                                     \
+            B1 = vec_add (Y3,ux1);                                      \
+            R  = vec_packclp (R0,R1);                                   \
+            G  = vec_packclp (G0,G1);                                   \
+            B  = vec_packclp (B0,B1);                                   \
+                                                                        \
+                                                                        \
+            out_pixels(R,G,B,outo);                                     \
+                                                                        \
+            y1i  += 16;                                                 \
+            y2i  += 16;                                                 \
+            ui   += 8;                                                  \
+            vi   += 8;                                                  \
+                                                                        \
+        }                                                               \
+                                                                        \
+        outo  += (outstrides[0])&gt;&gt;4;                                    \
+        oute  += (outstrides[0])&gt;&gt;4;                                    \
+                                                                        \
+        ui    += instrides_scl[1];                                      \
+        vi    += instrides_scl[2];                                      \
+        y1i   += instrides_scl[0];                                      \
+        y2i   += instrides_scl[0];                                      \
+    }                                                                   \
+    return srcSliceH;                                                   \
+}
+
+
+#define out_abgr(a,b,c,ptr)  vec_mstrgb32(typeof(a),((typeof (a))AVV(0)),c,b,a,ptr)
+#define out_bgra(a,b,c,ptr)  vec_mstrgb32(typeof(a),c,b,a,((typeof (a))AVV(0)),ptr)
+#define out_rgba(a,b,c,ptr)  vec_mstrgb32(typeof(a),a,b,c,((typeof (a))AVV(0)),ptr)
+#define out_argb(a,b,c,ptr)  vec_mstrgb32(typeof(a),((typeof (a))AVV(0)),a,b,c,ptr)
+#define out_rgb24(a,b,c,ptr) vec_mstrgb24(a,b,c,ptr)
+#define out_bgr24(a,b,c,ptr) vec_mstbgr24(a,b,c,ptr)
+
+DEFCSP420_CVT (yuv2_abgr, out_abgr)
+#if 1
+DEFCSP420_CVT (yuv2_bgra, out_bgra)
+#else
+static int altivec_yuv2_bgra32 (SwsContext *c,
+                                unsigned char **in, int *instrides,
+                                int srcSliceY,        int srcSliceH,
+                                unsigned char **oplanes, int *outstrides)
+{
+    int w = c-&gt;srcW;
+    int h = srcSliceH;
+    int i,j;
+    int instrides_scl[3];
+    vector unsigned char y0,y1;
+
+    vector signed char  u,v;
+
+    vector signed short Y0,Y1,Y2,Y3;
+    vector signed short U,V;
+    vector signed short vx,ux,uvx;
+    vector signed short vx0,ux0,uvx0;
+    vector signed short vx1,ux1,uvx1;
+    vector signed short R0,G0,B0;
+    vector signed short R1,G1,B1;
+    vector unsigned char R,G,B;
+
+    vector unsigned char *uivP, *vivP;
+    vector unsigned char align_perm;
+
+    vector signed short
+        lCY  = c-&gt;CY,
+        lOY  = c-&gt;OY,
+        lCRV = c-&gt;CRV,
+        lCBU = c-&gt;CBU,
+        lCGU = c-&gt;CGU,
+        lCGV = c-&gt;CGV;
+
+    vector unsigned short lCSHIFT = c-&gt;CSHIFT;
+
+    ubyte *y1i   = in[0];
+    ubyte *y2i   = in[0]+w;
+    ubyte *ui    = in[1];
+    ubyte *vi    = in[2];
+
+    vector unsigned char *oute
+        = (vector unsigned char *)
+          (oplanes[0]+srcSliceY*outstrides[0]);
+    vector unsigned char *outo
+        = (vector unsigned char *)
+          (oplanes[0]+srcSliceY*outstrides[0]+outstrides[0]);
+
+
+    instrides_scl[0] = instrides[0];
+    instrides_scl[1] = instrides[1]-w/2;  /* the loop moves ui by w/2 */
+    instrides_scl[2] = instrides[2]-w/2;  /* the loop moves vi by w/2 */
+
+
+    for (i=0;i&lt;h/2;i++) {
+        vec_dstst (outo, (0x02000002|(((w*3+32)/32)&lt;&lt;16)), 0);
+        vec_dstst (oute, (0x02000002|(((w*3+32)/32)&lt;&lt;16)), 1);
+
+        for (j=0;j&lt;w/16;j++) {
+
+            y0 = vec_ldl (0,y1i);
+            y1 = vec_ldl (0,y2i);
+            uivP = (vector unsigned char *)ui;
+            vivP = (vector unsigned char *)vi;
+
+            align_perm = vec_lvsl (0, ui);
+            u  = (vector signed char)vec_perm (uivP[0], uivP[1], align_perm);
+
+            align_perm = vec_lvsl (0, vi);
+            v  = (vector signed char)vec_perm (vivP[0], vivP[1], align_perm);
+            u  = (vector signed char)
+                 vec_sub (u,(vector signed char)
+                          vec_splat((vector signed char)AVV(128),0));
+
+            v  = (vector signed char)
+                 vec_sub (v, (vector signed char)
+                          vec_splat((vector signed char)AVV(128),0));
+
+            U  = vec_unpackh (u);
+            V  = vec_unpackh (v);
+
+
+            Y0 = vec_unh (y0);
+            Y1 = vec_unl (y0);
+            Y2 = vec_unh (y1);
+            Y3 = vec_unl (y1);
+
+            Y0 = vec_mradds (Y0, lCY, lOY);
+            Y1 = vec_mradds (Y1, lCY, lOY);
+            Y2 = vec_mradds (Y2, lCY, lOY);
+            Y3 = vec_mradds (Y3, lCY, lOY);
+
+            /*   ux  = (CBU*(u&lt;&lt;CSHIFT)+0x4000)&gt;&gt;15 */
+            ux = vec_sl (U, lCSHIFT);
+            ux = vec_mradds (ux, lCBU, (vector signed short)AVV(0));
+            ux0  = vec_mergeh (ux,ux);
+            ux1  = vec_mergel (ux,ux);
+
+            /* vx  = (CRV*(v&lt;&lt;CSHIFT)+0x4000)&gt;&gt;15;        */
+            vx = vec_sl (V, lCSHIFT);
+            vx = vec_mradds (vx, lCRV, (vector signed short)AVV(0));
+            vx0  = vec_mergeh (vx,vx);
+            vx1  = vec_mergel (vx,vx);
+            /* uvx = ((CGU*u) + (CGV*v))&gt;&gt;15 */
+            uvx = vec_mradds (U, lCGU, (vector signed short)AVV(0));
+            uvx = vec_mradds (V, lCGV, uvx);
+            uvx0 = vec_mergeh (uvx,uvx);
+            uvx1 = vec_mergel (uvx,uvx);
+            R0 = vec_add (Y0,vx0);
+            G0 = vec_add (Y0,uvx0);
+            B0 = vec_add (Y0,ux0);
+            R1 = vec_add (Y1,vx1);
+            G1 = vec_add (Y1,uvx1);
+            B1 = vec_add (Y1,ux1);
+            R  = vec_packclp (R0,R1);
+            G  = vec_packclp (G0,G1);
+            B  = vec_packclp (B0,B1);
+
+            out_argb(R,G,B,oute);
+            R0 = vec_add (Y2,vx0);
+            G0 = vec_add (Y2,uvx0);
+            B0 = vec_add (Y2,ux0);
+            R1 = vec_add (Y3,vx1);
+            G1 = vec_add (Y3,uvx1);
+            B1 = vec_add (Y3,ux1);
+            R  = vec_packclp (R0,R1);
+            G  = vec_packclp (G0,G1);
+            B  = vec_packclp (B0,B1);
+
+            out_argb(R,G,B,outo);
+            y1i  += 16;
+            y2i  += 16;
+            ui   += 8;
+            vi   += 8;
+
+        }
+
+        outo  += (outstrides[0])&gt;&gt;4;
+        oute  += (outstrides[0])&gt;&gt;4;
+
+        ui    += instrides_scl[1];
+        vi    += instrides_scl[2];
+        y1i   += instrides_scl[0];
+        y2i   += instrides_scl[0];
+    }
+    return srcSliceH;
+}
+
+#endif
+
+
+DEFCSP420_CVT (yuv2_rgba, out_rgba)
+DEFCSP420_CVT (yuv2_argb, out_argb)
+DEFCSP420_CVT (yuv2_rgb24,  out_rgb24)
+DEFCSP420_CVT (yuv2_bgr24,  out_bgr24)
+
+
+// uyvy|uyvy|uyvy|uyvy
+// 0123 4567 89ab cdef
+static
+const vector unsigned char
+    demux_u = (const vector unsigned char)AVV(0x10,0x00,0x10,0x00,
+                                              0x10,0x04,0x10,0x04,
+                                              0x10,0x08,0x10,0x08,
+                                              0x10,0x0c,0x10,0x0c),
+    demux_v = (const vector unsigned char)AVV(0x10,0x02,0x10,0x02,
+                                              0x10,0x06,0x10,0x06,
+                                              0x10,0x0A,0x10,0x0A,
+                                              0x10,0x0E,0x10,0x0E),
+    demux_y = (const vector unsigned char)AVV(0x10,0x01,0x10,0x03,
+                                              0x10,0x05,0x10,0x07,
+                                              0x10,0x09,0x10,0x0B,
+                                              0x10,0x0D,0x10,0x0F);
+
+/*
+  this is so I can play live CCIR raw video
+*/
+static int altivec_uyvy_rgb32 (SwsContext *c,
+                               unsigned char **in, int *instrides,
+                               int srcSliceY,        int srcSliceH,
+                               unsigned char **oplanes, int *outstrides)
+{
+    int w = c-&gt;srcW;
+    int h = srcSliceH;
+    int i,j;
+    vector unsigned char uyvy;
+    vector signed   short Y,U,V;
+    vector signed   short R0,G0,B0,R1,G1,B1;
+    vector unsigned char  R,G,B;
+    vector unsigned char *out;
+    ubyte *img;
+
+    img = in[0];
+    out = (vector unsigned char *)(oplanes[0]+srcSliceY*outstrides[0]);
+
+    for (i=0;i&lt;h;i++) {
+        for (j=0;j&lt;w/16;j++) {
+            uyvy = vec_ld (0, img);
+            U = (vector signed short)
+                vec_perm (uyvy, (vector unsigned char)AVV(0), demux_u);
+
+            V = (vector signed short)
+                vec_perm (uyvy, (vector unsigned char)AVV(0), demux_v);
+
+            Y = (vector signed short)
+                vec_perm (uyvy, (vector unsigned char)AVV(0), demux_y);
+
+            cvtyuvtoRGB (c, Y,U,V,&amp;R0,&amp;G0,&amp;B0);
+
+            uyvy = vec_ld (16, img);
+            U = (vector signed short)
+                vec_perm (uyvy, (vector unsigned char)AVV(0), demux_u);
+
+            V = (vector signed short)
+                vec_perm (uyvy, (vector unsigned char)AVV(0), demux_v);
+
+            Y = (vector signed short)
+                vec_perm (uyvy, (vector unsigned char)AVV(0), demux_y);
+
+            cvtyuvtoRGB (c, Y,U,V,&amp;R1,&amp;G1,&amp;B1);
+
+            R  = vec_packclp (R0,R1);
+            G  = vec_packclp (G0,G1);
+            B  = vec_packclp (B0,B1);
+
+            //      vec_mstbgr24 (R,G,B, out);
+            out_rgba (R,G,B,out);
+
+            img += 32;
+        }
+    }
+    return srcSliceH;
+}
+
+
+
+/* Ok currently the acceleration routine only supports
+   inputs of widths a multiple of 16
+   and heights a multiple 2
+
+   So we just fall back to the C codes for this.
+*/
+SwsFunc yuv2rgb_init_altivec (SwsContext *c)
+{
+    if (!(c-&gt;flags &amp; SWS_CPU_CAPS_ALTIVEC))
+        return NULL;
+
+    /*
+      and this seems not to matter too much I tried a bunch of
+      videos with abnormal widths and MPlayer crashes elsewhere.
+      mplayer -vo x11 -rawvideo on:w=350:h=240 raw-350x240.eyuv
+      boom with X11 bad match.
+
+    */
+    if ((c-&gt;srcW &amp; 0xf) != 0)    return NULL;
+
+    switch (c-&gt;srcFormat) {
+    case PIX_FMT_YUV410P:
+    case PIX_FMT_YUV420P:
+    /*case IMGFMT_CLPL:        ??? */
+    case PIX_FMT_GRAY8:
+    case PIX_FMT_NV12:
+    case PIX_FMT_NV21:
+        if ((c-&gt;srcH &amp; 0x1) != 0)
+            return NULL;
+
+        switch(c-&gt;dstFormat){
+        case PIX_FMT_RGB24:
+            av_log(c, AV_LOG_WARNING, &quot;ALTIVEC: Color Space RGB24\n&quot;);
+            return altivec_yuv2_rgb24;
+        case PIX_FMT_BGR24:
+            av_log(c, AV_LOG_WARNING, &quot;ALTIVEC: Color Space BGR24\n&quot;);
+            return altivec_yuv2_bgr24;
+        case PIX_FMT_ARGB:
+            av_log(c, AV_LOG_WARNING, &quot;ALTIVEC: Color Space ARGB\n&quot;);
+            return altivec_yuv2_argb;
+        case PIX_FMT_ABGR:
+            av_log(c, AV_LOG_WARNING, &quot;ALTIVEC: Color Space ABGR\n&quot;);
+            return altivec_yuv2_abgr;
+        case PIX_FMT_RGBA:
+            av_log(c, AV_LOG_WARNING, &quot;ALTIVEC: Color Space RGBA\n&quot;);
+            return altivec_yuv2_rgba;
+        case PIX_FMT_BGRA:
+            av_log(c, AV_LOG_WARNING, &quot;ALTIVEC: Color Space BGRA\n&quot;);
+            return altivec_yuv2_bgra;
+        default: return NULL;
+        }
+        break;
+
+    case PIX_FMT_UYVY422:
+        switch(c-&gt;dstFormat){
+        case PIX_FMT_BGR32:
+            av_log(c, AV_LOG_WARNING, &quot;ALTIVEC: Color Space UYVY -&gt; RGB32\n&quot;);
+            return altivec_uyvy_rgb32;
+        default: return NULL;
+        }
+        break;
+
+    }
+    return NULL;
+}
+
+void yuv2rgb_altivec_init_tables (SwsContext *c, const int inv_table[4],int brightness,int contrast, int saturation)
+{
+    union {
+        signed short tmp[8] __attribute__ ((aligned(16)));
+        vector signed short vec;
+    } buf;
+
+    buf.tmp[0] =  ((0xffffLL) * contrast&gt;&gt;8)&gt;&gt;9;                        //cy
+    buf.tmp[1] =  -256*brightness;                                      //oy
+    buf.tmp[2] =  (inv_table[0]&gt;&gt;3) *(contrast&gt;&gt;16)*(saturation&gt;&gt;16);   //crv
+    buf.tmp[3] =  (inv_table[1]&gt;&gt;3) *(contrast&gt;&gt;16)*(saturation&gt;&gt;16);   //cbu
+    buf.tmp[4] = -((inv_table[2]&gt;&gt;1)*(contrast&gt;&gt;16)*(saturation&gt;&gt;16));  //cgu
+    buf.tmp[5] = -((inv_table[3]&gt;&gt;1)*(contrast&gt;&gt;16)*(saturation&gt;&gt;16));  //cgv
+
+
+    c-&gt;CSHIFT = (vector unsigned short)vec_splat_u16(2);
+    c-&gt;CY   = vec_splat ((vector signed short)buf.vec, 0);
+    c-&gt;OY   = vec_splat ((vector signed short)buf.vec, 1);
+    c-&gt;CRV  = vec_splat ((vector signed short)buf.vec, 2);
+    c-&gt;CBU  = vec_splat ((vector signed short)buf.vec, 3);
+    c-&gt;CGU  = vec_splat ((vector signed short)buf.vec, 4);
+    c-&gt;CGV  = vec_splat ((vector signed short)buf.vec, 5);
+#if 0
+    {
+    int i;
+    char *v[6]={&quot;cy&quot;,&quot;oy&quot;,&quot;crv&quot;,&quot;cbu&quot;,&quot;cgu&quot;,&quot;cgv&quot;};
+    for (i=0; i&lt;6; i++)
+        printf(&quot;%s %d &quot;, v[i],buf.tmp[i] );
+        printf(&quot;\n&quot;);
+    }
+#endif
+    return;
+}
+
+
+void
+altivec_yuv2packedX (SwsContext *c,
+                     int16_t *lumFilter, int16_t **lumSrc, int lumFilterSize,
+                     int16_t *chrFilter, int16_t **chrSrc, int chrFilterSize,
+                     uint8_t *dest, int dstW, int dstY)
+{
+    int i,j;
+    vector signed short X,X0,X1,Y0,U0,V0,Y1,U1,V1,U,V;
+    vector signed short R0,G0,B0,R1,G1,B1;
+
+    vector unsigned char R,G,B;
+    vector unsigned char *out,*nout;
+
+    vector signed short   RND = vec_splat_s16(1&lt;&lt;3);
+    vector unsigned short SCL = vec_splat_u16(4);
+    unsigned long scratch[16] __attribute__ ((aligned (16)));
+
+    vector signed short *YCoeffs, *CCoeffs;
+
+    YCoeffs = c-&gt;vYCoeffsBank+dstY*lumFilterSize;
+    CCoeffs = c-&gt;vCCoeffsBank+dstY*chrFilterSize;
+
+    out = (vector unsigned char *)dest;
+
+    for (i=0; i&lt;dstW; i+=16){
+        Y0 = RND;
+        Y1 = RND;
+        /* extract 16 coeffs from lumSrc */
+        for (j=0; j&lt;lumFilterSize; j++) {
+            X0 = vec_ld (0,  &amp;lumSrc[j][i]);
+            X1 = vec_ld (16, &amp;lumSrc[j][i]);
+            Y0 = vec_mradds (X0, YCoeffs[j], Y0);
+            Y1 = vec_mradds (X1, YCoeffs[j], Y1);
+        }
+
+        U = RND;
+        V = RND;
+        /* extract 8 coeffs from U,V */
+        for (j=0; j&lt;chrFilterSize; j++) {
+            X  = vec_ld (0, &amp;chrSrc[j][i/2]);
+            U  = vec_mradds (X, CCoeffs[j], U);
+            X  = vec_ld (0, &amp;chrSrc[j][i/2+2048]);
+            V  = vec_mradds (X, CCoeffs[j], V);
+        }
+
+        /* scale and clip signals */
+        Y0 = vec_sra (Y0, SCL);
+        Y1 = vec_sra (Y1, SCL);
+        U  = vec_sra (U,  SCL);
+        V  = vec_sra (V,  SCL);
+
+        Y0 = vec_clip_s16 (Y0);
+        Y1 = vec_clip_s16 (Y1);
+        U  = vec_clip_s16 (U);
+        V  = vec_clip_s16 (V);
+
+        /* now we have
+          Y0= y0 y1 y2 y3 y4 y5 y6 y7     Y1= y8 y9 y10 y11 y12 y13 y14 y15
+          U= u0 u1 u2 u3 u4 u5 u6 u7      V= v0 v1 v2 v3 v4 v5 v6 v7
+
+          Y0= y0 y1 y2 y3 y4 y5 y6 y7    Y1= y8 y9 y10 y11 y12 y13 y14 y15
+          U0= u0 u0 u1 u1 u2 u2 u3 u3    U1= u4 u4 u5 u5 u6 u6 u7 u7
+          V0= v0 v0 v1 v1 v2 v2 v3 v3    V1= v4 v4 v5 v5 v6 v6 v7 v7
+        */
+
+        U0 = vec_mergeh (U,U);
+        V0 = vec_mergeh (V,V);
+
+        U1 = vec_mergel (U,U);
+        V1 = vec_mergel (V,V);
+
+        cvtyuvtoRGB (c, Y0,U0,V0,&amp;R0,&amp;G0,&amp;B0);
+        cvtyuvtoRGB (c, Y1,U1,V1,&amp;R1,&amp;G1,&amp;B1);
+
+        R  = vec_packclp (R0,R1);
+        G  = vec_packclp (G0,G1);
+        B  = vec_packclp (B0,B1);
+
+        switch(c-&gt;dstFormat) {
+            case PIX_FMT_ABGR:  out_abgr  (R,G,B,out); break;
+            case PIX_FMT_BGRA:  out_bgra  (R,G,B,out); break;
+            case PIX_FMT_RGBA:  out_rgba  (R,G,B,out); break;
+            case PIX_FMT_ARGB:  out_argb  (R,G,B,out); break;
+            case PIX_FMT_RGB24: out_rgb24 (R,G,B,out); break;
+            case PIX_FMT_BGR24: out_bgr24 (R,G,B,out); break;
+            default:
+            {
+                /* If this is reached, the caller should have called yuv2packedXinC
+                   instead. */
+                static int printed_error_message;
+                if (!printed_error_message) {
+                    av_log(c, AV_LOG_ERROR, &quot;altivec_yuv2packedX doesn't support %s output\n&quot;,
+                           sws_format_name(c-&gt;dstFormat));
+                    printed_error_message=1;
+                }
+                return;
+            }
+        }
+    }
+
+    if (i &lt; dstW) {
+        i -= 16;
+
+        Y0 = RND;
+        Y1 = RND;
+        /* extract 16 coeffs from lumSrc */
+        for (j=0; j&lt;lumFilterSize; j++) {
+            X0 = vec_ld (0,  &amp;lumSrc[j][i]);
+            X1 = vec_ld (16, &amp;lumSrc[j][i]);
+            Y0 = vec_mradds (X0, YCoeffs[j], Y0);
+            Y1 = vec_mradds (X1, YCoeffs[j], Y1);
+        }
+
+        U = RND;
+        V = RND;
+        /* extract 8 coeffs from U,V */
+        for (j=0; j&lt;chrFilterSize; j++) {
+            X  = vec_ld (0, &amp;chrSrc[j][i/2]);
+            U  = vec_mradds (X, CCoeffs[j], U);
+            X  = vec_ld (0, &amp;chrSrc[j][i/2+2048]);
+            V  = vec_mradds (X, CCoeffs[j], V);
+        }
+
+        /* scale and clip signals */
+        Y0 = vec_sra (Y0, SCL);
+        Y1 = vec_sra (Y1, SCL);
+        U  = vec_sra (U,  SCL);
+        V  = vec_sra (V,  SCL);
+
+        Y0 = vec_clip_s16 (Y0);
+        Y1 = vec_clip_s16 (Y1);
+        U  = vec_clip_s16 (U);
+        V  = vec_clip_s16 (V);
+
+        /* now we have
+           Y0= y0 y1 y2 y3 y4 y5 y6 y7     Y1= y8 y9 y10 y11 y12 y13 y14 y15
+           U = u0 u1 u2 u3 u4 u5 u6 u7     V = v0 v1 v2 v3 v4 v5 v6 v7
+
+           Y0= y0 y1 y2 y3 y4 y5 y6 y7    Y1= y8 y9 y10 y11 y12 y13 y14 y15
+           U0= u0 u0 u1 u1 u2 u2 u3 u3    U1= u4 u4 u5 u5 u6 u6 u7 u7
+           V0= v0 v0 v1 v1 v2 v2 v3 v3    V1= v4 v4 v5 v5 v6 v6 v7 v7
+        */
+
+        U0 = vec_mergeh (U,U);
+        V0 = vec_mergeh (V,V);
+
+        U1 = vec_mergel (U,U);
+        V1 = vec_mergel (V,V);
+
+        cvtyuvtoRGB (c, Y0,U0,V0,&amp;R0,&amp;G0,&amp;B0);
+        cvtyuvtoRGB (c, Y1,U1,V1,&amp;R1,&amp;G1,&amp;B1);
+
+        R  = vec_packclp (R0,R1);
+        G  = vec_packclp (G0,G1);
+        B  = vec_packclp (B0,B1);
+
+        nout = (vector unsigned char *)scratch;
+        switch(c-&gt;dstFormat) {
+            case PIX_FMT_ABGR:  out_abgr  (R,G,B,nout); break;
+            case PIX_FMT_BGRA:  out_bgra  (R,G,B,nout); break;
+            case PIX_FMT_RGBA:  out_rgba  (R,G,B,nout); break;
+            case PIX_FMT_ARGB:  out_argb  (R,G,B,nout); break;
+            case PIX_FMT_RGB24: out_rgb24 (R,G,B,nout); break;
+            case PIX_FMT_BGR24: out_bgr24 (R,G,B,nout); break;
+            default:
+                /* Unreachable, I think. */
+                av_log(c, AV_LOG_ERROR, &quot;altivec_yuv2packedX doesn't support %s output\n&quot;,
+                       sws_format_name(c-&gt;dstFormat));
+                return;
+        }
+
+        memcpy (&amp;((uint32_t*)dest)[i], scratch, (dstW-i)/4);
+    }
+
+}

Modified: branches/avidemux_2.5_branch_gruntster/avidemux/ADM_libraries/ADM_ffmpeg/ADM_libswscale/yuv2rgb_template.c
===================================================================
--- branches/avidemux_2.5_branch_gruntster/avidemux/ADM_libraries/ADM_ffmpeg/ADM_libswscale/yuv2rgb_template.c	2008-03-21 06:48:49 UTC (rev 3911)
+++ branches/avidemux_2.5_branch_gruntster/avidemux/ADM_libraries/ADM_ffmpeg/ADM_libswscale/yuv2rgb_template.c	2008-03-21 06:51:32 UTC (rev 3912)
@@ -2,7 +2,6 @@
  * yuv2rgb_mmx.c, Software YUV to RGB converter with Intel MMX &quot;technology&quot;
  *
  * Copyright (C) 2000, Silicon Integrated System Corp.
- * All Rights Reserved.
  *
  * Author: Olie Lho &lt;<A HREF="https://lists.berlios.de/mailman/listinfo/avidemux-svn-commit">ollie at sis.com.tw</A>&gt;
  *
@@ -138,16 +137,16 @@
     //printf(&quot;%X %X %X %X %X %X %X %X %X %X\n&quot;, (int)&amp;c-&gt;redDither, (int)&amp;b5Dither, (int)src[0], (int)src[1], (int)src[2], (int)dst[0],
     //srcStride[0],srcStride[1],srcStride[2],dstStride[0]);
     for (y= 0; y&lt;srcSliceH; y++ ) {
-        uint8_t *_image = dst[0] + (y+srcSliceY)*dstStride[0];
-        uint8_t *_py = src[0] + y*srcStride[0];
-        uint8_t *_pu = src[1] + (y&gt;&gt;1)*srcStride[1];
-        uint8_t *_pv = src[2] + (y&gt;&gt;1)*srcStride[2];
+        uint8_t *image = dst[0] + (y+srcSliceY)*dstStride[0];
+        uint8_t *py = src[0] + y*srcStride[0];
+        uint8_t *pu = src[1] + (y&gt;&gt;1)*srcStride[1];
+        uint8_t *pv = src[2] + (y&gt;&gt;1)*srcStride[2];
         long index= -h_size/2;
 
-        b5Dither= dither8[y&amp;1];
-        g6Dither= dither4[y&amp;1];
-        g5Dither= dither8[y&amp;1];
-        r5Dither= dither8[(y+1)&amp;1];
+        b5Dither= ff_dither8[y&amp;1];
+        g6Dither= ff_dither4[y&amp;1];
+        g5Dither= ff_dither8[y&amp;1];
+        r5Dither= ff_dither8[(y+1)&amp;1];
         /* this mmx assembly code deals with SINGLE scan line at a time, it convert 8
            pixels in each iteration */
         __asm__ __volatile__ (
@@ -207,8 +206,8 @@
         &quot;add  $4, %0    \n\t&quot;
         &quot; js  1b        \n\t&quot;
 
-        : &quot;+r&quot; (index), &quot;+r&quot; (_image)
-        : &quot;r&quot; (_pu - index), &quot;r&quot; (_pv - index), &quot;r&quot;(&amp;c-&gt;redDither), &quot;r&quot; (_py - 2*index)
+        : &quot;+r&quot; (index), &quot;+r&quot; (image)
+        : &quot;r&quot; (pu - index), &quot;r&quot; (pv - index), &quot;r&quot;(&amp;c-&gt;redDither), &quot;r&quot; (py - 2*index)
         );
     }
 
@@ -233,16 +232,16 @@
     //printf(&quot;%X %X %X %X %X %X %X %X %X %X\n&quot;, (int)&amp;c-&gt;redDither, (int)&amp;b5Dither, (int)src[0], (int)src[1], (int)src[2], (int)dst[0],
     //srcStride[0],srcStride[1],srcStride[2],dstStride[0]);
     for (y= 0; y&lt;srcSliceH; y++ ) {
-        uint8_t *_image = dst[0] + (y+srcSliceY)*dstStride[0];
-        uint8_t *_py = src[0] + y*srcStride[0];
-        uint8_t *_pu = src[1] + (y&gt;&gt;1)*srcStride[1];
-        uint8_t *_pv = src[2] + (y&gt;&gt;1)*srcStride[2];
+        uint8_t *image = dst[0] + (y+srcSliceY)*dstStride[0];
+        uint8_t *py = src[0] + y*srcStride[0];
+        uint8_t *pu = src[1] + (y&gt;&gt;1)*srcStride[1];
+        uint8_t *pv = src[2] + (y&gt;&gt;1)*srcStride[2];
         long index= -h_size/2;
 
-        b5Dither= dither8[y&amp;1];
-        g6Dither= dither4[y&amp;1];
-        g5Dither= dither8[y&amp;1];
-        r5Dither= dither8[(y+1)&amp;1];
+        b5Dither= ff_dither8[y&amp;1];
+        g6Dither= ff_dither4[y&amp;1];
+        g5Dither= ff_dither8[y&amp;1];
+        r5Dither= ff_dither8[(y+1)&amp;1];
         /* this mmx assembly code deals with SINGLE scan line at a time, it convert 8
            pixels in each iteration */
         __asm__ __volatile__ (
@@ -297,8 +296,8 @@
         &quot;add $16, %1            \n\t&quot;
         &quot;add $4, %0             \n\t&quot;
         &quot; js 1b                 \n\t&quot;
-        : &quot;+r&quot; (index), &quot;+r&quot; (_image)
-        : &quot;r&quot; (_pu - index), &quot;r&quot; (_pv - index), &quot;r&quot;(&amp;c-&gt;redDither), &quot;r&quot; (_py - 2*index)
+        : &quot;+r&quot; (index), &quot;+r&quot; (image)
+        : &quot;r&quot; (pu - index), &quot;r&quot; (pv - index), &quot;r&quot;(&amp;c-&gt;redDither), &quot;r&quot; (py - 2*index)
         );
     }
 
@@ -321,10 +320,10 @@
     __asm__ __volatile__ (&quot;pxor %mm4, %mm4;&quot; /* zero mm4 */ );
 
     for (y= 0; y&lt;srcSliceH; y++ ) {
-        uint8_t *_image = dst[0] + (y+srcSliceY)*dstStride[0];
-        uint8_t *_py = src[0] + y*srcStride[0];
-        uint8_t *_pu = src[1] + (y&gt;&gt;1)*srcStride[1];
-        uint8_t *_pv = src[2] + (y&gt;&gt;1)*srcStride[2];
+        uint8_t *image = dst[0] + (y+srcSliceY)*dstStride[0];
+        uint8_t *py = src[0] + y*srcStride[0];
+        uint8_t *pu = src[1] + (y&gt;&gt;1)*srcStride[1];
+        uint8_t *pv = src[2] + (y&gt;&gt;1)*srcStride[2];
         long index= -h_size/2;
 
         /* this mmx assembly code deals with SINGLE scan line at a time, it convert 8
@@ -339,8 +338,8 @@
 YUV2RGB
         /* mm0=B, %%mm2=G, %%mm1=R */
 #ifdef HAVE_MMX2
-        &quot;movq &quot;MANGLE(M24A)&quot;, %%mm4     \n\t&quot;
-        &quot;movq &quot;MANGLE(M24C)&quot;, %%mm7     \n\t&quot;
+        &quot;movq &quot;MANGLE(ff_M24A)&quot;, %%mm4     \n\t&quot;
+        &quot;movq &quot;MANGLE(ff_M24C)&quot;, %%mm7     \n\t&quot;
         &quot;pshufw $0x50, %%mm0, %%mm5     \n\t&quot; /* B3 B2 B3 B2  B1 B0 B1 B0 */
         &quot;pshufw $0x50, %%mm2, %%mm3     \n\t&quot; /* G3 G2 G3 G2  G1 G0 G1 G0 */
         &quot;pshufw $0x00, %%mm1, %%mm6     \n\t&quot; /* R1 R0 R1 R0  R1 R0 R1 R0 */
@@ -359,7 +358,7 @@
         &quot;pshufw $0x55, %%mm2, %%mm3     \n\t&quot; /* G4 G3 G4 G3  G4 G3 G4 G3 */
         &quot;pshufw $0xA5, %%mm1, %%mm6     \n\t&quot; /* R5 R4 R5 R4  R3 R2 R3 R2 */
 
-        &quot;pand &quot;MANGLE(M24B)&quot;, %%mm5     \n\t&quot; /* B5       B4        B3    */
+        &quot;pand &quot;MANGLE(ff_M24B)&quot;, %%mm5     \n\t&quot; /* B5       B4        B3    */
         &quot;pand          %%mm7, %%mm3     \n\t&quot; /*       G4        G3       */
         &quot;pand          %%mm4, %%mm6     \n\t&quot; /*    R4        R3       R2 */
 
@@ -374,7 +373,7 @@
 
         &quot;pand          %%mm7, %%mm5     \n\t&quot; /*       B7        B6       */
         &quot;pand          %%mm4, %%mm3     \n\t&quot; /*    G7        G6       G5 */
-        &quot;pand &quot;MANGLE(M24B)&quot;, %%mm6     \n\t&quot; /* R7       R6        R5    */
+        &quot;pand &quot;MANGLE(ff_M24B)&quot;, %%mm6     \n\t&quot; /* R7       R6        R5    */
         &quot;movd 4 (%3, %0), %%mm1;&quot; /* Load 4 Cr 00 00 00 00 v3 v2 v1 v0 */
 \
         &quot;por          %%mm5, %%mm3      \n\t&quot;
@@ -443,8 +442,8 @@
         &quot;add  $4, %0    \n\t&quot;
         &quot; js  1b        \n\t&quot;
 
-        : &quot;+r&quot; (index), &quot;+r&quot; (_image)
-        : &quot;r&quot; (_pu - index), &quot;r&quot; (_pv - index), &quot;r&quot;(&amp;c-&gt;redDither), &quot;r&quot; (_py - 2*index)
+        : &quot;+r&quot; (index), &quot;+r&quot; (image)
+        : &quot;r&quot; (pu - index), &quot;r&quot; (pv - index), &quot;r&quot;(&amp;c-&gt;redDither), &quot;r&quot; (py - 2*index)
         );
     }
 
@@ -467,10 +466,10 @@
     __asm__ __volatile__ (&quot;pxor %mm4, %mm4;&quot; /* zero mm4 */ );
 
     for (y= 0; y&lt;srcSliceH; y++ ) {
-        uint8_t *_image = dst[0] + (y+srcSliceY)*dstStride[0];
-        uint8_t *_py = src[0] + y*srcStride[0];
-        uint8_t *_pu = src[1] + (y&gt;&gt;1)*srcStride[1];
-        uint8_t *_pv = src[2] + (y&gt;&gt;1)*srcStride[2];
+        uint8_t *image = dst[0] + (y+srcSliceY)*dstStride[0];
+        uint8_t *py = src[0] + y*srcStride[0];
+        uint8_t *pu = src[1] + (y&gt;&gt;1)*srcStride[1];
+        uint8_t *pv = src[2] + (y&gt;&gt;1)*srcStride[2];
         long index= -h_size/2;
 
         /* this mmx assembly code deals with SINGLE scan line at a time, it convert 8
@@ -529,8 +528,8 @@
         &quot;add  $4, %0    \n\t&quot;
         &quot; js  1b        \n\t&quot;
 
-        : &quot;+r&quot; (index), &quot;+r&quot; (_image)
-        : &quot;r&quot; (_pu - index), &quot;r&quot; (_pv - index), &quot;r&quot;(&amp;c-&gt;redDither), &quot;r&quot; (_py - 2*index)
+        : &quot;+r&quot; (index), &quot;+r&quot; (image)
+        : &quot;r&quot; (pu - index), &quot;r&quot; (pv - index), &quot;r&quot;(&amp;c-&gt;redDither), &quot;r&quot; (py - 2*index)
         );
     }
 


</PRE>

<!--endarticle-->
    <HR>
    <P><UL>
        <!--threads-->
	<LI>Previous message: <A HREF="001173.html">[Avidemux-svn-commit] r3910 - in	branches/avidemux_2.5_branch_gruntster:	avidemux/plugins/ADM_audiodecoder/ADM_ad_dca cmake
</A></li>
	<LI>Next message: <A HREF="001175.html">[Avidemux-svn-commit] r3913 -	branches/avidemux_2.5_branch_gruntster/avidemux/ADM_libraries/ADM_ffmpeg/ADM_lavformat
</A></li>
         <LI> <B>Messages sorted by:</B> 
              <a href="date.html#1174">[ date ]</a>
              <a href="thread.html#1174">[ thread ]</a>
              <a href="subject.html#1174">[ subject ]</a>
              <a href="author.html#1174">[ author ]</a>
         </LI>
       </UL>

<hr>
<a href="https://lists.berlios.de/mailman/listinfo/avidemux-svn-commit">More information about the Avidemux-svn-commit
mailing list</a><br>
</body></html>
